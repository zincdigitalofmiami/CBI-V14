CBI-V14 COMPLETE IMPLEMENTATION PLAN - EVERY DETAIL

CRITICAL RULE: NO MOCK DATA EVER - Every component uses real data from BigQuery or shows "No data yet"

═══════════════════════════════════════════════════════════════════
PHASE -1: CLEANUP & GIT RESET (30 minutes) @critical
═══════════════════════════════════════════════════════════════════

Review What to Keep:
  ☐ Open Cursor file explorer, review every file in ~/CBI-V14
  ☐ Mark these for DELETION: inventory_*.sh, complete_all_tables.sh, purge_*.sh, safe_ingest.py, smart_csv_loader.py
  ☐ Mark these for DELETION: bigquery_harden_*.sh, bootstrap_storage_*.sh, fix_*.sh (all redundant setup scripts)
  ☐ Keep: forecast/, cbi-v14-ingestion/, docker-compose.yml, .gitignore, plan.todo
  ☐ Delete: index.html (old static mock), Dockerfile (root level - we have forecast/Dockerfile)
  ☐ Delete: forecasting-app-backend.json (Cloud Run config dump, not needed locally)

Fix Broken Code:
  ☐ Open cbi-v14-ingestion/intelligent_ingestion.py
  ☐ Delete lines 17-21 (CBI-V13 postgres connection - WRONG PROJECT)
  ☐ Delete lines 24-25 (sys.path imports to CBI-V13)
  ☐ This file is broken and tries to use postgres - mark for rewrite later
  ☐ Open forecast/main.py
  ☐ Fix line 11: PROJECT = os.environ.get("PROJECT","cbi-v14")
  ☐ Fix line 12: DATASET = os.environ.get("DATASET","forecasting_data_warehouse")

Clean Git History:
  ☐ Run: cd ~/CBI-V14
  ☐ Run: git status (see what's uncommitted)
  ☐ Delete all the shell script junk: rm -f inventory_*.sh bigquery_*.sh bootstrap_*.sh complete_*.sh fix_*.sh purge_*.sh safe_ingest.py smart_csv_loader.py
  ☐ Delete: rm -f index.html Dockerfile forecasting-app-backend.json
  ☐ Run: git add -A
  ☐ Run: git commit -m "cleanup: remove redundant scripts and broken postgres references"
  ☐ Run: git push origin main
  ☐ Verify: git status should show "working tree clean"

Update .gitignore:
  ☐ Add these lines to .gitignore:
*.pyc
__pycache__/
.env
service-account-key.json
*.db
*.sqlite
venv/
.venv/
node_modules/
dist/
build/
*.log
  ☐ Commit: git add .gitignore && git commit -m "update gitignore" && git push

═══════════════════════════════════════════════════════════════════
PHASE 0: FIX REMAINING CODE (15 minutes) @critical
═══════════════════════════════════════════════════════════════════

Verify Fixes:
  ☐ Open forecast/main.py and confirm PROJECT="cbi-v14", DATASET="forecasting_data_warehouse"
  ☐ Check docker-compose.yml exists with correct environment variables
  ☐ Verify cbi-v14-ingestion/ exists but note it needs complete rewrite (Phase 1)

Test Local Stack:
  ☐ Run: cd ~/CBI-V14 && docker-compose down
  ☐ Run: docker-compose up -d --build
  ☐ Run: docker ps (should see: cbi-v14-forecast-api-1, cbi-v14-postgres-1)
  ☐ Test: curl http://localhost:8080/health
  ☐ Expected: {"ok": true, "ts": "2025-10-03T..."}
  ☐ If it works: docker-compose down (save resources)

BigQuery Reality Check:
  ☐ Run: bq query "SELECT COUNT(*) as rows FROM \`cbi-v14.forecasting_data_warehouse.soybean_prices\`"
  ☐ Expected: 0 rows (table empty)
  ☐ Run: bq query "SELECT COUNT(*) as rows FROM \`cbi-v14.forecasting_data_warehouse.weather_data\`"
  ☐ Expected: 0 rows (table empty)
  ☐ Conclusion: You have infrastructure but NO DATA - Phase 1 is the real starting point

═══════════════════════════════════════════════════════════════════
PHASE 1: ZL FUTURES DATA INGESTION - COMPLETE IMPLEMENTATION
═══════════════════════════════════════════════════════════════════

Your Polygon.io API Key: NviTUrYkic8_0Tk_mHj63W8luEvcTyMJ

Step 1.1: Test Polygon API Access (5 minutes)
```bash
# Test API with a single day of ZL data
curl "https://api.polygon.io/v2/aggs/ticker/ZL/range/1/day/2024-01-01/2024-01-05?apiKey=NviTUrYkic8_0Tk_mHj63W8luEvcTyMJ"

# Expected response structure:
# {
#   "ticker": "ZL",
#   "results": [
#     {
#       "v": 12345,      # volume
#       "o": 58.42,      # open
#       "c": 58.67,      # close
#       "h": 58.89,      # high
#       "l": 58.31,      # low
#       "t": 1704096000000  # timestamp (milliseconds)
#     }
#   ]
# }
```

Step 1.2: Create BigQuery Table Schema (10 minutes)
```bash
# Create exact schema for soybean_prices table
cd ~/CBI-V14

cat > bigquery_schema_soybean_prices.json << 'EOF'
[
  {"name": "time", "type": "TIMESTAMP", "mode": "REQUIRED"},
  {"name": "symbol", "type": "STRING", "mode": "REQUIRED"},
  {"name": "open", "type": "FLOAT64", "mode": "NULLABLE"},
  {"name": "high", "type": "FLOAT64", "mode": "NULLABLE"},
  {"name": "low", "type": "FLOAT64", "mode": "NULLABLE"},
  {"name": "close", "type": "FLOAT64", "mode": "REQUIRED"},
  {"name": "volume", "type": "INT64", "mode": "NULLABLE"}
]
EOF

# Verify table exists or create it
bq show cbi-v14:forecasting_data_warehouse.soybean_prices || \
bq mk --table \
  --schema bigquery_schema_soybean_prices.json \
  --time_partitioning_field time \
  --time_partitioning_type DAY \
  --clustering_fields symbol \
  cbi-v14:forecasting_data_warehouse.soybean_prices
```

Step 1.3: Write Complete Ingestion Script (30 minutes)
```bash
cat > cbi-v14-ingestion/ingest_zl_futures.py << 'EOFPYTHON'
#!/usr/bin/env python3
"""
ZL Futures Price Ingestion from Polygon.io
Backfills 5 years of daily OHLCV data into BigQuery
"""

import requests
import pandas as pd
from google.cloud import bigquery
from datetime import datetime, timedelta
import time
import sys
import argparse

POLYGON_API_KEY = "NviTUrYkic8_0Tk_mHj63W8luEvcTyMJ"
PROJECT_ID = "cbi-v14"
DATASET_ID = "forecasting_data_warehouse"
TABLE_ID = "soybean_prices"
SYMBOL = "ZL"  # Soybean oil futures

def fetch_polygon_data(symbol, start_date, end_date, max_retries=3):
    """
    Fetch daily aggregates from Polygon.io with retry logic
    
    API Docs: https://polygon.io/docs/stocks/get_v2_aggs_ticker__stocksticker__range__multiplier___timespan___from___to
    Rate limit: 5 requests/minute on free tier
    """
    url = f"https://api.polygon.io/v2/aggs/ticker/{symbol}/range/1/day/{start_date}/{end_date}"
    params = {
        "apiKey": POLYGON_API_KEY,
        "adjusted": "true",
        "sort": "asc",
        "limit": 50000  # Polygon max
    }
    
    for attempt in range(max_retries):
        try:
            print(f"  Fetching {symbol} from {start_date} to {end_date} (attempt {attempt + 1})")
            response = requests.get(url, params=params, timeout=30)
            
            if response.status_code == 429:
                print(f"  Rate limited. Waiting 60 seconds...")
                time.sleep(60)
                continue
                
            response.raise_for_status()
            data = response.json()
            
            if data.get("status") == "ERROR":
                print(f"  API Error: {data.get('error', 'Unknown error')}")
                return None
                
            if "results" not in data or not data["results"]:
                print(f"  No data returned for {start_date} to {end_date}")
                return None
                
            print(f"  Success: {len(data['results'])} bars retrieved")
            return data["results"]
            
        except requests.exceptions.RequestException as e:
            print(f"  Request failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(5)
            else:
                return None
    
    return None

def polygon_to_dataframe(results, symbol):
    """
    Convert Polygon.io results to BigQuery-compatible DataFrame
    
    Polygon schema:
    - t: timestamp in milliseconds
    - o: open price
    - h: high price
    - l: low price
    - c: close price
    - v: volume
    """
    if not results:
        return pd.DataFrame()
    
    df = pd.DataFrame(results)
    
    # Convert milliseconds to datetime
    df['time'] = pd.to_datetime(df['t'], unit='ms', utc=True)
    
    # Rename columns to match BigQuery schema
    df = df.rename(columns={
        'o': 'open',
        'h': 'high',
        'l': 'low',
        'c': 'close',
        'v': 'volume'
    })
    
    # Add symbol column
    df['symbol'] = symbol
    
    # Select only needed columns in correct order
    df = df[['time', 'symbol', 'open', 'high', 'low', 'close', 'volume']]
    
    # Remove any duplicates
    df = df.drop_duplicates(subset=['time', 'symbol'])
    
    return df

def load_to_bigquery(df, project_id, dataset_id, table_id):
    """
    Load DataFrame to BigQuery with error handling
    Uses WRITE_APPEND to avoid overwriting existing data
    """
    if df.empty:
        print("  Empty DataFrame, skipping BigQuery load")
        return False
    
    client = bigquery.Client(project=project_id)
    table_ref = f"{project_id}.{dataset_id}.{table_id}"
    
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_APPEND",  # Append to existing data
        schema=[
            bigquery.SchemaField("time", "TIMESTAMP", mode="REQUIRED"),
            bigquery.SchemaField("symbol", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("open", "FLOAT64"),
            bigquery.SchemaField("high", "FLOAT64"),
            bigquery.SchemaField("low", "FLOAT64"),
            bigquery.SchemaField("close", "FLOAT64", mode="REQUIRED"),
            bigquery.SchemaField("volume", "INT64"),
        ],
    )
    
    try:
        print(f"  Loading {len(df)} rows to {table_ref}")
        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()  # Wait for completion
        print(f"  Success: Loaded {len(df)} rows")
        return True
    except Exception as e:
        print(f"  BigQuery load failed: {e}")
        return False

def backfill_historical_data(symbol, years=5):
    """
    Backfill historical data in 6-month chunks to avoid API limits
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=years * 365)
    
    print(f"\nBackfilling {symbol} from {start_date.date()} to {end_date.date()}")
    print("=" * 60)
    
    chunk_months = 6
    current_start = start_date
    total_rows = 0
    
    while current_start < end_date:
        current_end = min(current_start + timedelta(days=chunk_months * 30), end_date)
        
        start_str = current_start.strftime("%Y-%m-%d")
        end_str = current_end.strftime("%Y-%m-%d")
        
        print(f"\nChunk: {start_str} to {end_str}")
        
        # Fetch data
        results = fetch_polygon_data(symbol, start_str, end_str)
        
        if results:
            # Convert to DataFrame
            df = polygon_to_dataframe(results, symbol)
            
            # Load to BigQuery
            if load_to_bigquery(df, PROJECT_ID, DATASET_ID, TABLE_ID):
                total_rows += len(df)
        
        # Move to next chunk
        current_start = current_end + timedelta(days=1)
        
        # Rate limiting: wait 12 seconds between chunks (5 req/min)
        if current_start < end_date:
            print("  Waiting 12 seconds (rate limit)...")
            time.sleep(12)
    
    print(f"\n{'=' * 60}")
    print(f"Backfill complete: {total_rows} total rows loaded")
    return total_rows

def update_recent_data(symbol, days=5):
    """
    Update with most recent data (for daily refresh)
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    start_str = start_date.strftime("%Y-%m-%d")
    end_str = end_date.strftime("%Y-%m-%d")
    
    print(f"\nUpdating {symbol} for last {days} days")
    print("=" * 60)
    
    results = fetch_polygon_data(symbol, start_str, end_str)
    
    if results:
        df = polygon_to_dataframe(results, symbol)
        if load_to_bigquery(df, PROJECT_ID, DATASET_ID, TABLE_ID):
            print(f"Update complete: {len(df)} rows loaded")
            return len(df)
    
    print("Update failed")
    return 0

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Ingest ZL futures data from Polygon.io")
    parser.add_argument("--backfill", action="store_true", help="Backfill 5 years of historical data")
    parser.add_argument("--years", type=int, default=5, help="Number of years to backfill (default: 5)")
    parser.add_argument("--update", action="store_true", help="Update with last 5 days of data")
    parser.add_argument("--days", type=int, default=5, help="Number of days to update (default: 5)")
    
    args = parser.parse_args()
    
    if args.backfill:
        rows = backfill_historical_data(SYMBOL, years=args.years)
        sys.exit(0 if rows > 0 else 1)
    elif args.update:
        rows = update_recent_data(SYMBOL, days=args.days)
        sys.exit(0 if rows > 0 else 1)
    else:
        parser.print_help()
        sys.exit(1)
EOFPYTHON

chmod +x cbi-v14-ingestion/ingest_zl_futures.py
```

Step 1.4: Install Dependencies (5 minutes)
```bash
cat > cbi-v14-ingestion/requirements.txt << 'EOF'
requests==2.31.0
pandas==2.1.4
google-cloud-bigquery==3.14.0
db-dtypes==1.2.0
EOF

cd cbi-v14-ingestion
pip install -r requirements.txt
```

Step 1.5: Run Backfill (60 minutes - it's automated)
```bash
cd ~/CBI-V14/cbi-v14-ingestion

# Do a small test first (1 month)
python ingest_zl_futures.py --backfill --years 0.08

# If that works, run full 5-year backfill
# This will take 45-60 minutes due to rate limiting
python ingest_zl_futures.py --backfill --years 5

# Watch it run - should see:
# Chunk: 2020-01-01 to 2020-06-30
#   Fetching ZL from 2020-01-01 to 2020-06-30
#   Success: 127 bars retrieved
#   Loading 127 rows to cbi-v14.forecasting_data_warehouse.soybean_prices
#   Success: Loaded 127 rows
#   Waiting 12 seconds (rate limit)...
```

Step 1.6: Verify Data (5 minutes)
```bash
# Check row count
bq query --use_legacy_sql=false \
"SELECT 
  COUNT(*) as total_rows,
  MIN(time) as earliest_date,
  MAX(time) as latest_date,
  COUNT(DISTINCT DATE(time)) as trading_days
FROM \`cbi-v14.forecasting_data_warehouse.soybean_prices\`
WHERE symbol = 'ZL'"

# Expected results:
# total_rows: ~1250 (5 years × 250 trading days)
# earliest_date: 2020-10-03 (approximately)
# latest_date: 2025-10-02 or 2025-10-03
# trading_days: ~1250

# Check recent data quality
bq query --use_legacy_sql=false \
"SELECT *
FROM \`cbi-v14.forecasting_data_warehouse.soybean_prices\`
WHERE symbol = 'ZL'
ORDER BY time DESC
LIMIT 5"

# Should show last 5 trading days with realistic prices (0.50-0.70 range)
```

Step 1.7: Set Up Daily Updates (10 minutes)
```bash
# Test daily update mode
python ingest_zl_futures.py --update --days=5

# Create cron job for daily updates (runs at 6 PM Eastern after market close)
crontab -e

# Add this line:
# 0 18 * * 1-5 cd /Users/zincdigital/CBI-V14/cbi-v14-ingestion && /usr/local/bin/python3 ingest_zl_futures.py --update --days=5 >> ~/cbi-v14-ingestion.log 2>&1
```

Step 1.8: Commit to Git (5 minutes)
```bash
cd ~/CBI-V14
git add cbi-v14-ingestion/ingest_zl_futures.py
git add cbi-v14-ingestion/requirements.txt
git add bigquery_schema_soybean_prices.json
git commit -m "Phase 1: Add ZL futures ingestion from Polygon.io"
git push origin main
```

═══════════════════════════════════════════════════════════════════
PHASE 2: WEATHER DATA - NOAA API - COMPLETE IMPLEMENTATION
═══════════════════════════════════════════════════════════════════

Your NOAA API Token: rxoLrCxYOlQyWvVjbBGRlMMhIRElWKZi

Step 2.1: Store NOAA Token (2 minutes)
```bash
cd ~/CBI-V14/cbi-v14-ingestion

# Store your NOAA token
export NOAA_API_TOKEN="rxoLrCxYOlQyWvVjbBGRlMMhIRElWKZi"
echo 'export NOAA_API_TOKEN="rxoLrCxYOlQyWvVjbBGRlMMhIRElWKZi"' >> ~/.zshrc

# Test the token works
curl "https://www.ncei.noaa.gov/cdo-web/api/v2/datasets" \
  -H "token: rxoLrCxYOlQyWvVjbBGRlMMhIRElWKZi"

# Should return JSON with available datasets
```

Step 2.2: Create Weather Schema (5 minutes)
```bash
cat > bigquery_schema_weather_data.json << 'EOF'
[
  {"name": "date", "type": "DATE", "mode": "REQUIRED"},
  {"name": "region", "type": "STRING", "mode": "REQUIRED"},
  {"name": "station_id", "type": "STRING", "mode": "REQUIRED"},
  {"name": "precip_mm", "type": "FLOAT64", "mode": "NULLABLE"},
  {"name": "temp_max", "type": "FLOAT64", "mode": "NULLABLE"},
  {"name": "temp_min", "type": "FLOAT64", "mode": "NULLABLE"}
]
EOF

bq mk --table \
  --schema bigquery_schema_weather_data.json \
  --time_partitioning_field date \
  --time_partitioning_type DAY \
  --clustering_fields region,station_id \
  cbi-v14:forecasting_data_warehouse.weather_data
```

Step 2.3: Write Weather Ingestion Script with ACTUAL Station IDs (45 minutes)
```bash
cat > cbi-v14-ingestion/ingest_weather_noaa.py << 'EOFPYTHON'
#!/usr/bin/env python3
"""
NOAA Weather Data Ingestion for Agricultural Regions
Fetches precipitation and temperature data
"""

import requests
import pandas as pd
from google.cloud import bigquery
from datetime import datetime, timedelta
import time
import os
import sys
import argparse

NOAA_API_TOKEN = "rxoLrCxYOlQyWvVjbBGRlMMhIRElWKZi"
PROJECT_ID = "cbi-v14"
DATASET_ID = "forecasting_data_warehouse"
TABLE_ID = "weather_data"

# Station IDs researched for soybean growing regions
STATIONS = {
    "Brazil": [
        "GHCND:BR000083361",  # Cuiaba/Marechal Rondon Airport, Mato Grosso
        "GHCND:BR000083531"   # Campo Grande, Mato Grosso do Sul
    ],
    "Argentina": [
        "GHCND:AR000875760",  # Buenos Aires Aero
        "GHCND:AR000875850"   # Rosario Aero
    ],
    "US": [
        "GHCND:USW00014933",  # Des Moines International Airport, Iowa
        "GHCND:USW00094846"   # Chicago O'Hare, Illinois
    ]
}

def fetch_noaa_data(station_id, start_date, end_date, datatypes=["TMAX", "TMIN", "PRCP"]):
    """
    Fetch weather data from NOAA API
    
    API Docs: https://www.ncdc.noaa.gov/cdo-web/webservices/v2
    Rate limit: 5 requests per second, 10,000 per day
    
    datatypes:
    - PRCP: Precipitation (tenths of mm)
    - TMAX: Maximum temperature (tenths of degrees C)
    - TMIN: Minimum temperature (tenths of degrees C)
    """
    url = "https://www.ncei.noaa.gov/cdo-web/api/v2/data"
    
    headers = {"token": NOAA_API_TOKEN}
    
    params = {
        "datasetid": "GHCND",  # Global Historical Climatology Network Daily
        "stationid": station_id,
        "startdate": start_date,
        "enddate": end_date,
        "datatypeid": ",".join(datatypes),
        "units": "metric",
        "limit": 1000  # Max per request
    }
    
    try:
        print(f"  Fetching {station_id} from {start_date} to {end_date}")
        response = requests.get(url, headers=headers, params=params, timeout=30)
        
        if response.status_code == 429:
            print("  Rate limited. Waiting 10 seconds...")
            time.sleep(10)
            return None
        
        response.raise_for_status()
        data = response.json()
        
        if "results" not in data:
            print(f"  No data for {station_id}")
            return None
        
        print(f"  Retrieved {len(data['results'])} observations")
        return data["results"]
        
    except requests.exceptions.RequestException as e:
        print(f"  Request failed: {e}")
        return None

def process_noaa_results(results, region, station_id):
    """
    Convert NOAA results to BigQuery format
    
    NOAA returns one row per datatype per date, need to pivot
    """
    if not results:
        return pd.DataFrame()
    
    df = pd.DataFrame(results)
    
    # Parse date
    df['date'] = pd.to_datetime(df['date']).dt.date
    
    # Pivot: one row per date with columns for each datatype
    pivot = df.pivot_table(
        index='date',
        columns='datatype',
        values='value',
        aggfunc='first'
    ).reset_index()
    
    # Convert from tenths to actual units
    if 'PRCP' in pivot.columns:
        pivot['precip_mm'] = pivot['PRCP'] / 10.0
    else:
        pivot['precip_mm'] = None
    
    if 'TMAX' in pivot.columns:
        pivot['temp_max'] = pivot['TMAX'] / 10.0
    else:
        pivot['temp_max'] = None
    
    if 'TMIN' in pivot.columns:
        pivot['temp_min'] = pivot['TMIN'] / 10.0
    else:
        pivot['temp_min'] = None
    
    # Add metadata
    pivot['region'] = region
    pivot['station_id'] = station_id
    
    # Select final columns
    pivot = pivot[['date', 'region', 'station_id', 'precip_mm', 'temp_max', 'temp_min']]
    
    return pivot

def load_to_bigquery(df):
    """Load weather data to BigQuery"""
    if df.empty:
        return False
    
    client = bigquery.Client(project=PROJECT_ID)
    table_ref = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"
    
    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_APPEND",
        schema=[
            bigquery.SchemaField("date", "DATE", mode="REQUIRED"),
            bigquery.SchemaField("region", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("station_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("precip_mm", "FLOAT64"),
            bigquery.SchemaField("temp_max", "FLOAT64"),
            bigquery.SchemaField("temp_min", "FLOAT64"),
        ],
    )
    
    try:
        print(f"  Loading {len(df)} rows to {table_ref}")
        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()
        print(f"  Success")
        return True
    except Exception as e:
        print(f"  Failed: {e}")
        return False

def backfill_weather(years=2):
    """Backfill weather data for all stations"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=years * 365)
    
    print(f"\nBackfilling weather from {start_date.date()} to {end_date.date()}")
    print("=" * 60)
    
    total_rows = 0
    
    for region, station_list in STATIONS.items():
        for station_id in station_list:
            print(f"\nRegion: {region}, Station: {station_id}")
            
            # Fetch in 1-year chunks
            current_start = start_date
            while current_start < end_date:
                current_end = min(current_start + timedelta(days=365), end_date)
                
                start_str = current_start.strftime("%Y-%m-%d")
                end_str = current_end.strftime("%Y-%m-%d")
                
                results = fetch_noaa_data(station_id, start_str, end_str)
                
                if results:
                    df = process_noaa_results(results, region, station_id)
                    if load_to_bigquery(df):
                        total_rows += len(df)
                
                current_start = current_end + timedelta(days=1)
                time.sleep(0.5)  # Rate limiting
    
    print(f"\nBackfill complete: {total_rows} rows loaded")
    return total_rows

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--backfill", action="store_true")
    parser.add_argument("--years", type=int, default=2)
    args = parser.parse_args()
    
    if args.backfill:
        backfill_weather(years=args.years)
    else:
        parser.print_help()
EOFPYTHON

chmod +x cbi-v14-ingestion/ingest_weather_noaa.py
```

Step 2.4: Run Weather Backfill (30 minutes)
```bash
cd ~/CBI-V14/cbi-v14-ingestion

# Test with 1 month first
python ingest_weather_noaa.py --backfill --years 0.08

# If successful, run full 2-year backfill (takes ~30 minutes)
python ingest_weather_noaa.py --backfill --years 2

# Watch output for each station
```

Step 2.5: Verify Weather Data (5 minutes)
```bash
# Verify data loaded
bq query --use_legacy_sql=false \
"SELECT 
  region,
  COUNT(*) as days,
  MIN(date) as earliest,
  MAX(date) as latest,
  AVG(precip_mm) as avg_precip,
  AVG(temp_max) as avg_temp_max
FROM \`cbi-v14.forecasting_data_warehouse.weather_data\`
GROUP BY region"

# Expected: ~730 rows per region (2 years × 365 days)
```

Step 2.6: Create Weather Aggregation View (5 minutes)
```bash
bq query --use_legacy_sql=false \
"CREATE VIEW forecasting_data_warehouse.weather_daily_by_region AS
SELECT 
  date,
  region,
  AVG(precip_mm) as avg_precip,
  AVG(temp_max) as avg_temp_max,
  AVG(temp_min) as avg_temp_min
FROM forecasting_data_warehouse.weather_data
GROUP BY date, region"

# Test view
bq query --use_legacy_sql=false \
"SELECT * FROM forecasting_data_warehouse.weather_daily_by_region LIMIT 10"
```

Step 2.7: Commit Weather Code (5 minutes)
```bash
cd ~/CBI-V14
git add cbi-v14-ingestion/ingest_weather_noaa.py
git add bigquery_schema_weather_data.json
git commit -m "Phase 2: Add NOAA weather data ingestion"
git push origin main
```

═══════════════════════════════════════════════════════════════════
START HERE - IMMEDIATE ACTIONS
═══════════════════════════════════════════════════════════════════

1. Run Phase -1 cleanup (delete all junk scripts)
2. Commit cleaned repo to git  
3. Fix forecast/main.py project IDs
4. Test Polygon.io API with your key
5. Create ZL futures ingestion script and backfill 5 years
6. Test NOAA API with your token
7. Create weather ingestion script and backfill 2 years

Stop at each phase completion and verify before proceeding.

This plan contains EVERY DETAIL with copy-paste ready code, API keys, exact commands, and verification steps. No guessing required.
