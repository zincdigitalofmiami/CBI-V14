#!/usr/bin/env python3
"""
ingest_brazil_weather_manual.py
Manual CSV processing for INMET Brazil weather data
CRITICAL: Brazil weather accounts for 35-45% of soybean oil price variance

This script processes manually downloaded CSV files from INMET BDMEP portal:
https://bdmep.inmet.gov.br/

CURSOR RULES COMPLIANT:
- Uses existing weather_data table (no new tables)
- Appends with region='Brazil' 
- Uses INMET_ station ID prefix
- Follows exact schema: date, region, station_id, precip_mm, temp_max, temp_min
"""

import pandas as pd
import os
import logging
from datetime import datetime
from bigquery_utils import intelligence_collector
from cache_utils import get_cache
import glob

logger = logging.getLogger(__name__)

class INMETManualProcessor:
    """
    Process manually downloaded INMET CSV files from BDMEP portal
    Handles the standard INMET CSV format with semicolon delimiters
    """
    
    def __init__(self):
        self.cache = get_cache()
        self.csv_directory = "/Users/zincdigital/CBI-V14/inmet_csv_data"
        
        # Ensure CSV directory exists
        os.makedirs(self.csv_directory, exist_ok=True)
        
        # INMET stations from PROJECT_CONTEXT.md (ALREADY DEFINED)
        self.stations = {
            'A901': {'name': 'Sorriso', 'state': 'Mato Grosso'},
            'A923': {'name': 'Sinop', 'state': 'Mato Grosso'},
            'A936': {'name': 'Alta Floresta', 'state': 'Mato Grosso'},
            'A702': {'name': 'Campo Grande', 'state': 'MS'},
            'A736': {'name': 'Dourados', 'state': 'MS'}
        }
        
        logger.info(f"CSV directory: {self.csv_directory}")
        logger.info(f"Expected stations: {list(self.stations.keys())}")
    
    @intelligence_collector('weather_data', cache_ttl_hours=24)
    def process_inmet_csv_files(self):
        """
        Process all INMET CSV files in the designated directory
        
        Expected CSV format from BDMEP portal:
        - Semicolon-delimited
        - Headers in Portuguese
        - Date format: DD/MM/YYYY
        - Station ID in filename or header
        
        Returns:
            DataFrame with processed weather data
        """
        logger.info("Processing INMET CSV files from BDMEP portal")
        
        # Find all CSV files in directory
        csv_files = glob.glob(os.path.join(self.csv_directory, "*.csv"))
        
        if not csv_files:
            logger.warning(f"No CSV files found in {self.csv_directory}")
            logger.info("To use this script:")
            logger.info("1. Visit https://bdmep.inmet.gov.br/")
            logger.info("2. Download CSV files for stations: A901, A923, A936, A702, A736")
            logger.info(f"3. Place CSV files in: {self.csv_directory}")
            logger.info("4. Run this script again")
            return pd.DataFrame()
        
        logger.info(f"Found {len(csv_files)} CSV files to process")
        
        all_weather_data = []
        
        for csv_file in csv_files:
            logger.info(f"Processing: {os.path.basename(csv_file)}")
            
            try:
                # Process individual CSV file
                station_data = self._process_single_csv(csv_file)
                
                if not station_data.empty:
                    all_weather_data.append(station_data)
                    logger.info(f"âœ… Processed {len(station_data)} records from {os.path.basename(csv_file)}")
                else:
                    logger.warning(f"âš ï¸ No valid data in {os.path.basename(csv_file)}")
                    
            except Exception as e:
                logger.error(f"âŒ Failed to process {os.path.basename(csv_file)}: {e}")
                continue
        
        if not all_weather_data:
            logger.warning("No weather data processed from any CSV files")
            return pd.DataFrame()
        
        # Combine all station data
        combined_df = pd.concat(all_weather_data, ignore_index=True)
        
        # Add Brazil region identifier (CRITICAL: maintains separation from US/Argentina)
        combined_df['region'] = 'Brazil'
        
        # Remove duplicates
        combined_df = combined_df.drop_duplicates(subset=['date', 'station_id'])
        
        # Sort by date and station
        combined_df = combined_df.sort_values(['station_id', 'date']).reset_index(drop=True)
        
        logger.info(f"âœ… Total Brazil weather records: {len(combined_df)}")
        logger.info(f"ðŸ“Š Date range: {combined_df['date'].min()} to {combined_df['date'].max()}")
        logger.info(f"ðŸŒ¡ï¸ Stations: {combined_df['station_id'].nunique()}")
        
        # Show summary by station
        self._log_station_summary(combined_df)
        
        return combined_df
    
    def _process_single_csv(self, csv_file):
        """
        Process a single INMET CSV file
        
        Args:
            csv_file: Path to CSV file
            
        Returns:
            DataFrame with processed weather data
        """
        filename = os.path.basename(csv_file)
        
        # Try to detect station ID from filename
        station_id = self._extract_station_id(filename)
        
        if not station_id:
            logger.warning(f"Could not detect station ID from filename: {filename}")
            return pd.DataFrame()
        
        try:
            # Read CSV with common INMET formats
            # Try different encodings and separators
            df = None
            
            for encoding in ['utf-8', 'latin-1', 'iso-8859-1']:
                for sep in [';', ',']:
                    try:
                        df = pd.read_csv(csv_file, encoding=encoding, sep=sep, low_memory=False)
                        if len(df.columns) > 3:  # Valid CSV should have multiple columns
                            logger.info(f"Successfully read with encoding={encoding}, sep='{sep}'")
                            break
                    except Exception:
                        continue
                if df is not None and len(df.columns) > 3:
                    break
            
            if df is None or len(df.columns) <= 3:
                logger.error(f"Could not read CSV file with any encoding/separator combination")
                return pd.DataFrame()
            
            logger.info(f"CSV shape: {df.shape}, columns: {list(df.columns)[:5]}...")
            
            # Process the DataFrame
            processed_data = self._standardize_inmet_data(df, station_id)
            
            return processed_data
            
        except Exception as e:
            logger.error(f"Error processing CSV {filename}: {e}")
            return pd.DataFrame()
    
    def _extract_station_id(self, filename):
        """Extract station ID from filename"""
        filename_upper = filename.upper()
        
        # Check for known station IDs in filename
        for station_id in self.stations.keys():
            if station_id in filename_upper:
                return station_id
        
        # Try to extract A### pattern
        import re
        match = re.search(r'A\d{3}', filename_upper)
        if match:
            return match.group()
        
        return None
    
    def _standardize_inmet_data(self, df, station_id):
        """
        Standardize INMET CSV data to weather_data schema
        
        Args:
            df: Raw DataFrame from CSV
            station_id: INMET station identifier
            
        Returns:
            DataFrame in weather_data format
        """
        processed_records = []
        
        # Common INMET column mappings (Portuguese to English)
        column_mappings = {
            # Date columns
            'DATA': 'date',
            'Data': 'date', 
            'data': 'date',
            'DT_MEDICAO': 'date',
            
            # Precipitation columns
            'CHUVA': 'precip_mm',
            'PRECIPITACAO': 'precip_mm',
            'Precipitacao': 'precip_mm',
            'Chuva': 'precip_mm',
            'PREC': 'precip_mm',
            
            # Temperature columns
            'TEMP_MAX': 'temp_max',
            'TEM_MAX': 'temp_max',
            'Temp_Max': 'temp_max',
            'TEMPERATURA_MAXIMA': 'temp_max',
            
            'TEMP_MIN': 'temp_min',
            'TEM_MIN': 'temp_min',
            'Temp_Min': 'temp_min',
            'TEMPERATURA_MINIMA': 'temp_min'
        }
        
        # Find matching columns
        date_col = None
        precip_col = None
        temp_max_col = None
        temp_min_col = None
        
        for col in df.columns:
            col_clean = str(col).strip()
            if col_clean in column_mappings:
                mapping = column_mappings[col_clean]
                if mapping == 'date' and date_col is None:
                    date_col = col
                elif mapping == 'precip_mm' and precip_col is None:
                    precip_col = col
                elif mapping == 'temp_max' and temp_max_col is None:
                    temp_max_col = col
                elif mapping == 'temp_min' and temp_min_col is None:
                    temp_min_col = col
        
        if not date_col:
            logger.error(f"No date column found in CSV for station {station_id}")
            return pd.DataFrame()
        
        logger.info(f"Mapped columns - Date: {date_col}, Precip: {precip_col}, TempMax: {temp_max_col}, TempMin: {temp_min_col}")
        
        # Process each row
        for _, row in df.iterrows():
            try:
                # Parse date
                date_str = str(row[date_col]).strip()
                if not date_str or date_str.lower() in ['nan', 'null', '']:
                    continue
                
                # Try different date formats
                record_date = None
                for date_format in ['%d/%m/%Y', '%Y-%m-%d', '%d-%m-%Y', '%Y/%m/%d']:
                    try:
                        record_date = pd.to_datetime(date_str, format=date_format).date()
                        break
                    except:
                        continue
                
                if not record_date:
                    continue
                
                # Extract weather values
                precip_mm = self._safe_float(row[precip_col]) if precip_col else None
                temp_max = self._safe_float(row[temp_max_col]) if temp_max_col else None
                temp_min = self._safe_float(row[temp_min_col]) if temp_min_col else None
                
                # Skip records with no useful data
                if all(v is None for v in [precip_mm, temp_max, temp_min]):
                    continue
                
                # Create standardized record (EXACT schema match)
                processed_record = {
                    'date': record_date,
                    'station_id': f'INMET_{station_id}',  # Prefix per PROJECT_CONTEXT
                    'precip_mm': precip_mm,
                    'temp_max': temp_max,
                    'temp_min': temp_min
                }
                
                processed_records.append(processed_record)
                
            except Exception as e:
                logger.debug(f"Failed to process row for station {station_id}: {e}")
                continue
        
        if not processed_records:
            logger.warning(f"No valid records processed for station {station_id}")
            return pd.DataFrame()
        
        # Convert to DataFrame
        df_processed = pd.DataFrame(processed_records)
        
        # Sort by date
        df_processed = df_processed.sort_values('date').reset_index(drop=True)
        
        logger.info(f"Processed {len(df_processed)} records for {self.stations.get(station_id, {}).get('name', station_id)}")
        
        return df_processed
    
    def _safe_float(self, value):
        """Safely convert value to float, handling None and invalid values"""
        if value is None or pd.isna(value):
            return None
        
        # Handle string values
        if isinstance(value, str):
            value = value.strip().replace(',', '.')  # Handle European decimal format
            if value == '' or value.lower() in ['nan', 'null', '-']:
                return None
        
        try:
            return float(value)
        except (ValueError, TypeError):
            return None
    
    def _log_station_summary(self, df):
        """Log summary of processed data by station"""
        if df.empty:
            return
        
        logger.info("ðŸ“Š BRAZIL WEATHER PROCESSING SUMMARY:")
        
        for station_id in df['station_id'].unique():
            station_data = df[df['station_id'] == station_id]
            inmet_id = station_id.replace('INMET_', '')
            station_info = self.stations.get(inmet_id, {'name': 'Unknown', 'state': 'Unknown'})
            
            data_quality = {
                'total': len(station_data),
                'precip': station_data['precip_mm'].notna().sum(),
                'temp_max': station_data['temp_max'].notna().sum(),
                'temp_min': station_data['temp_min'].notna().sum(),
                'date_range': f"{station_data['date'].min()} to {station_data['date'].max()}"
            }
            
            logger.info(f"  ðŸŒ¡ï¸ {station_info['name']}, {station_info['state']} ({inmet_id}):")
            logger.info(f"    Records: {data_quality['total']}")
            logger.info(f"    Date range: {data_quality['date_range']}")
            logger.info(f"    Data quality: Precip {data_quality['precip']}/{data_quality['total']}, "
                       f"TempMax {data_quality['temp_max']}/{data_quality['total']}, "
                       f"TempMin {data_quality['temp_min']}/{data_quality['total']}")


def main():
    """Execute Brazil weather manual CSV processing"""
    processor = INMETManualProcessor()
    
    print("=== BRAZIL WEATHER MANUAL CSV PROCESSING ===")
    print("Processing INMET BDMEP portal CSV downloads")
    print("CRITICAL: Brazil weather = 35-45% of price variance")
    print("=" * 60)
    
    # Process CSV files
    weather_df = processor.process_inmet_csv_files()
    
    if not weather_df.empty:
        print(f"âœ… Processed {len(weather_df)} Brazil weather records")
        
        # Show station coverage
        station_counts = weather_df['station_id'].value_counts()
        print(f"\nðŸ“Š STATION COVERAGE:")
        for station_id, count in station_counts.items():
            inmet_id = station_id.replace('INMET_', '')
            if inmet_id in processor.stations:
                station_info = processor.stations[inmet_id]
                print(f"  - {station_info['name']}, {station_info['state']}: {count} records")
        
        # Show date range
        print(f"\nðŸ“… DATE RANGE: {weather_df['date'].min()} to {weather_df['date'].max()}")
        
        # Show data completeness
        total_records = len(weather_df)
        completeness = {
            'precipitation': (weather_df['precip_mm'].notna().sum() / total_records) * 100,
            'max_temp': (weather_df['temp_max'].notna().sum() / total_records) * 100,
            'min_temp': (weather_df['temp_min'].notna().sum() / total_records) * 100
        }
        
        print(f"\nðŸŒ¡ï¸ DATA COMPLETENESS:")
        for metric, percentage in completeness.items():
            print(f"  - {metric}: {percentage:.1f}%")
        
        print(f"\nâœ… Ready to load {len(weather_df)} records to weather_data table")
        print("âœ… CRITICAL 35-45% variance gap will be RESOLVED")
    
    else:
        print("âŒ No Brazil weather data processed")
        print("\nðŸ“‹ MANUAL STEPS REQUIRED:")
        print("1. Visit: https://bdmep.inmet.gov.br/")
        print("2. Download CSV files for stations: A901, A923, A936, A702, A736")
        print(f"3. Place files in: {processor.csv_directory}")
        print("4. Run this script again")
    
    print("âœ… Brazil weather manual processing completed")


if __name__ == "__main__":
    main()






