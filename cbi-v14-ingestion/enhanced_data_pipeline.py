#!/usr/bin/env python3
"""
CBI-V14 Enhanced Data Pipeline with Comprehensive Validation
Integrates the data validation framework with existing CBI-V14 infrastructure
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta, timezone
import re
import requests
import yfinance as yf
from google.cloud import bigquery
import logging
from typing import Dict, List, Any, Optional, Tuple
import os
import sys
import time
import uuid

# Add the parent directory to path to import existing utilities
sys.path.append('/Users/zincdigital/CBI-V14/cbi-v14-ingestion')
from bigquery_utils import safe_load_to_bigquery, intelligence_collector

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/zincdigital/CBI-V14/logs/enhanced_data_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# CBI-V14 Configuration
PROJECT_ID = "cbi-v14"
DATASET_ID = "forecasting_data_warehouse"
FRED_API_KEY = "dc195c8658c46ee1df83bcd4fd8a690b"

class DataValidator:
    def __init__(self):
        # Updated schemas to match CBI-V14 BigQuery tables
        self.schemas = {
            'forex': {
                'required_columns': ['date', 'from_currency', 'to_currency', 'rate'],
                'numeric_columns': ['rate'],
                'expected_ranges': {
                    'rate': {
                        'USD/BRL': [2.5, 12.0],  # Wider range for govt uncertainty
                        'USD/CNY': [4.5, 10.0],  # Slightly wider for volatility
                        'USD/ARS': [50.0, 5000.0]  # VERY wide - ARS extremely volatile (current ~1490)
                    }
                },
                'expected_frequency': '1D',
                'expected_variance': {'min': 0.0001, 'max': 0.05}
            },
            'interest_rates': {
                'required_columns': ['time', 'indicator', 'value'],
                'numeric_columns': ['value'],
                'expected_ranges': {
                    'value': {
                        'fed_funds_rate': [0.0, 20.0], 
                        'ten_year_treasury': [0.0, 20.0],
                        'yield_curve': [-15.0, 20.0]  # Allow very negative yield curves during govt shutdown
                    }
                },
                'expected_frequency': '1D',
                'expected_variance': {'min': 0.0, 'max': 0.5}
            },
            'crush_margins': {
                'required_columns': ['date', 'oil_price_lb', 'bean_price_bu', 'meal_price_ton', 'crush_margin_usd_per_bu'],
                'numeric_columns': ['oil_price_lb', 'bean_price_bu', 'meal_price_ton', 'crush_margin_usd_per_bu'],
                'expected_ranges': {
                    'oil_price_lb': [0.20, 1.50],
                    'bean_price_bu': [8.0, 20.0],
                    'meal_price_ton': [250.0, 600.0],
                    'crush_margin_usd_per_bu': [-5.0, 10.0]
                },
                'expected_frequency': '1D',
                'expected_variance': {'min': 0.001, 'max': 0.10}
            },
            'export_sales': {
                'required_columns': ['report_date', 'destination_country', 'net_sales_mt', 'cumulative_exports_mt'],
                'numeric_columns': ['net_sales_mt', 'cumulative_exports_mt'],
                'expected_ranges': {
                    'net_sales_mt': [0, 5000000],  # 5M MT max weekly
                    'cumulative_exports_mt': [0, 100000000]  # 100M MT max yearly
                },
                'expected_frequency': '7D',
                'expected_variance': {'min': 0.01, 'max': 2.0}
            }
        }
        
        # Common placeholder patterns
        self.placeholder_patterns = {
            'exact_values': [0, 0.5, -1, -999, 999999],
            'repeated_decimals': r'(-?\d*\.\d{6,})\1+',
            'suspiciously_round': [10.0, 100.0, 1000.0]
        }
        
    def validate_data(self, df: pd.DataFrame, data_type: str, source_id: str) -> Dict[str, Any]:
        """Comprehensive data validation matching CBI-V14 standards"""
        issues = {
            'critical': [],
            'warnings': [],
            'metadata': {
                'source': source_id,
                'data_type': data_type,
                'rows': len(df),
                'validation_timestamp': datetime.now().isoformat()
            }
        }
        
        # VERSION TRACKING: Log validation version and active rules
        validation_version = "v2.0_govt_shutdown_adjusted"
        active_rules = ["wide_volatility_ranges", "placeholder_detection", "duplicate_prevention", "govt_source_tracking"]
        
        logger.info(f"Validating {data_type} data from {source_id}: {len(df)} rows")
        logger.info(f"Validation Version: {validation_version}")
        logger.info(f"Active Rules: {', '.join(active_rules)}")
        
        # 1. Schema validation
        schema_issues = self._validate_schema(df, data_type)
        issues['critical'].extend(schema_issues['critical'])
        issues['warnings'].extend(schema_issues['warnings'])
        
        if schema_issues['critical']:
            logger.error(f"Critical schema issues found in {data_type}: {schema_issues['critical']}")
            return issues
        
        # 2. Data type validation
        dtype_issues = self._validate_data_types(df, data_type)
        issues['critical'].extend(dtype_issues['critical'])
        issues['warnings'].extend(dtype_issues['warnings'])
        
        # 3. Range validation
        range_issues = self._validate_ranges(df, data_type)
        issues['critical'].extend(range_issues['critical'])
        issues['warnings'].extend(range_issues['warnings'])
        
        # 4. Placeholder detection (CRITICAL for CBI-V14)
        placeholder_issues = self._detect_placeholders(df, data_type)
        issues['critical'].extend(placeholder_issues['critical'])
        issues['warnings'].extend(placeholder_issues['warnings'])
        
        # 5. Freshness check
        freshness_issues = self._check_freshness(df, data_type)
        issues['critical'].extend(freshness_issues['critical'])
        issues['warnings'].extend(freshness_issues['warnings'])
        
        # 6. Distribution check
        distribution_issues = self._check_distribution(df, data_type)
        issues['critical'].extend(distribution_issues['critical'])
        issues['warnings'].extend(distribution_issues['warnings'])
        
        # Log validation summary
        if issues['critical']:
            logger.error(f"Validation FAILED for {data_type}: {len(issues['critical'])} critical issues")
        elif issues['warnings']:
            logger.warning(f"Validation PASSED with warnings for {data_type}: {len(issues['warnings'])} warnings")
        else:
            logger.info(f"Validation PASSED for {data_type}: No issues found")
        
        return issues
    
    # Include all the validation methods from the original script
    def _validate_schema(self, df, data_type):
        """Check if dataframe matches expected CBI-V14 schema"""
        issues = {'critical': [], 'warnings': []}
        
        schema = self.schemas.get(data_type)
        if not schema:
            issues['critical'].append(f"Unknown data type: {data_type}")
            return issues
        
        # Check required columns
        missing_cols = [col for col in schema['required_columns'] if col not in df.columns]
        if missing_cols:
            issues['critical'].append(f"Missing required columns: {', '.join(missing_cols)}")
        
        return issues
    
    def _validate_data_types(self, df, data_type):
        """Validate data types match BigQuery expectations"""
        issues = {'critical': [], 'warnings': []}
        
        schema = self.schemas.get(data_type)
        if not schema:
            return issues
        
        # Ensure date/time columns are datetime
        date_cols = ['date', 'time', 'report_date']
        for col in date_cols:
            if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):
                try:
                    pd.to_datetime(df[col])
                except:
                    issues['critical'].append(f"Date column {col} cannot be converted to datetime")
        
        # Check numeric columns
        for col in schema['numeric_columns']:
            if col in df.columns:
                if not pd.api.types.is_numeric_dtype(df[col]):
                    issues['critical'].append(f"Column {col} should be numeric but isn't")
                elif df[col].isna().mean() > 0.2:
                    issues['warnings'].append(f"Column {col} has {df[col].isna().mean():.2%} missing values")
        
        return issues
    
    def _validate_ranges(self, df, data_type):
        """Check if values are within expected ranges for CBI-V14"""
        issues = {'critical': [], 'warnings': []}
        
        schema = self.schemas.get(data_type)
        if not schema or 'expected_ranges' not in schema:
            return issues
        
        for col, ranges in schema['expected_ranges'].items():
            if col not in df.columns:
                continue
            
            # For CBI-V14, we know our specific ranges
            if isinstance(ranges, dict):
                # Multiple ranges based on currency/indicator
                for key, range_vals in ranges.items():
                    # Use the range if we can identify which one applies
                    min_val, max_val = range_vals
                    
                # GOVERNMENT SHUTDOWN ADJUSTED VALIDATION - Very lenient ranges
                # Skip critical range checks for known volatile cases
                skip_critical = False
                
                # Argentine peso during uncertainty - skip critical checks
                if 'rate' in col and 'ARS' in str(key):
                    skip_critical = True
                    logger.info(f"Skipping critical range validation for ARS during govt uncertainty: {df[col].min()}-{df[col].max()}")
                
                # Yield curve during uncertainty - allow deep negative
                if 'yield_curve' in str(key) or col == 'value' and min_val < 0:
                    skip_critical = True
                    logger.info(f"Allowing negative yield curve during govt uncertainty: {df[col].min()}")
                
                # Only warn about range issues during government shutdown
                if df[col].min() < min_val:
                    if not skip_critical and df[col].min() < min_val * 0.1:
                        issues['critical'].append(f"{col} has values far below expected range: {df[col].min()} << {min_val}")
                    else:
                        issues['warnings'].append(f"{col} has values below expected minimum: {df[col].min()} < {min_val}")
                
                if df[col].max() > max_val:
                    if not skip_critical and df[col].max() > max_val * 3.0:
                        issues['critical'].append(f"{col} has values far above expected range: {df[col].max()} >> {max_val}")
                    else:
                        issues['warnings'].append(f"{col} has values above expected maximum: {df[col].max()} > {max_val}")
                    
                    break  # Use first applicable range
            else:
                # Single range
                min_val, max_val = ranges
                if df[col].min() < min_val or df[col].max() > max_val:
                    issues['warnings'].append(f"{col} values outside expected range [{min_val}, {max_val}]: actual [{df[col].min()}, {df[col].max()}]")
        
        return issues
    
    def _detect_placeholders(self, df, data_type):
        """CRITICAL: Advanced placeholder detection for CBI-V14"""
        issues = {'critical': [], 'warnings': []}
        
        schema = self.schemas.get(data_type)
        if not schema:
            return issues
            
        # Check for the dreaded 0.5 placeholder pattern that contaminated our models
        for col in schema['numeric_columns']:
            if col not in df.columns:
                continue
                
            value_counts = df[col].value_counts(normalize=True)
            
            # CRITICAL: Check for 0.5 placeholder (our nemesis)
            if 0.5 in value_counts and value_counts[0.5] > 0.05:  # More than 5% = suspicious
                issues['critical'].append(
                    f"PLACEHOLDER DETECTED: {col} has 0.5 values in {value_counts[0.5]:.2%} of rows - REJECTING DATA")
            
            # Check for other common placeholders
            for placeholder in self.placeholder_patterns['exact_values']:
                if placeholder in value_counts and value_counts[placeholder] > 0.1:
                    issues['critical'].append(
                        f"Placeholder value in {col}: {placeholder} appears in {value_counts[placeholder]:.2%} of rows")
            
            # Check for suspiciously low variance (synthetic data indicator)
            if len(df[col].unique()) < 5 and len(df) > 20:
                issues['critical'].append(f"Column {col} has suspiciously few unique values: {len(df[col].unique())}")
            
            # Check for repeated decimal patterns
            col_str = df[col].astype(str)
            pattern_counts = col_str.value_counts()
            
            for pattern, count in pattern_counts.items():
                if re.search(r'\.\d{6,}', pattern) and count > 5:
                    issues['critical'].append(
                        f"Repeated decimal pattern in {col}: {pattern} appears {count} times")
        
        return issues
    
    def _check_freshness(self, df, data_type):
        """Check data freshness for CBI-V14 requirements"""
        issues = {'critical': [], 'warnings': []}
        
        date_cols = ['date', 'time', 'report_date']
        date_col = None
        for col in date_cols:
            if col in df.columns:
                date_col = col
                break
        
        if not date_col:
            issues['critical'].append("Cannot check freshness: no date column found")
            return issues
        
        # Ensure date is datetime
        if not pd.api.types.is_datetime64_any_dtype(df[date_col]):
            df[date_col] = pd.to_datetime(df[date_col])
        
        # Get latest date in data and handle timezone issues
        latest_date = df[date_col].max()
        
        # Convert to timezone-naive datetime for comparison
        if hasattr(latest_date, 'tz_localize'):
            if latest_date.tz is not None:
                latest_date = latest_date.tz_convert(None)
            else:
                latest_date = latest_date.tz_localize(None) if hasattr(latest_date, 'tz_localize') else latest_date
        
        # Ensure both datetimes are timezone-naive for comparison
        now_naive = datetime.now()
        latest_naive = pd.Timestamp(latest_date).to_pydatetime().replace(tzinfo=None)
        
        days_old = (now_naive - latest_naive).days
        
        # CBI-V14 freshness requirements
        if data_type == 'forex' and days_old > 2:
            issues['critical'].append(f"FX data is too old: {days_old} days (max 2 days for trading)")
        elif data_type == 'interest_rates' and days_old > 5:
            issues['critical'].append(f"Interest rate data is too old: {days_old} days (max 5 days)")
        elif data_type == 'export_sales' and days_old > 14:
            issues['warnings'].append(f"Export sales data is {days_old} days old (weekly data)")
        elif days_old > 7:
            issues['warnings'].append(f"Data is {days_old} days old")
        
        return issues
    
    def _check_distribution(self, df, data_type):
        """Check distribution characteristics for financial data"""
        issues = {'critical': [], 'warnings': []}
        
        schema = self.schemas.get(data_type)
        if not schema:
            return issues
        
        # Check for realistic variance in financial data
        for col in schema['numeric_columns']:
            if col not in df.columns or len(df) < 5:
                continue
            
            # Calculate coefficient of variation
            cv = df[col].std() / df[col].mean() if df[col].mean() != 0 else 0
            
            # Financial data should have some variation
            if cv < 0.001:  # Less than 0.1% variation
                issues['critical'].append(
                    f"{col} has unrealistically low variation: CV={cv:.6f} - possible synthetic data")
            elif cv > 10.0:  # More than 1000% variation
                issues['warnings'].append(
                    f"{col} has very high variation: CV={cv:.2f} - check for outliers")
        
        return issues


class CBI_V14_DataIngestionPipeline:
    def __init__(self):
        self.validator = DataValidator()
        self.client = bigquery.Client(project=PROJECT_ID)
        
    def ingest_and_validate(self, source_id: str, data_type: str, raw_data: pd.DataFrame) -> Dict[str, Any]:
        """Ingest, validate, and store data in CBI-V14 BigQuery"""
        logger.info(f"Processing {data_type} data from {source_id}")
        
        try:
            # 1. Transform data to CBI-V14 format
            transformed_data = self._transform_data(raw_data, source_id, data_type)
            
            # 2. Validate transformed data
            validation_results = self.validator.validate_data(transformed_data, data_type, source_id)
            
            # 3. CRITICAL: Reject if any critical issues (especially placeholders)
            if validation_results['critical']:
                logger.error(f"REJECTING {data_type} data due to critical issues: {validation_results['critical']}")
                return {
                    'status': 'rejected',
                    'data': None,
                    'validation': validation_results,
                    'reason': 'Critical validation failures'
                }
            
            # 4. Store validated data in BigQuery
            success = self._store_to_bigquery(transformed_data, data_type)
            
            if success:
                logger.info(f"Successfully stored {len(transformed_data)} rows of {data_type} data")
                return {
                    'status': 'success',
                    'data': transformed_data,
                    'validation': validation_results,
                    'rows_stored': len(transformed_data)
                }
            else:
                return {
                    'status': 'failed',
                    'data': None,
                    'validation': validation_results,
                    'reason': 'BigQuery storage failed'
                }
                
        except Exception as e:
            logger.error(f"Exception processing {data_type} from {source_id}: {str(e)}")
            return {
                'status': 'error',
                'data': None,
                'validation': None,
                'reason': f"Exception: {str(e)}"
            }
    
    def _transform_data(self, df: pd.DataFrame, source_id: str, data_type: str) -> pd.DataFrame:
        """Transform data to CBI-V14 BigQuery schema"""
        transformed_df = df.copy()
        
        # Add provenance metadata with explicit source tracking
        transformed_df['source_name'] = source_id
        
        # GOVERNMENT SHUTDOWN TRACKING: Classify data sources
        if any(gov_source in source_id.lower() for gov_source in ['fred', 'usda', 'bls', 'census', 'treasury']):
            transformed_df['source_type'] = 'government'
            transformed_df['govt_shutdown_risk'] = True
            transformed_df['confidence_score'] = 0.85  # Slightly lower during govt uncertainty
        else:
            transformed_df['source_type'] = 'private_sector'
            transformed_df['govt_shutdown_risk'] = False
            transformed_df['confidence_score'] = 0.9  # High confidence for private sources
        
        transformed_df['ingest_timestamp_utc'] = datetime.now(timezone.utc)
        transformed_df['provenance_uuid'] = [str(uuid.uuid4()) for _ in range(len(transformed_df))]
        
        # VERSION TRACKING: Record which validation rules were active
        transformed_df['validation_version'] = 'v2.0_govt_shutdown_adjusted'
        transformed_df['validation_rules_active'] = f"wide_volatility_ranges,placeholder_detection,duplicate_prevention,{datetime.now().strftime('%Y%m%d')}"
        
        # Apply data type specific transformations
        if data_type == 'forex':
            return self._transform_forex_data(transformed_df, source_id)
        elif data_type == 'interest_rates':
            return self._transform_interest_rate_data(transformed_df, source_id)
        elif data_type == 'crush_margins':
            return self._transform_crush_margin_data(transformed_df, source_id)
        elif data_type == 'export_sales':
            return self._transform_export_sales_data(transformed_df, source_id)
        
        return transformed_df
    
    def _transform_forex_data(self, df: pd.DataFrame, source_id: str) -> pd.DataFrame:
        """Transform to CBI-V14 currency_data schema"""
        # Ensure required columns exist
        if 'date' not in df.columns and df.index.name == 'Date':
            df = df.reset_index()
            df['date'] = df['Date']
        
        # Ensure we have from_currency and to_currency
        if 'from_currency' not in df.columns:
            df['from_currency'] = 'USD'
        
        # Set to_currency based on the data source
        if 'to_currency' not in df.columns:
            if source_id.upper().endswith('BRL') or 'brl' in source_id.lower():
                df['to_currency'] = 'BRL'
            elif source_id.upper().endswith('CNY') or 'cny' in source_id.lower():
                df['to_currency'] = 'CNY'
            elif source_id.upper().endswith('ARS') or 'ars' in source_id.lower():
                df['to_currency'] = 'ARS'
        
        # Rename rate column if needed
        if 'rate' not in df.columns and 'Close' in df.columns:
            df['rate'] = df['Close']
        elif 'rate' not in df.columns and 'close' in df.columns:
            df['rate'] = df['close']
        
        # Select only required columns
        required_cols = ['date', 'from_currency', 'to_currency', 'rate', 'source_name', 
                        'confidence_score', 'ingest_timestamp_utc', 'provenance_uuid']
        
        return df[[col for col in required_cols if col in df.columns]]
    
    def _transform_interest_rate_data(self, df: pd.DataFrame, source_id: str) -> pd.DataFrame:
        """Transform to CBI-V14 economic_indicators schema"""
        # Ensure time column exists
        if 'time' not in df.columns and 'date' in df.columns:
            df['time'] = pd.to_datetime(df['date'])
        
        # Ensure indicator column exists
        if 'indicator' not in df.columns:
            df['indicator'] = 'unknown'  # This should be set by the caller
        
        # Ensure value column exists
        if 'value' not in df.columns and 'rate' in df.columns:
            df['value'] = df['rate']
        
        # Select required columns
        required_cols = ['time', 'indicator', 'value', 'source_name', 
                        'confidence_score', 'ingest_timestamp_utc', 'provenance_uuid']
        
        return df[[col for col in required_cols if col in df.columns]]
    
    def _transform_crush_margin_data(self, df: pd.DataFrame, source_id: str) -> pd.DataFrame:
        """Transform to CBI-V14 crush margins schema - data is already calculated"""
        # This data comes from our existing crush margin calculation
        # Just add metadata
        return df
    
    def _transform_export_sales_data(self, df: pd.DataFrame, source_id: str) -> pd.DataFrame:
        """Transform to CBI-V14 export sales schema"""
        # Ensure required columns exist
        if 'marketing_year' not in df.columns:
            # Derive marketing year from report_date (Sept 1 - Aug 31)
            df['marketing_year'] = df['report_date'].apply(
                lambda x: x.year if x.month >= 9 else x.year - 1
            )
        
        return df
    
    def _check_for_duplicates_in_bigquery(self, df: pd.DataFrame, data_type: str) -> pd.DataFrame:
        """CRITICAL: Check for existing data in BigQuery to prevent duplicates"""
        try:
            if data_type == 'forex':
                # Check existing FX data for each currency pair
                unique_pairs = df[['from_currency', 'to_currency']].drop_duplicates()
                new_data_list = []
                
                for _, row in unique_pairs.iterrows():
                    from_curr = row['from_currency']
                    to_curr = row['to_currency']
                    
                    # Get date range for this pair in new data
                    pair_data = df[(df['from_currency'] == from_curr) & (df['to_currency'] == to_curr)]
                    
                    # Ensure date column is datetime
                    if not pd.api.types.is_datetime64_any_dtype(pair_data['date']):
                        pair_data['date'] = pd.to_datetime(pair_data['date'])
                    
                    min_date = pair_data['date'].min().strftime('%Y-%m-%d')
                    max_date = pair_data['date'].max().strftime('%Y-%m-%d')
                    
                    # Check what already exists in BigQuery
                    check_query = f'''
                    SELECT DISTINCT DATE(date) as existing_date
                    FROM `{PROJECT_ID}.{DATASET_ID}.currency_data`
                    WHERE from_currency = '{from_curr}'
                    AND to_currency = '{to_curr}'
                    AND DATE(date) BETWEEN '{min_date}' AND '{max_date}'
                    '''
                    
                    existing_dates = self.client.query(check_query).to_dataframe()
                    
                    if not existing_dates.empty:
                        existing_date_set = set(existing_dates['existing_date'].dt.date)
                        
                        # Filter out duplicates
                        pair_data['date_only'] = pair_data['date'].dt.date
                        new_pair_data = pair_data[~pair_data['date_only'].isin(existing_date_set)]
                        new_pair_data = new_pair_data.drop('date_only', axis=1)
                        
                        if len(new_pair_data) < len(pair_data):
                            duplicates_removed = len(pair_data) - len(new_pair_data)
                            logger.info(f"✅ PREVENTED {duplicates_removed} duplicates for {from_curr}/{to_curr}")
                        
                        if len(new_pair_data) > 0:
                            new_data_list.append(new_pair_data)
                    else:
                        new_data_list.append(pair_data)
                
                if new_data_list:
                    return pd.concat(new_data_list, ignore_index=True)
                else:
                    logger.info("✅ No new FX data to insert - all dates already exist")
                    return pd.DataFrame()
                    
            elif data_type == 'interest_rates':
                # Check existing interest rate data
                indicators = df['indicator'].unique()
                new_data_list = []
                
                for indicator in indicators:
                    indicator_data = df[df['indicator'] == indicator]
                    
                    # Ensure time column is datetime
                    if not pd.api.types.is_datetime64_any_dtype(indicator_data['time']):
                        indicator_data['time'] = pd.to_datetime(indicator_data['time'])
                    
                    min_date = indicator_data['time'].min().strftime('%Y-%m-%d')
                    max_date = indicator_data['time'].max().strftime('%Y-%m-%d')
                    
                    check_query = f'''
                    SELECT DISTINCT DATE(time) as existing_date
                    FROM `{PROJECT_ID}.{DATASET_ID}.economic_indicators`
                    WHERE indicator = '{indicator}'
                    AND DATE(time) BETWEEN '{min_date}' AND '{max_date}'
                    '''
                    
                    existing_dates = self.client.query(check_query).to_dataframe()
                    
                    if not existing_dates.empty:
                        existing_date_set = set(existing_dates['existing_date'].dt.date)
                        
                        # Filter out duplicates
                        indicator_data['date_only'] = indicator_data['time'].dt.date
                        new_indicator_data = indicator_data[~indicator_data['date_only'].isin(existing_date_set)]
                        new_indicator_data = new_indicator_data.drop('date_only', axis=1)
                        
                        if len(new_indicator_data) < len(indicator_data):
                            duplicates_removed = len(indicator_data) - len(new_indicator_data)
                            logger.info(f"✅ PREVENTED {duplicates_removed} duplicates for {indicator}")
                        
                        if len(new_indicator_data) > 0:
                            new_data_list.append(new_indicator_data)
                    else:
                        new_data_list.append(indicator_data)
                
                if new_data_list:
                    return pd.concat(new_data_list, ignore_index=True)
                else:
                    logger.info("✅ No new interest rate data to insert - all dates already exist")
                    return pd.DataFrame()
            
            else:
                # For other data types, return as-is
                return df
                
        except Exception as e:
            logger.error(f"Error checking for duplicates: {str(e)}")
            # If duplicate check fails, better to be safe and not insert anything
            logger.warning("Duplicate check failed - NOT inserting data to prevent corruption")
            return pd.DataFrame()

    def _store_to_bigquery(self, df: pd.DataFrame, data_type: str) -> bool:
        """Store validated data to appropriate BigQuery table with duplicate protection"""
        try:
            # CRITICAL: Check for duplicates first
            df_deduplicated = self._check_for_duplicates_in_bigquery(df, data_type)
            
            if df_deduplicated.empty:
                logger.info(f"No new {data_type} data to store after deduplication")
                return True  # Not an error - just nothing new to store
            
            # Map data types to BigQuery tables
            table_mapping = {
                'forex': f'{PROJECT_ID}.{DATASET_ID}.currency_data',
                'interest_rates': f'{PROJECT_ID}.{DATASET_ID}.economic_indicators',
                'crush_margins': f'{PROJECT_ID}.curated.vw_crush_margins_daily',  # This is a view
                'export_sales': f'{PROJECT_ID}.curated.vw_usda_export_sales_soy_weekly'  # This is a view
            }
            
            table_id = table_mapping.get(data_type)
            if not table_id:
                logger.error(f"No table mapping for data type: {data_type}")
                return False
            
            # For views, we need to insert into the underlying table
            if data_type == 'crush_margins':
                # This data is calculated, not stored directly
                logger.info("Crush margin data is calculated in real-time, not stored")
                return True
            elif data_type == 'export_sales':
                table_id = f'{PROJECT_ID}.{DATASET_ID}.usda_export_sales'
            
            # FINAL SAFETY CHECK: Ensure no obvious placeholder values
            for col in df_deduplicated.select_dtypes(include=[np.number]).columns:
                placeholder_count = (df_deduplicated[col] == 0.5).sum()
                if placeholder_count > 0:
                    logger.error(f"CRITICAL: Found {placeholder_count} placeholder 0.5 values in {col} - ABORTING insert")
                    return False
            
            # Use existing BigQuery utility
            success = safe_load_to_bigquery(
                df_deduplicated, 
                table_id, 
                if_exists='append',
                create_if_missing=True
            )
            
            if success:
                logger.info(f"✅ Successfully stored {len(df_deduplicated)} NEW {data_type} records (duplicates prevented)")
            
            return success
            
        except Exception as e:
            logger.error(f"Error storing {data_type} to BigQuery: {str(e)}")
            return False


# DATA FETCHING IMPLEMENTATIONS

def fetch_yahoo_forex_data() -> pd.DataFrame:
    """Fetch ONLY RECENT FX data from Yahoo Finance - NO HISTORICAL YEARS"""
    logger.info("Fetching RECENT FX data from Yahoo Finance (last 15 days only)")
    
    currency_pairs = ['USDBRL=X', 'USDCNY=X', 'USDARS=X']
    all_data = []
    
    for pair in currency_pairs:
        try:
            ticker = yf.Ticker(pair)
            # CRITICAL: Only get last 15 days to avoid corrupting historical data
            # This fills the 12-day gap we identified without risking years of bad data
            data = ticker.history(period='15d')
            
            if not data.empty:
                data = data.reset_index()
                data['pair'] = pair
                
                # SAFETY CHECK: Ensure data is recent (within last 20 days)
                latest_date = data['Date'].max()
                # Convert to timezone-naive for comparison
                if hasattr(latest_date, 'tz_localize'):
                    latest_date = latest_date.tz_localize(None) if latest_date.tz is None else latest_date.tz_convert(None)
                days_old = (datetime.now() - pd.Timestamp(latest_date)).days
                
                if days_old > 20:
                    logger.warning(f"Yahoo data for {pair} is {days_old} days old - REJECTING to avoid stale data")
                    continue
                
                # SAFETY CHECK: Ensure reasonable data range
                if 'Close' in data.columns:
                    min_price = data['Close'].min()
                    max_price = data['Close'].max()
                    
                    # Sanity check ranges - WIDENED FOR GOVERNMENT SHUTDOWN VOLATILITY
                    if pair == 'USDBRL=X' and (min_price < 2.5 or max_price > 12.0):
                        logger.warning(f"USD/BRL data out of range [{min_price}, {max_price}] - REJECTING (govt volatility ranges)")
                        continue
                    elif pair == 'USDCNY=X' and (min_price < 4.5 or max_price > 10.0):
                        logger.warning(f"USD/CNY data out of range [{min_price}, {max_price}] - REJECTING (govt volatility ranges)")
                        continue
                    elif pair == 'USDARS=X' and (min_price < 50.0 or max_price > 5000.0):
                        logger.warning(f"USD/ARS data out of range [{min_price}, {max_price}] - REJECTING (govt volatility ranges)")
                        continue
                
                all_data.append(data)
                logger.info(f"✅ Fetched {len(data)} VALIDATED records for {pair} (latest: {latest_date.date()})")
            else:
                logger.warning(f"No data returned for {pair}")
                
        except Exception as e:
            logger.error(f"Error fetching {pair}: {str(e)}")
            continue
    
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        logger.info(f"✅ Combined RECENT FX data: {len(combined_df)} total records")
        return combined_df
    else:
        logger.error("❌ No FX data could be fetched")
        return pd.DataFrame()


def fetch_fred_interest_rates() -> pd.DataFrame:
    """Fetch RECENT interest rate data from FRED API - LIMITED LOOKBACK"""
    logger.info("Fetching RECENT interest rates from FRED (last 30 days only)")
    
    # Key interest rate series
    fred_series = {
        'DFF': 'fed_funds_rate',
        'DGS10': 'ten_year_treasury',
        'DGS2': 'two_year_treasury'
    }
    
    all_data = []
    base_url = "https://api.stlouisfed.org/fred/series/observations"
    
    for series_id, indicator_name in fred_series.items():
        try:
            # CRITICAL: Only get last 30 days to avoid historical data corruption
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            
            params = {
                'series_id': series_id,
                'api_key': FRED_API_KEY,
                'file_type': 'json',
                'observation_start': start_date
            }
            
            response = requests.get(base_url, params=params)
            response.raise_for_status()
            
            data = response.json()
            
            if 'observations' in data:
                df = pd.DataFrame(data['observations'])
                df['date'] = pd.to_datetime(df['date'])
                df['value'] = pd.to_numeric(df['value'], errors='coerce')
                df['indicator'] = indicator_name
                df['series_id'] = series_id
                
                # SAFETY CHECK: Remove obviously bad values
                df = df.dropna(subset=['value'])
                
                # SAFETY CHECK: Ensure rates are in reasonable ranges
                if indicator_name == 'fed_funds_rate' and (df['value'].min() < 0 or df['value'].max() > 20):
                    logger.warning(f"Fed funds rate out of range [0, 20%] - REJECTING {indicator_name}")
                    continue
                elif 'treasury' in indicator_name and (df['value'].min() < 0 or df['value'].max() > 20):
                    logger.warning(f"Treasury rate out of range [0, 20%] - REJECTING {indicator_name}")
                    continue
                
                all_data.append(df)
                logger.info(f"✅ Fetched {len(df)} VALIDATED records for {indicator_name}")
            
            # Rate limit to avoid API abuse
            time.sleep(0.2)
            
        except Exception as e:
            logger.error(f"Error fetching {series_id}: {str(e)}")
            continue
    
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # Calculate yield curve ONLY if we have both rates
        ten_year_data = combined_df[combined_df['indicator'] == 'ten_year_treasury']
        fed_funds_data = combined_df[combined_df['indicator'] == 'fed_funds_rate']
        
        if len(ten_year_data) > 0 and len(fed_funds_data) > 0:
            # Get most recent values for yield curve calculation
            latest_10y = ten_year_data['value'].iloc[-1]
            latest_fed = fed_funds_data['value'].iloc[-1]
            
            # SAFETY CHECK: Ensure yield curve makes sense
            yield_spread = latest_10y - latest_fed
            if -5.0 <= yield_spread <= 10.0:  # Reasonable yield curve range
                yield_curve_row = {
                    'date': datetime.now().date(),
                    'value': yield_spread,
                    'indicator': 'yield_curve',
                    'series_id': 'CALCULATED'
                }
                
                yield_curve_df = pd.DataFrame([yield_curve_row])
                combined_df = pd.concat([combined_df, yield_curve_df], ignore_index=True)
                logger.info(f"✅ Added yield curve: {yield_spread:.2f}%")
            else:
                logger.warning(f"Yield curve out of range: {yield_spread:.2f}% - NOT adding")
        
        logger.info(f"✅ Combined RECENT interest rate data: {len(combined_df)} total records")
        return combined_df
    else:
        logger.error("❌ No interest rate data could be fetched")
        return pd.DataFrame()


def fetch_commodity_price_data() -> pd.DataFrame:
    """Fetch commodity price data (placeholder - implement as needed)"""
    logger.info("Commodity price data fetched from existing pipelines")
    return pd.DataFrame()  # This comes from existing ingestion


def store_validated_data(df: pd.DataFrame, data_type: str) -> bool:
    """Store data using CBI-V14 pipeline"""
    pipeline = CBI_V14_DataIngestionPipeline()
    return pipeline._store_to_bigquery(df, data_type)


# BIDAILY DATA COLLECTION WITH CBI-V14 INTEGRATION

def bidaily_data_collection() -> Tuple[bool, Dict[str, Any]]:
    """Run bidaily data collection with CBI-V14 validation"""
    logger.info(f"Starting CBI-V14 bidaily data collection at {datetime.now()}")
    
    # Create pipeline
    pipeline = CBI_V14_DataIngestionPipeline()
    
    # Track results for reporting
    results = {
        'forex': {'status': 'not_started', 'issues': []},
        'interest_rates': {'status': 'not_started', 'issues': []},
        'crush_margins': {'status': 'not_started', 'issues': []},
        'export_sales': {'status': 'not_started', 'issues': []}
    }
    
    # 1. Collect Forex data with validation
    try:
        logger.info("Collecting FX data...")
        yahoo_forex = fetch_yahoo_forex_data()
        
        if not yahoo_forex.empty:
            # Process each currency pair separately
            for pair in yahoo_forex['pair'].unique():
                pair_data = yahoo_forex[yahoo_forex['pair'] == pair].copy()
                
                # Set source ID and currency info
                if 'BRL' in pair:
                    source_id = 'yahoo_finance_brl'
                    pair_data['from_currency'] = 'USD'
                    pair_data['to_currency'] = 'BRL'
                elif 'CNY' in pair:
                    source_id = 'yahoo_finance_cny'
                    pair_data['from_currency'] = 'USD'
                    pair_data['to_currency'] = 'CNY'
                elif 'ARS' in pair:
                    source_id = 'yahoo_finance_ars'
                    pair_data['from_currency'] = 'USD'
                    pair_data['to_currency'] = 'ARS'
                else:
                    continue
                
                # Rename columns to match our schema
                pair_data['date'] = pair_data['Date']
                pair_data['rate'] = pair_data['Close']
                
                forex_result = pipeline.ingest_and_validate(source_id, 'forex', pair_data)
                
                if forex_result['status'] == 'success':
                    logger.info(f"Successfully processed {pair}: {forex_result['rows_stored']} rows")
                else:
                    logger.error(f"Failed to process {pair}: {forex_result.get('reason', 'Unknown error')}")
            
            results['forex'] = {'status': 'success', 'issues': []}
        else:
            results['forex'] = {'status': 'failed', 'issues': ['No FX data retrieved']}
            
    except Exception as e:
        logger.error(f"Exception in FX collection: {str(e)}")
        results['forex'] = {'status': 'error', 'issues': [f"Exception: {str(e)}"]}
    
    # 2. Collect Interest Rate data
    try:
        logger.info("Collecting interest rate data...")
        fred_rates = fetch_fred_interest_rates()
        
        if not fred_rates.empty:
            # Rename columns to match schema
            fred_rates['time'] = fred_rates['date']
            
            rates_result = pipeline.ingest_and_validate('fred', 'interest_rates', fred_rates)
            
            if rates_result['status'] == 'success':
                results['interest_rates'] = {'status': 'success', 'issues': []}
                logger.info(f"Successfully processed interest rates: {rates_result['rows_stored']} rows")
            else:
                results['interest_rates'] = {
                    'status': 'failed', 
                    'issues': [rates_result.get('reason', 'Unknown error')]
                }
        else:
            results['interest_rates'] = {'status': 'failed', 'issues': ['No interest rate data retrieved']}
            
    except Exception as e:
        logger.error(f"Exception in interest rate collection: {str(e)}")
        results['interest_rates'] = {'status': 'error', 'issues': [f"Exception: {str(e)}"]}
    
    # 3. Verify Crush Margins (calculated data)
    try:
        logger.info("Verifying crush margins...")
        # Check if crush margin data is current
        query = '''
        SELECT COUNT(*) as count, MAX(date) as latest_date
        FROM `cbi-v14.curated.vw_crush_margins_daily`
        WHERE date >= CURRENT_DATE() - 3
        '''
        
        result = pipeline.client.query(query).to_dataframe()
        if len(result) > 0 and result['count'].iloc[0] > 0:
            results['crush_margins'] = {'status': 'success', 'issues': []}
            logger.info("Crush margins are current")
        else:
            results['crush_margins'] = {'status': 'warning', 'issues': ['Crush margins may be stale']}
            
    except Exception as e:
        logger.error(f"Exception checking crush margins: {str(e)}")
        results['crush_margins'] = {'status': 'error', 'issues': [f"Exception: {str(e)}"]}
    
    # 4. Skip export sales for now (needs proper USDA API implementation)
    results['export_sales'] = {'status': 'skipped', 'issues': ['Awaiting USDA API implementation']}
    
    # Generate summary report
    success_count = sum(1 for r in results.values() if r['status'] == 'success')
    total_count = len([r for r in results.values() if r['status'] != 'skipped'])
    
    report = f"""
CBI-V14 BIDAILY DATA COLLECTION REPORT
=====================================
Date: {datetime.now()}
Success Rate: {success_count}/{total_count}

DETAILED RESULTS:
"""
    
    for data_type, result in results.items():
        report += f"\n{data_type.upper()}: {result['status'].upper()}\n"
        if result['issues']:
            report += "Issues:\n"
            for issue in result['issues']:
                report += f"  - {issue}\n"
        else:
            report += "No issues detected.\n"
    
    # Log the report
    logger.info(report)
    
    # Save report to file
    os.makedirs('/Users/zincdigital/CBI-V14/logs', exist_ok=True)
    with open(f"/Users/zincdigital/CBI-V14/logs/bidaily_collection_{datetime.now().strftime('%Y%m%d_%H%M')}.log", "w") as f:
        f.write(report)
    
    # Return overall success/failure
    overall_success = success_count == total_count
    return overall_success, results


# MAIN EXECUTION
if __name__ == "__main__":
    success, results = bidaily_data_collection()
    
    if success:
        logger.info("✅ Bidaily data collection completed successfully")
        exit(0)
    else:
        logger.error("❌ Bidaily data collection had failures")
        exit(1)
