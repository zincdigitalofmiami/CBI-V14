---
**‚ö†Ô∏è CRITICAL: NO FAKE DATA ‚ö†Ô∏è**
This project uses ONLY real, verified data sources. NO placeholders, NO synthetic data, NO fake values.
All data must come from authenticated APIs, official sources, or validated historical records.
---

# üöÄ EXECUTION SUMMARY: SCHEDULING & DATA PULLS
## Comprehensive Implementation Plan
**Date:** November 7, 2025  
**Status:** Ready for Execution

---

## üìã OVERVIEW

This document summarizes the comprehensive audit and implementation of:
1. **Deep parsing, schema, metadata & calculation audit** (`DEEP_PARSING_SCHEMA_CALCULATION_AUDIT.md`)
2. **Data ingestion pipeline audit** (`DATA_INGESTION_PIPELINE_AUDIT.md`)
3. **Comprehensive scheduling setup** (`scripts/COMPREHENSIVE_SCHEDULING_SETUP.sh`)
4. **Missing data pull script** (`scripts/PULL_ALL_MISSING_DATA.py`)

---

## ‚úÖ COMPLETED WORK

### **1. DEEP AUDIT COMPLETE**

**Files Created:**
- `DEEP_PARSING_SCHEMA_CALCULATION_AUDIT.md` - Comprehensive review of:
  - Parsing logic (date, type conversion, null handling)
  - Schema consistency (naming, data types, required fields)
  - Metadata patterns (provenance UUID, confidence scores, source names)
  - Calculation patterns (multipliers, spreads, ratios, lags, interactions)
  - Routing logic (staging ‚Üí production, deduplication)

**Key Findings:**
- ‚úÖ Standardized metadata pattern exists
- ‚úÖ Modern decorator pattern in `bigquery_utils.py`
- ‚ö†Ô∏è Date parsing inconsistencies (some remove timezone, others keep UTC)
- ‚ö†Ô∏è Schema inconsistencies (some tables missing canonical metadata)
- ‚ö†Ô∏è Hardcoded multipliers in SQL (need verification)
- ‚ö†Ô∏è Some scripts write directly to production without deduplication

**Standardization Recommendations:**
- Use `pd.to_datetime(..., utc=True)` for all date parsing
- Use `pd.to_numeric(..., errors='coerce')` for type conversion
- Add canonical metadata to all base tables
- Use `provenance_uuid` for deduplication
- Document all multipliers and defaults

### **2. PIPELINE AUDIT COMPLETE**

**Files Created:**
- `DATA_INGESTION_PIPELINE_AUDIT.md` - Complete inventory of:
  - 42+ ingestion scripts
  - Scheduled jobs (Cloud Scheduler + cron)
  - Critical gaps (6 identified)
  - Compliance score: 58% (7/12 checklist items)

**Critical Gaps Identified:**
1. ‚ùå China imports not scheduled (21 days stale)
2. ‚ùå RIN prices not scheduled (unknown freshness)
3. ‚ùå Production training data not scheduled (56 days stale!)
4. ‚ùå Trump sentiment quantification not scheduled
5. ‚ùå Big Eight signals not explicitly scheduled
6. ‚ùå Brazil/Argentina premiums missing script

### **3. SCHEDULING SOLUTION CREATED**

**File Created:**
- `scripts/COMPREHENSIVE_SCHEDULING_SETUP.sh` - Comprehensive cron setup:
  - All P0 critical jobs (production training data, China imports, RIN prices)
  - All P1 high priority jobs (Trump sentiment quantification, Big Eight signals)
  - All P2 medium priority jobs (USDA harvest, RFS mandates, volatility, news)
  - All existing scheduled jobs preserved
  - Proper logging and error handling

**New Jobs Added:**
- `update_production_datasets.py`: Daily 5 AM (P0)
- `ingest_china_imports_uncomtrade.py`: Weekdays 8 AM (P0)
- `ingest_epa_rin_prices.py`: Wednesday 9 AM (P0)
- `TRUMP_SENTIMENT_QUANT_ENGINE.py`: Daily 7 AM (P1)
- `collect_neural_data_sources.py`: Daily 6 AM (P1)
- Plus 5 additional missing jobs

### **4. DATA PULL SCRIPT CREATED**

**File Created:**
- `scripts/PULL_ALL_MISSING_DATA.py` - Comprehensive data pull:
  - Checks current data freshness
  - Executes P0/P1/P2 pulls in priority order
  - Re-checks freshness after pulls
  - Proper error handling and logging

---

## üéØ EXECUTION INSTRUCTIONS

### **STEP 1: Review Audit Documents**

```bash
# Review the deep audit
cat DEEP_PARSING_SCHEMA_CALCULATION_AUDIT.md

# Review the pipeline audit
cat DATA_INGESTION_PIPELINE_AUDIT.md
```

### **STEP 2: Pull All Missing Data (IMMEDIATE)**

```bash
# Execute comprehensive data pull
cd /Users/zincdigital/CBI-V14
python3 scripts/PULL_ALL_MISSING_DATA.py
```

**Expected Output:**
- Checks freshness of all critical tables
- Executes P0 pulls (production training data, China imports, RIN prices)
- Executes P1 pulls (Trump sentiment, Big Eight signals)
- Executes P2 pulls (USDA harvest, RFS mandates, volatility, news)
- Re-checks freshness after pulls

**Expected Duration:** 30-60 minutes (depending on data volumes)

### **STEP 3: Setup Comprehensive Scheduling**

```bash
# Run the comprehensive scheduling setup
cd /Users/zincdigital/CBI-V14
./scripts/COMPREHENSIVE_SCHEDULING_SETUP.sh
```

**What It Does:**
- Backs up current crontab
- Adds all missing scheduled jobs
- Preserves existing jobs
- Verifies installation

**Expected Output:**
- Backup saved to `/tmp/cbi_v14_cron_backup_*.txt`
- 30+ Python jobs scheduled
- Summary of new jobs added

### **STEP 4: Verify Scheduling**

```bash
# View current crontab
crontab -l

# Count scheduled jobs
crontab -l | grep -c "python3"

# Check specific jobs
crontab -l | grep "update_production_datasets"
crontab -l | grep "ingest_china_imports"
crontab -l | grep "ingest_epa_rin_prices"
```

### **STEP 5: Monitor First Runs**

```bash
# Monitor logs for first week
tail -f logs/production_refresh.log
tail -f logs/china_imports.log
tail -f logs/rin_prices.log
tail -f logs/trump_quant.log
tail -f logs/big_eight_signals.log

# Check for errors
grep -i error logs/*.log | tail -20
```

---

## üìä SUCCESS CRITERIA

### **Immediate (After Data Pull):**
- ‚úÖ Production training data updated (currently 56 days stale)
- ‚úÖ China imports updated (currently 21 days stale)
- ‚úÖ RIN prices updated (unknown freshness)
- ‚úÖ Trump sentiment quantified (raw data processed)
- ‚úÖ Big Eight signals refreshed

### **After First Week:**
- ‚úÖ All scheduled jobs running without errors
- ‚úÖ Data freshness < 7 days for all critical sources
- ‚úÖ No duplicate records (deduplication working)
- ‚úÖ Proper parsing (no timezone issues, no type errors)
- ‚úÖ Proper routing (staging ‚Üí production working)

### **After First Month:**
- ‚úÖ 100% compliance with plan requirements
- ‚úÖ Zero stale data (>7 days old)
- ‚úÖ All canonical metadata present
- ‚úÖ All calculations verified

---

## ‚ö†Ô∏è CRITICAL NOTES

### **Before Execution:**
1. **Review audit findings** - Understand parsing, schema, calculation issues
2. **Backup current state** - Scripts auto-backup, but verify
3. **Check disk space** - Data pulls may require significant storage
4. **Check API limits** - Some sources have rate limits

### **During Execution:**
1. **Monitor logs** - Watch for parsing errors, schema mismatches
2. **Check BigQuery costs** - Large data pulls may incur costs
3. **Verify deduplication** - Check for duplicate records
4. **Test calculations** - Verify multipliers, spreads, ratios

### **After Execution:**
1. **Verify data freshness** - Run freshness checks
2. **Review error logs** - Fix any parsing/routing issues
3. **Update documentation** - Document any changes made
4. **Schedule follow-up audit** - Review after 1 week

---

## üîß TROUBLESHOOTING

### **Issue: Script Not Found**
```bash
# Check script exists
ls -la scripts/update_production_datasets.py
ls -la ingestion/ingest_china_imports_uncomtrade.py

# If missing, check alternative locations
find . -name "update_production_datasets.py"
find . -name "ingest_china_imports*.py"
```

### **Issue: Permission Denied**
```bash
# Make scripts executable
chmod +x scripts/COMPREHENSIVE_SCHEDULING_SETUP.sh
chmod +x scripts/PULL_ALL_MISSING_DATA.py
```

### **Issue: BigQuery Errors**
```bash
# Check authentication
gcloud auth application-default login

# Check project
gcloud config get-value project

# Check table exists
bq ls cbi-v14:models_v4 | grep production_training_data
```

### **Issue: Parsing Errors**
```bash
# Check logs for specific errors
grep -i "parsing\|datetime\|timezone" logs/*.log

# Review parsing patterns in scripts
grep -r "pd.to_datetime" ingestion/
grep -r "datetime.now" ingestion/
```

### **Issue: Duplicate Records**
```bash
# Check for duplicates
bq query --use_legacy_sql=false "
SELECT provenance_uuid, COUNT(*) as cnt
FROM \`cbi-v14.forecasting_data_warehouse.trump_policy_intelligence\`
GROUP BY provenance_uuid
HAVING cnt > 1
"
```

---

## üìà NEXT STEPS

### **Immediate (This Week):**
1. ‚úÖ Execute data pulls (`PULL_ALL_MISSING_DATA.py`)
2. ‚úÖ Setup scheduling (`COMPREHENSIVE_SCHEDULING_SETUP.sh`)
3. ‚úÖ Monitor first runs
4. ‚úÖ Fix any parsing/schema issues found

### **Short-Term (Next 2 Weeks):**
1. Fix parsing issues in scripts (standardize date/timezone handling)
2. Add canonical metadata to all base tables
3. Add deduplication to direct-write scripts
4. Document all multipliers and defaults
5. Create Brazil/Argentina premiums script

### **Long-Term (Next Month):**
1. Automated testing for ingestion scripts
2. Data freshness monitoring dashboard
3. Alert system for stale data
4. Comprehensive documentation update

---

## üìù FILES CREATED

1. `DEEP_PARSING_SCHEMA_CALCULATION_AUDIT.md` - Deep audit of parsing, schema, metadata, calculations
2. `DATA_INGESTION_PIPELINE_AUDIT.md` - Complete pipeline audit with gaps
3. `scripts/COMPREHENSIVE_SCHEDULING_SETUP.sh` - Comprehensive cron setup script
4. `scripts/PULL_ALL_MISSING_DATA.py` - Missing data pull script
5. `EXECUTION_SUMMARY_SCHEDULING_AND_DATA_PULLS.md` - This document

---

## ‚úÖ CHECKLIST

- [x] Deep audit completed
- [x] Pipeline audit completed
- [x] Scheduling solution created
- [x] Data pull script created
- [ ] **EXECUTE: Pull all missing data**
- [ ] **EXECUTE: Setup comprehensive scheduling**
- [ ] **VERIFY: All jobs scheduled correctly**
- [ ] **MONITOR: First week of runs**
- [ ] **FIX: Any parsing/schema issues found**
- [ ] **DOCUMENT: Any changes made**

---

**READY FOR EXECUTION**  
**Execute Step 2 and Step 3 to complete implementation**

