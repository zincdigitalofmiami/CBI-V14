---
**‚ö†Ô∏è CRITICAL: NO FAKE DATA ‚ö†Ô∏è**
This project uses ONLY real, verified data sources. NO placeholders, NO synthetic data, NO fake values.
All data must come from authenticated APIs, official sources, or validated historical records.
---

# BigQuery Deployment Readiness Checklist
**Date:** November 18, 2025  
**Status:** Pre-Deployment - Action Items Required

---

## ‚úÖ Pre-Deployment Sign-Off

**IMPORTANT:** All items must be signed off before deployment proceeds.

### Validation & Testing
- [ ] **Schema SQL validated** (sqlfluff/dry-run passed)  
  Owner: _____________ Date: _________  
  Evidence: `DEPLOYMENT_VALIDATION_REPORT.md`

- [ ] **Scripts linted** (shellcheck passed)  
  Owner: _____________ Date: _________  
  Evidence: Linting report in validation output

- [ ] **Python tests passed** (unit tests completed)  
  Owner: _____________ Date: _________  
  Evidence: Test execution logs

- [ ] **Environment diff audit reviewed**  
  Owner: _____________ Date: _________  
  Evidence: `BQ_CURRENT_STATE_REPORT.md`

### Safety & Recovery
- [ ] **Idempotency verified** (scripts can run multiple times safely)  
  Owner: _____________ Date: _________  
  Evidence: Code review of deployment scripts

- [ ] **Rollback plan documented**  
  Owner: _____________ Date: _________  
  Evidence: Backup datasets created, restore procedures documented

- [ ] **Backups verified**  
  Owner: _____________ Date: _________  
  Evidence: Backup datasets exist with row counts matching

### Dry Run Execution
- [ ] **Dry run executed and reviewed**  
  Owner: _____________ Date: _________  
  Evidence: `DEPLOYMENT_DRY_RUN_RESULTS.md`  
  Command: `./scripts/deployment/deploy_bq_schema.sh --dry-run`

- [ ] **Validation script dry-run passed**  
  Owner: _____________ Date: _________  
  Evidence: Validation output logs

### Approvals
- [ ] **Technical Lead Approval**  
  Name: _____________ Date: _________

- [ ] **Deployment Window Scheduled**  
  Date/Time: _____________ Duration: _________

---

## üéØ Current State Summary

‚úÖ **Confirmed:** Assessment accurate - key gaps identified  
‚ùå **Missing:** 4 datasets, 40+ tables, overlay views, live folders  
‚ö†Ô∏è **Legacy:** Old tables with unprefixed columns need migration

---

## üìã Pre-Deployment Actions Required

### Phase 1: Create Missing Datasets (4 datasets)

**Action:** Run `PRODUCTION_READY_BQ_SCHEMA.sql` to create:

```sql
CREATE SCHEMA IF NOT EXISTS regimes;
CREATE SCHEMA IF NOT EXISTS drivers;
CREATE SCHEMA IF NOT EXISTS dim;
CREATE SCHEMA IF NOT EXISTS ops;
```

**Status:** ‚ùå **NOT DONE**  
**Command:** `bq query --use_legacy_sql=false < PRODUCTION_READY_BQ_SCHEMA.sql`

---

### Phase 2: Create Missing Tables (40+ tables)

#### `market_data` Dataset (9 tables missing)
- ‚ùå `databento_futures_ohlcv_1m`
- ‚ùå `databento_futures_ohlcv_1d`
- ‚ùå `databento_futures_continuous_1d`
- ‚ùå `roll_calendar`
- ‚ùå `futures_curve_1d`
- ‚ùå `cme_indices_eod`
- ‚ùå `fx_daily`
- ‚ùå `orderflow_1m`
- ‚ùå `yahoo_zl_historical_2000_2010`

**Action:** Run schema script - these tables are defined in `PRODUCTION_READY_BQ_SCHEMA.sql`

#### `raw_intelligence` Dataset (9 tables missing)
- ‚ùå `fred_economic`
- ‚ùå `eia_biofuels`
- ‚ùå `usda_granular`
- ‚ùå `weather_segmented`
- ‚ùå `weather_weighted`
- ‚ùå `cftc_positioning`
- ‚ùå `policy_events`
- ‚ùå `volatility_daily`
- ‚ùå `news_intelligence`
- ‚ùå `news_bucketed`

**Action:** Run schema script

#### `signals` Dataset (6 tables missing)
- ‚ùå `big_eight_live` ‚ö†Ô∏è **CRITICAL** (dashboard dependency)
- ‚ùå `calendar_spreads_1d`
- ‚ùå `crush_oilshare_daily`
- ‚ùå `energy_proxies_daily`
- ‚ùå `calculated_signals`
- ‚ùå `hidden_relationship_signals` (exists in schema, needs creation)

**Action:** Run schema script

#### `training` Dataset (12 MES tables missing)
- ‚ùå `mes_training_prod_allhistory_1min`
- ‚ùå `mes_training_prod_allhistory_5min`
- ‚ùå `mes_training_prod_allhistory_15min`
- ‚ùå `mes_training_prod_allhistory_30min`
- ‚ùå `mes_training_prod_allhistory_1hr`
- ‚ùå `mes_training_prod_allhistory_4hr`
- ‚ùå `mes_training_prod_allhistory_1d`
- ‚ùå `mes_training_prod_allhistory_7d`
- ‚ùå `mes_training_prod_allhistory_30d`
- ‚ùå `mes_training_prod_allhistory_3m`
- ‚ùå `mes_training_prod_allhistory_6m`
- ‚ùå `mes_training_prod_allhistory_12m`

**Action:** Run schema script

#### `features` Dataset (1 table needs rebuild)
- ‚ö†Ô∏è `master_features` - **EXISTS as `master_features_canonical`** but has OLD column names
  - Current: `yahoo_open`, `alpha_open` (not prefixed)
  - Required: `yahoo_zl_open`, `databento_zl_open` (prefixed)

**Action:** 
1. Create new `master_features` table with correct schema
2. Migrate/rebuild data with prefixed columns
3. Update all references

#### `regimes` Dataset (1 table missing)
- ‚ùå `market_regimes`

**Action:** Run schema script

#### `drivers` Dataset (2 tables missing)
- ‚ùå `primary_drivers`
- ‚ùå `meta_drivers`

**Action:** Run schema script

#### `neural` Dataset (1 table missing)
- ‚ùå `feature_vectors`

**Action:** Run schema script

#### `dim` Dataset (3 tables missing)
- ‚ùå `instrument_metadata`
- ‚ùå `production_weights`
- ‚ùå `crush_conversion_factors`

**Action:** Run schema script

#### `ops` Dataset (2 tables missing)
- ‚ùå `ingestion_runs`
- ‚ùå `data_quality_events`

**Action:** Run schema script

---

### Phase 3: Create External Drive Folders

**Action:** Create live data collection folders

```bash
mkdir -p "/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/live"
mkdir -p "/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/live_continuous"
```

**Status:** ‚ùå **NOT DONE**

**Structure:**
```
TrainingData/
‚îú‚îÄ‚îÄ live/
‚îÇ   ‚îú‚îÄ‚îÄ ZL/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 1m/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ date=YYYY-MM-DD/
‚îÇ   ‚îú‚îÄ‚îÄ MES/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 1m/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ date=YYYY-MM-DD/
‚îÇ   ‚îî‚îÄ‚îÄ ES/
‚îÇ       ‚îî‚îÄ‚îÄ 1m/
‚îÇ           ‚îî‚îÄ‚îÄ date=YYYY-MM-DD/
‚îî‚îÄ‚îÄ live_continuous/
    ‚îú‚îÄ‚îÄ ZL/
    ‚îÇ   ‚îî‚îÄ‚îÄ 1m/
    ‚îÇ       ‚îî‚îÄ‚îÄ date=YYYY-MM-DD/
    ‚îî‚îÄ‚îÄ MES/
        ‚îî‚îÄ‚îÄ 1m/
            ‚îî‚îÄ‚îÄ date=YYYY-MM-DD/
```

---

### Phase 4: Create Overlay Views (31 views)

**Action:** Create `scripts/deployment/create_overlay_views.sql`

**Views Required:**
- 17 API overlay views (`api.vw_futures_overlay_*`)
- 5 Prediction overlay views (`predictions.vw_zl_*_latest`)
- 1 Regime overlay view (`regimes.vw_live_regime_overlay`)
- 5 Compatibility views (`training.vw_zl_training_*`)
- 1 Signals composite view (`signals.vw_big_seven_signals`)
- 2 MES overlay views (`features.vw_mes_*`)

**Status:** ‚ùå **NOT DONE** (SQL file needs creation)

---

### Phase 5: Update Legacy Tables

#### Tables Needing Migration/Rebuild

1. **`features.master_features_canonical`** ‚Üí **`features.master_features`**
   - **Issue:** Old column names (`yahoo_open`, `alpha_open`)
   - **Action:** Rebuild with prefixed columns (`yahoo_zl_open`, `databento_zl_open`)
   - **Method:** 
     - Create new table with correct schema
     - Migrate data with column mapping
     - Update all view references

2. **`neural.vw_big_eight_signals`** (VIEW)
   - **Issue:** References old signal views, not `signals.big_eight_live` table
   - **Action:** Update view to read from `signals.big_eight_live` table
   - **Or:** Drop view and use table directly

3. **ZL Training Tables** (5 tables exist with old naming)
   - **Status:** ‚úÖ Tables exist but need verification
   - **Action:** Verify schema matches new naming convention
   - **Check:** Column names, partitioning, clustering

---

### Phase 6: Data Migration Strategy

#### Step 1: Backup Legacy Tables
```bash
# Create backup datasets
bq mk cbi-v14:market_data_backup_20251118
bq mk cbi-v14:features_backup_20251118
bq mk cbi-v14:training_backup_20251118
```

#### Step 2: Map Old ‚Üí New Tables
- `yahoo_finance_enhanced` ‚Üí `market_data.yahoo_zl_historical_2000_2010`
- `master_features_canonical` ‚Üí `features.master_features` (with column mapping)
- Old signal views ‚Üí `signals.big_eight_live` table

#### Step 3: Migrate Data
- Use `bq cp` for simple copies
- Use `bq query` with `INSERT INTO ... SELECT` for column mapping
- Verify row counts match

#### Step 4: Update References
- Update all views to point to new tables
- Update dashboard queries
- Update training export scripts

---

## ‚úÖ Deployment Readiness Checklist

### Prerequisites
- [ ] Run `PRODUCTION_READY_BQ_SCHEMA.sql` to create all datasets and tables
- [ ] Create external drive folders (`live/`, `live_continuous/`)
- [ ] Create overlay views SQL script
- [ ] Backup legacy tables to `*_backup_YYYYMMDD` datasets

### Data Migration
- [ ] Migrate `master_features_canonical` ‚Üí `master_features` with prefixed columns
- [ ] Verify ZL training tables have correct schema
- [ ] Map and migrate old signal views ‚Üí `signals.big_eight_live`
- [ ] Verify row counts match between old and new tables

### View Updates
- [ ] Update `neural.vw_big_eight_signals` to use new table
- [ ] Create all 31 overlay views
- [ ] Test view queries return expected results

### Validation
- [ ] Verify all 12 datasets exist
- [ ] Verify all 55+ tables exist with correct schema
- [ ] Verify partitioning and clustering are correct
- [ ] Verify all views can query successfully
- [ ] Test Big 8 refresh job end-to-end

### Documentation
- [ ] Update table mapping matrix with old ‚Üí new mappings
- [ ] Document migration steps taken
- [ ] Update deployment scripts

---

## üöÄ Deployment Command Sequence

### Step 1: Create Schema
```bash
cd /Users/kirkmusick/Documents/GitHub/CBI-V14
bq query --use_legacy_sql=false < PRODUCTION_READY_BQ_SCHEMA.sql
```

### Step 2: Create Folders
```bash
mkdir -p "/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/live"
mkdir -p "/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/live_continuous"
```

### Step 3: Create Overlay Views
```bash
# After creating scripts/deployment/create_overlay_views.sql
bq query --use_legacy_sql=false < scripts/deployment/create_overlay_views.sql
```

### Step 4: Migrate Data
```bash
# Run migration scripts (to be created)
python3 scripts/migration/migrate_master_features.py
python3 scripts/migration/migrate_signals.py
```

### Step 5: Validate
```bash
python3 scripts/validation/validate_bq_deployment.py
```

---

## ‚ö†Ô∏è Critical Dependencies

1. **`signals.big_eight_live`** - Dashboard reads from this (must exist)
2. **`features.master_features`** - Training exports read from this (must have prefixed columns)
3. **All MES training tables** - Training pipeline requires all 12 horizons
4. **Overlay views** - Dashboard queries depend on these views

---

## üìä Current vs. Required State

| Component | Current | Required | Status |
|-----------|---------|----------|--------|
| Datasets | 8/12 | 12/12 | ‚ùå 4 missing |
| Market Data Tables | 4 legacy | 9 new | ‚ùå All missing |
| Training Tables | 5 ZL | 17 total | ‚ùå 12 MES missing |
| Signals Tables | Views only | 6 tables | ‚ùå All missing |
| Features Table | Old cols | Prefixed | ‚ö†Ô∏è Needs rebuild |
| Overlay Views | 0 | 31 | ‚ùå All missing |
| Live Folders | 0 | 2 | ‚ùå Missing |

---

## üéØ Ready for Deployment?

**Current Status:** ‚ùå **NOT READY**

**Blockers:**
1. Missing 4 datasets (regimes, drivers, dim, ops)
2. Missing 40+ tables
3. Missing overlay views
4. Missing live folders
5. Legacy tables need migration

**Estimated Time to Ready:**
- Schema creation: 15 minutes
- Folder creation: 2 minutes
- Overlay views creation: 30 minutes
- Data migration: 2-4 hours (depending on data volume)
- Validation: 30 minutes

**Total:** ~3-5 hours to deployment-ready state

---

## üîí Deployment Approval Gate

**STOP:** Before executing deployment, verify all sign-offs above are complete.

**Run pre-flight validation:**
```bash
./scripts/deployment/pre_flight_validation.sh
```

**If all checks pass:**
```bash
# 1. Dry run first
./scripts/deployment/deploy_bq_schema.sh --dry-run

# 2. Review dry run results

# 3. Execute actual deployment
./scripts/deployment/deploy_bq_schema.sh
```

---
**Next Action:** Complete pre-deployment sign-offs above, then run pre-flight validation

**See:** `DEPLOYMENT_EXECUTION_PLAN.md` for step-by-step execution guide

