---
**‚ö†Ô∏è CRITICAL: NO FAKE DATA ‚ö†Ô∏è**
This project uses ONLY real, verified data sources. NO placeholders, NO synthetic data, NO fake values.
All data must come from authenticated APIs, official sources, or validated historical records.
---

# Continuous Connection BigQuery Cost Analysis
**Date:** November 18, 2025  
**Question:** What if it's a continuous connection (real-time streaming) instead of batch loads?

---

## üéØ QUICK ANSWER

**Cost: Still $0.00-0.09/month** (same as batch loads)

A continuous connection uses **BigQuery Streaming Inserts**, which are:
- **FREE** (first 2 TB/month)
- **Your usage:** ~6 GB/month ‚Üí **$0.00**
- **Storage:** Same as batch ($0.04-0.09/month)
- **TOTAL:** **$0.04-0.09/month** (no change)

**Key Difference:** Continuous connection = same cost, but **real-time data** (no 15-minute delay).

---

## üìä CONTINUOUS CONNECTION OPTIONS

### Option 1: BigQuery Streaming Inserts (Real-Time)

**How It Works:**
```python
from google.cloud import bigquery

client = bigquery.Client(project='cbi-v14')

# Continuous connection - push data as it arrives
def stream_data_to_bigquery(rows):
    errors = client.insert_rows_json(
        "cbi-v14.market_data.databento_futures_ohlcv_1m",
        rows
    )
    if errors:
        print(f"Errors: {errors}")

# Called continuously as data arrives from DataBento
while True:
    new_data = databento_client.get_latest_bars()
    rows = convert_to_bigquery_rows(new_data)
    stream_data_to_bigquery(rows)
    time.sleep(1)  # Push every second
```

**Pricing:**
- **First 2 TB/month:** **FREE** ‚úÖ
- **After 2 TB:** $0.05 per 200 MB
- **Your usage:** ~6 GB/month ‚Üí **$0.00**

**Data Availability:**
- **Latency:** <1 second (real-time)
- **Queryable:** Immediately (no batch job wait)

---

### Option 2: Pub/Sub ‚Üí BigQuery (Managed Streaming)

**How It Works:**
```
DataBento API ‚Üí Cloud Pub/Sub ‚Üí BigQuery Streaming
```

**Architecture:**
```python
# Publisher (collector script)
from google.cloud import pubsub_v1

publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path('cbi-v14', 'databento-stream')

# Push to Pub/Sub as data arrives
while True:
    new_data = databento_client.get_latest_bars()
    message = json.dumps(new_data)
    publisher.publish(topic_path, message.encode())
    time.sleep(1)

# Subscriber (BigQuery subscription - managed by GCP)
# Automatically streams Pub/Sub messages to BigQuery
```

**Pricing:**
- **Pub/Sub:** $0.40 per million messages (first 10 GB/month FREE)
- **BigQuery Streaming:** Same as Option 1 (FREE under 2 TB)
- **Your usage:** ~6 GB/month ‚Üí **$0.00** (under 10 GB free tier)

**Data Availability:**
- **Latency:** <1 second (real-time)
- **Reliability:** Higher (Pub/Sub handles retries, batching)

---

### Option 3: Dataflow ‚Üí BigQuery (Enterprise Streaming)

**How It Works:**
```
DataBento API ‚Üí Cloud Dataflow ‚Üí BigQuery
```

**Pricing:**
- **Dataflow:** $0.05-0.10 per GB processed
- **Your usage:** ~6 GB/month ‚Üí **$0.30-0.60/month**
- **Overkill for your use case** ‚ùå

**When to Use:**
- High volume (>100 GB/month)
- Complex transformations
- Multiple destinations
- **Not needed for your 6 GB/month**

---

## üí∞ COST COMPARISON: BATCH vs. CONTINUOUS

### Batch Loads (Every 15 Minutes)

| Component | Cost | Latency |
|-----------|------|---------|
| **Data Ingestion** | $0.00 (FREE) | 15 minutes |
| **Storage** | $0.04-0.09/month | Immediate after load |
| **Queries** | $0.00 | Immediate |
| **TOTAL** | **$0.04-0.09/month** | 15-minute delay |

### Continuous Streaming (Real-Time)

| Component | Cost | Latency |
|-----------|------|---------|
| **Data Ingestion** | $0.00 (FREE under 2 TB) | <1 second |
| **Storage** | $0.04-0.09/month | Immediate |
| **Queries** | $0.00 | Immediate |
| **TOTAL** | **$0.04-0.09/month** | Real-time |

**Verdict:** Same cost, but continuous = real-time data.

---

## üìà CONTINUOUS CONNECTION VOLUME ANALYSIS

### DataBento Live Feed (1-Minute Bars)

**Symbols:** ZL, MES, ES (3 primary) + 7 secondary = 10 symbols  
**Bars per symbol:** 1,440/day (24 hours √ó 60 minutes)  
**Total bars/day:** 14,400 bars  
**Data per bar:** ~100 bytes (OHLCV + metadata)  
**Daily volume:** ~1.4 MB  
**Monthly volume:** ~42 MB (0.042 GB)

### With Continuous Connection

**Streaming inserts per day:**
- Option A: 1 insert per bar = 14,400 inserts/day
- Option B: Batch 60 bars/minute = 1,440 inserts/day (recommended)
- Option C: Batch 1,440 bars/hour = 24 inserts/day

**Recommended:** Batch 60 bars/minute (1 insert per minute)
- **Inserts/day:** 1,440
- **Inserts/month:** 43,200
- **Data/month:** ~42 MB (0.042 GB)
- **Cost:** **$0.00** (under 2 TB free tier)

---

## üîÑ CONTINUOUS CONNECTION ARCHITECTURE

### Pattern 1: Direct Streaming (Simplest)

```python
#!/usr/bin/env python3
"""
Continuous DataBento ‚Üí BigQuery streaming
"""

import databento
from google.cloud import bigquery
import time
from datetime import datetime

# Initialize clients
db = databento.Historical('db-cSwxrJxRGGbqSBX74iuh9gqPrF4xf')
bq_client = bigquery.Client(project='cbi-v14')

# Continuous connection
def stream_continuously():
    symbols = ['ZL.FUT', 'MES.FUT', 'ES.FUT']
    buffer = []
    
    while True:
        # Get latest bars from DataBento
        for symbol in symbols:
            latest = db.timeseries.get_range(
                dataset='GLBX.MDP3',
                symbols=symbol,
                schema='ohlcv-1m',
                start=datetime.now() - timedelta(minutes=1),
                end=datetime.now()
            )
            
            if latest:
                # Convert to BigQuery format
                rows = convert_to_bigquery_rows(latest)
                buffer.extend(rows)
        
        # Batch insert every minute (60 bars)
        if len(buffer) >= 60:
            errors = bq_client.insert_rows_json(
                "cbi-v14.market_data.databento_futures_ohlcv_1m",
                buffer
            )
            if errors:
                print(f"Errors: {errors}")
            buffer = []
        
        time.sleep(60)  # Check every minute

# Run continuously
if __name__ == "__main__":
    stream_continuously()
```

**Cost:** $0.00 (streaming inserts FREE under 2 TB)

---

### Pattern 2: Pub/Sub ‚Üí BigQuery (More Reliable)

```python
#!/usr/bin/env python3
"""
DataBento ‚Üí Pub/Sub ‚Üí BigQuery (managed streaming)
"""

import databento
from google.cloud import pubsub_v1
import json

# Initialize
db = databento.Historical('db-cSwxrJxRGGbqSBX74iuh9gqPrF4xf')
publisher = pubsub_v1.PublisherClient()
topic_path = publisher.topic_path('cbi-v14', 'databento-stream')

# Continuous connection
def stream_to_pubsub():
    symbols = ['ZL.FUT', 'MES.FUT', 'ES.FUT']
    
    while True:
        for symbol in symbols:
            latest = db.timeseries.get_range(
                dataset='GLBX.MDP3',
                symbols=symbol,
                schema='ohlcv-1m',
                start=datetime.now() - timedelta(minutes=1),
                end=datetime.now()
            )
            
            if latest:
                # Push to Pub/Sub
                message = json.dumps(convert_to_dict(latest))
                publisher.publish(topic_path, message.encode())
        
        time.sleep(60)

# BigQuery subscription (configured in GCP Console)
# Automatically streams Pub/Sub ‚Üí BigQuery
```

**Cost:** $0.00 (Pub/Sub free tier: 10 GB/month, you use 0.042 GB)

---

## ‚ö†Ô∏è CONSIDERATIONS FOR CONTINUOUS CONNECTION

### 1. Rate Limits

**BigQuery Streaming Inserts:**
- **Limit:** 100,000 rows/second per table
- **Your usage:** ~1,440 rows/minute = 24 rows/second
- **Headroom:** 99.98% available ‚úÖ

**Pub/Sub:**
- **Limit:** 10,000 messages/second per topic
- **Your usage:** ~1,440 messages/minute = 24 messages/second
- **Headroom:** 99.8% available ‚úÖ

**No rate limit concerns.**

---

### 2. Connection Overhead

**Direct Streaming:**
- **Connection:** HTTP/2 (stateless, no persistent connection)
- **Overhead:** Minimal (just API calls)
- **Cost:** $0.00 (no connection charges)

**Pub/Sub:**
- **Connection:** Managed service (GCP handles)
- **Overhead:** Minimal (just message publishing)
- **Cost:** $0.00 (under free tier)

**No connection costs.**

---

### 3. Deduplication Strategy

**Challenge:** Continuous streaming can create duplicates if:
- Network retries
- Script restarts
- Multiple collectors

**Solution:** Use `insertId` for deduplication

```python
import hashlib
import json

def get_insert_id(row):
    # Create unique ID from row data
    key = f"{row['symbol']}_{row['timestamp']}_{row['close']}"
    return hashlib.md5(key.encode()).hexdigest()

# Include insertId in streaming insert
rows_with_ids = [
    {
        **row,
        'insertId': get_insert_id(row)  # Prevents duplicates
    }
    for row in rows
]

errors = bq_client.insert_rows_json(
    "cbi-v14.market_data.databento_futures_ohlcv_1m",
    rows_with_ids
)
```

**Cost:** $0.00 (deduplication is free)

---

### 4. Storage Growth (Same as Batch)

**With Continuous Streaming:**
- **Data written:** ~42 MB/month (same as batch)
- **Storage growth:** 225 MB/month (with MERGE/dedup)
- **Cost:** $0.04-0.09/month (same as batch)

**Key:** Still need daily deduplication job (same as batch).

---

## üìä COST BREAKDOWN: CONTINUOUS CONNECTION

### Scenario: Real-Time Streaming (1-Minute Bars)

| Component | Volume | Cost |
|-----------|--------|------|
| **Streaming Inserts** | 42 MB/month | **$0.00** (under 2 TB) |
| **Pub/Sub** (if used) | 42 MB/month | **$0.00** (under 10 GB) |
| **Storage** | 1.8-4.3 GB | **$0.04-0.09/month** |
| **Queries** | 23 GB/month | **$0.00** (under 1 TB) |
| **TOTAL** | ‚Äî | **$0.04-0.09/month** |

**Same cost as batch loads!**

---

## üéØ RECOMMENDATION

### For Your Use Case (6 GB/month, 10 symbols)

**Option 1: Direct Streaming** ‚úÖ **RECOMMENDED**
- **Cost:** $0.04-0.09/month
- **Latency:** <1 second
- **Complexity:** Low (just Python script)
- **Reliability:** Good (with retry logic)

**Option 2: Pub/Sub ‚Üí BigQuery** (If you need higher reliability)
- **Cost:** $0.04-0.09/month
- **Latency:** <1 second
- **Complexity:** Medium (Pub/Sub setup)
- **Reliability:** Excellent (managed service)

**Option 3: Batch Loads** (If 15-minute delay is acceptable)
- **Cost:** $0.04-0.09/month
- **Latency:** 15 minutes
- **Complexity:** Low (cron job)
- **Reliability:** Good

**Verdict:** Continuous connection = same cost, real-time data. **Worth it if you need real-time.**

---

## ‚úÖ SUMMARY

### Continuous Connection Cost

**Data Ingestion:**
- Streaming inserts: **$0.00** (FREE under 2 TB/month)
- Pub/Sub (if used): **$0.00** (FREE under 10 GB/month)
- Your usage: ~42 MB/month ‚Üí **$0.00**

**Storage:**
- Same as batch: $0.04-0.09/month
- Growth: 225 MB/month (with dedup)

**Queries:**
- Same as batch: $0.00 (under 1 TB)

**TOTAL:** **$0.04-0.09/month** (same as batch)

### Key Differences

| Aspect | Batch Loads | Continuous Connection |
|--------|-------------|----------------------|
| **Cost** | $0.04-0.09/month | $0.04-0.09/month |
| **Latency** | 15 minutes | <1 second |
| **Complexity** | Low (cron) | Medium (script) |
| **Reliability** | Good | Excellent (with Pub/Sub) |

**Bottom Line:** Continuous connection costs the same as batch loads, but gives you **real-time data**. If you need real-time, use continuous streaming. If 15-minute delay is acceptable, batch loads are simpler.

---

**Reference Documents:**
- `LIVE_FEED_BIGQUERY_COST.md` - Batch load costs
- `COST_WITH_DATA_COLLECTION.md` - Full collection analysis





