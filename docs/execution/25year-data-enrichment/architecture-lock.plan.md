<!-- 54cc6542-3f87-4cff-8720-b5d318bccad7 296fa9d9-0f21-45d0-95a7-2bc790577525 -->
# Complete 25-Year Data Enrichment - Institutional Grade

## Philosophy

**"Fully loaded, 25 years, all the fancy math, production-grade guardrails"**

- Mac M4 = compute engine (free, fast, deterministic)
- BigQuery = thin dashboard read layer only
- Staging + quarantine = no bad data reaches training
- Declarative join specs with automated tests
- QA gates block between every phase
- Single-pass feature build (efficient)
- Pre-flight harness ensures metric parity

---

## Related Plans & Documentation

**üß† Complete System Knowledge**: See `COMPLETE_SYSTEM_KNOWLEDGE.md` for EVERYTHING you need to know:
- Core mission and architecture
- Data source structure (local drive, not BigQuery)
- Zero tolerance fake data policy
- All scripts and their purposes
- User personas (Chris, Kevin)
- Page structure and buildout status
- Key decisions and rationale
- Quick reference guide

**üìã Page Buildout Roadmap**: See `../../PAGE_BUILDOUT_ROADMAP.md` for complete dashboard page specifications, including:
- Completed pages (Dashboard, Legislative, Strategy, Vegas Intel)
- Planned pages (Sentiment page - future buildout)
- Page priority matrix
- Feature specifications

**üìä System Understanding**: See `../../SYSTEM_UNDERSTANDING_COMPLETE.md` for:
- Complete system architecture
- User personas (Chris, Kevin)
- Page structure and purposes
- Data flow diagrams

**üí° Enhancement Ideas**: See `../../plans/CRYSTAL_BALL_ENHANCEMENT_IDEAS.md` for strategic enhancement concepts:
- Proactive intelligence ("reverse Google search")
- Market pulse indicators (red/yellow/green standardization)
- Advanced correlation analysis
- Conversational interface concepts
- Cost avoidance tracking & ROI
- Enterprise scalability considerations
- **Note**: Reference ideas for future consideration, not blocking dependencies

**üì° Data Sources Catalog**: See `../../plans/DATA_SOURCES_REFERENCE.md` for comprehensive data source catalog:
- Weather and climate APIs (INMET, NOAA, Argentina SMN, Copernicus, Meteomatics)
- Economic data sources (FRED, Treasury, BLS, Central Banks, Economic Calendars)
- Market data APIs (TradingEconomics, Polygon.io, Alpha Vantage)
- Trade/policy/news sources (Federal Register, government agencies, think tanks)
- Social media sources (Truth Social, Facebook, Reddit via ScrapeCreators)
- Shipping/logistics (MarineTraffic)
- China soybean import data alternatives
- South American harvest data sources
- **Security Notes**: API key management recommendations and hardcoded key warnings

---

## Directory Structure (External Drive)

```
/Volumes/Satechi Hub/Projects/CBI-V14/
‚îú‚îÄ‚îÄ TrainingData/
‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Immutable source zone (API downloads, never edited)
‚îÇ   ‚îú‚îÄ‚îÄ staging/       # Validated, conformed (units/timezones/dtypes fixed)
‚îÇ   ‚îú‚îÄ‚îÄ features/      # Engineered signals (Big 8, tech, macro, weather)
‚îÇ   ‚îú‚îÄ‚îÄ labels/        # Forward targets by horizon (7d, 30d, 90d, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ exports/       # Final training parquet per horizon
‚îÇ   ‚îî‚îÄ‚îÄ quarantine/    # Failed validations (human triage)
‚îú‚îÄ‚îÄ registry/
‚îÇ   ‚îú‚îÄ‚îÄ feature_registry.json    # Semantic metadata (reliability, impact)
‚îÇ   ‚îú‚îÄ‚îÄ join_spec.yaml           # Declarative joins with tests
‚îÇ   ‚îî‚îÄ‚îÄ regime_calendar.parquet  # Static regime assignments
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ ingest/        # API pulls ‚Üí raw/ (or quarantine/)
    ‚îú‚îÄ‚îÄ conform/       # raw/ ‚Üí staging/ (validation)
    ‚îú‚îÄ‚îÄ features/      # staging/ ‚Üí features/ (calculations)
    ‚îú‚îÄ‚îÄ labels/        # Date offsets ‚Üí labels/
    ‚îú‚îÄ‚îÄ assemble/      # features + labels ‚Üí exports/
    ‚îî‚îÄ‚îÄ qa/            # Automated QA gates
```

---

## Phase 0: Setup + Forensic Inventory (2 hours)

### 0.1 Create Directory Structure

```bash
cd "/Volumes/Satechi Hub/Projects/CBI-V14"
mkdir -p TrainingData/{raw,staging,features,labels,exports,quarantine}
mkdir -p registry
mkdir -p scripts/{ingest,conform,features,labels,assemble,qa}
```

### 0.2 Forensic Audit of Existing Data

Inventory what's already on external drive, identify keep-vs-replace decisions.

**Script:** `scripts/qa/forensic_audit.py`

```python
import pandas as pd
from pathlib import Path

drive = Path("/Volumes/Satechi Hub/Projects/CBI-V14")

# Check existing exports
for f in (drive / "TrainingData/exports").glob("*.parquet"):
    df = pd.read_parquet(f)
    print(f"{f.name}: {len(df)} rows, {len(df.columns)} cols, "
          f"{df['date'].min()} to {df['date'].max()}")

# Decision: Keep yahoo_finance_comprehensive, archive old regime files
```

**Output:** Gap analysis - what to pull, what to keep.

---

## Phase 1: Data Collection with Validation (10 hours)

**Strategy:** Fill Tier 1 critical gaps. Turn watchlists into dated, numeric time series.

### Tier 1 Critical Gaps (Must-Have)

**1. China Demand Composite (8 sub-series)**

- Monthly China soy imports (FAS/USDA)
- Weekly purchase pace
- Dalian vs CBOT basis spread
- State reserve actions (Sinograin/COFCO announcements)
- China crush margins
- Hog herd size (meal demand proxy)
- ASF outbreak severity indices
- Tariff/quota event timeline

**2. Tariff Intelligence (Dated Events)**

- Section 301 timelines (announcement vs effective dates)
- Exclusion lists (product-level exemptions)
- Retaliatory schedules (China's response)
- Trade deal milestones

**3. Biofuel Policy & Prices**

- EIA biodiesel production (weekly)
- EIA renewable diesel production
- RIN prices (D4, D5, D6)
- LCFS credit prices (California/Oregon)
- Mandate paths (RFS, state LCFS, SAF, Brazil RenovaBio, Indonesia B35‚ÜíB40)

**4. Substitute Oils (Full History)**

- Palm oil (FCPO/MPOB continuous series)
- Sunflower oil (Ukraine/Russia exports + prices)
- Rapeseed/canola oil (ICE/Euronext)
- Corn oil (distillers oil)
- FOB spreads between oils

### 1.1 Create Feature Registry (Metadata)

**File:** `registry/feature_registry.json`

```json
{
  "metadata_version": "1.0",
  "last_updated": "2025-11-16",
  "features": {
    "fed_funds_rate": {
      "source": "FRED:DFF",
      "reliability": 0.98,
      "policy_impact": 85,
      "category": "macro_rates",
      "update_frequency": "daily",
      "available_from": "2000-01-01"
    },
    "vix": {
      "source": "FRED:VIXCLS",
      "reliability": 0.95,
      "policy_impact": 90,
      "category": "volatility",
      "update_frequency": "daily",
      "available_from": "2000-01-01"
    },
    "feature_tariff_threat": {
      "source": "computed_big8",
      "reliability": 0.75,
      "policy_impact": 95,
      "category": "big8_signals",
      "update_frequency": "daily",
      "available_from": "2020-01-01"
    }
  }
}
```

### 1.2 Collect FRED Data (with retry logic)

**Script:** `scripts/ingest/collect_fred_historical.py`

```python
#!/usr/bin/env python3
import requests
import pandas as pd
import time
from pathlib import Path

FRED_API_KEY = "dc195c8658c46ee1df83bcd4fd8a690b"
RAW_DIR = Path("/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/raw")
CACHE_DIR = RAW_DIR / ".cache"
CACHE_DIR.mkdir(exist_ok=True)

def retry_with_backoff(func, max_retries=3):
    """Retry with exponential backoff"""
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                # Check cache
                cache_file = CACHE_DIR / f"{func.__name__}_last_good.pkl"
                if cache_file.exists():
                    print(f"‚ö†Ô∏è  Using cached value from {cache_file}")
                    return pd.read_pickle(cache_file)
                raise e
            wait = 2 ** attempt
            print(f"‚ùå Attempt {attempt + 1} failed, retrying in {wait}s...")
            time.sleep(wait)

def fetch_fred_series(series_id, series_name):
    """Fetch with retry and caching (FIXED: env secrets + dual cache)"""
    def _fetch():
        # FIX #4: Use environment variable, not hardcoded
        api_key = os.getenv("FRED_API_KEY")
        if not api_key:
            raise RuntimeError("FRED_API_KEY not set in environment")
        
        response = requests.get(
            "https://api.stlouisfed.org/fred/series/observations",
            params={'series_id': series_id, 'api_key': api_key, 
                    'file_type': 'json', 'observation_start': '2000-01-01'}
        )
        response.raise_for_status()
        data = response.json()
        df = pd.DataFrame(data['observations'])
        df['date'] = pd.to_datetime(df['date'])
        df['value'] = pd.to_numeric(df['value'], errors='coerce')
        
        # FIX #5: Save BOTH series-specific and last-good cache
        pd.to_pickle(df, CACHE_DIR / f"fred_{series_id}.pkl")
        pd.to_pickle(df, CACHE_DIR / f"last_good_{series_id}.pkl")
        
        return df[['date', 'value']].rename(columns={'value': series_name})
    
    # FIX #5: Check both cache locations
    def _get_cached():
        for cache_file in [CACHE_DIR / f"fred_{series_id}.pkl", 
                          CACHE_DIR / f"last_good_{series_id}.pkl"]:
            if cache_file.exists():
                return pd.read_pickle(cache_file)
        return None
    
    return retry_with_backoff(_fetch, fallback_func=_get_cached)

# FRED series to pull (30+)
FRED_SERIES = {
    'DFF': 'fed_funds_rate',
    'DGS10': 'treasury_10y',
    'DGS2': 'treasury_2y',
    'T10Y2Y': 'yield_curve_spread',
    'CPIAUCSL': 'cpi_all',
    'VIXCLS': 'vix',
    'DTWEXBGS': 'usd_broad_index',
    'DCOILWTICO': 'crude_oil_wti',
    'BALTIC': 'baltic_dry_index',
    # ... add all 30+ series
}

# Fetch all
all_series = []
for series_id, name in FRED_SERIES.items():
    df = fetch_fred_series(series_id, name)
    print(f"‚úÖ {name}: {len(df)} rows")
    all_series.append(df)

# Merge
fred_master = all_series[0]
for df in all_series[1:]:
    fred_master = fred_master.merge(df, on='date', how='outer')

# Save to raw
fred_master.to_parquet(RAW_DIR / "fred_macro_2000_2025.parquet")
print(f"‚úÖ Saved: {len(fred_master)} rows √ó {len(fred_master.columns)} cols")
```

### 1.3 Conformance & Validation (raw ‚Üí staging)

**Script:** `scripts/conform/validate_and_conform.py`

```python
#!/usr/bin/env python3
"""
Validate raw data and conform to staging.
Failed rows ‚Üí quarantine for human review.
"""

import pandas as pd
from pathlib import Path

RAW_DIR = Path("/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/raw")
STAGING_DIR = Path("/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/staging")
QUARANTINE_DIR = Path("/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData/quarantine")

def validate_range(df, col, min_val, max_val):
    """Check value ranges"""
    mask = (df[col] < min_val) | (df[col] > max_val)
    return df[~mask], df[mask]

def validate_jumps(df, col, threshold=0.30):
    """Flag >30% jumps"""
    pct_change = df[col].pct_change().abs()
    mask = pct_change > threshold
    return df[~mask], df[mask]

def conform_fred_data():
    """Conform FRED data to staging"""
    df = pd.read_parquet(RAW_DIR / "fred_macro_2000_2025.parquet")
    
    quarantined = pd.DataFrame()
    
    # Validate VIX range (0-100)
    if 'vix' in df.columns:
        df_clean, df_bad = validate_range(df, 'vix', 0, 150)
        if len(df_bad) > 0:
            df_bad['quarantine_reason'] = 'vix_out_of_range'
            quarantined = pd.concat([quarantined, df_bad])
            df = df_clean
    
    # Validate sudden jumps in fed_funds_rate
    if 'fed_funds_rate' in df.columns:
        df_clean, df_bad = validate_jumps(df, 'fed_funds_rate', 0.20)
        if len(df_bad) > 0:
            df_bad['quarantine_reason'] = 'fed_rate_jump_>20pct'
            quarantined = pd.concat([quarantined, df_bad])
            df = df_clean
    
    # Check for duplicates
    dups = df[df.duplicated(subset=['date'], keep=False)]
    if len(dups) > 0:
        dups['quarantine_reason'] = 'duplicate_date'
        quarantined = pd.concat([quarantined, dups])
        df = df.drop_duplicates(subset=['date'], keep='first')
    
    # Save clean data to staging
    df.to_parquet(STAGING_DIR / "fred_macro_2000_2025.parquet")
    print(f"‚úÖ Staging: {len(df)} rows passed validation")
    
    # Save quarantined data
    if len(quarantined) > 0:
        quarantined.to_parquet(QUARANTINE_DIR / f"fred_quarantine_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.parquet")
        print(f"‚ö†Ô∏è  Quarantine: {len(quarantined)} rows ({quarantined['quarantine_reason'].value_counts().to_dict()})")
    
    return df

# Run for all raw sources
conform_fred_data()
# conform_noaa_data()
# conform_cftc_data()
# ...
```

**QA Gate:** Block Phase 2 if >10% of data quarantined.

---

## Phase 2: Feature Engineering - Single Pass (4 hours)

### 2.1 Create Join Specification

**File:** `registry/join_spec.yaml`

```yaml
version: "1.0"
description: "Declarative join specification with automated tests"

joins:
  # Base: Yahoo price data (all 55 symbols)
 - name: "base_prices"
    source: "staging/yahoo_historical_all_symbols.parquet"
    tests:
   - expect_date_range: ["2000-11-13", "2025-11-06"]
   - expect_symbols_count_gte: 55
   - expect_zl_rows_gte: 6000

  # Join FRED macro
 - name: "add_macro"
    left: "<<base_prices>>"
    right: "staging/fred_macro_2000_2025.parquet"
    on: ["date"]
    how: "left"
    null_policy:
      allow: true  # Macro data starts at different dates
      fill_method: "ffill"
    tests:
   - expect_rows_preserved
   - expect_columns_added: ["fed_funds_rate", "vix", "treasury_10y"]
   - expect_null_rate_below: {"fed_funds_rate": 0.05}

  # Join NOAA weather
 - name: "add_weather"
    left: "<<add_macro>>"
    right: "staging/noaa_weather_2000_2025.parquet"
    on: ["date"]
    how: "left"
    null_policy:
      allow: true
      fill: {"us_midwest_precip_30d": 0.0}
    tests:
   - expect_rows_preserved
   - expect_null_rate_below: {"us_midwest_precip_30d": 0.30}

  # Join CFTC positioning
 - name: "add_cftc"
    left: "<<add_weather>>"
    right: "staging/cftc_cot_2006_2025.parquet"
    on: ["date"]
    how: "left"
    null_policy:
      allow: true  # Only available 2006+
    tests:
   - expect_rows_preserved
   - expect_cftc_available_after: "2006-01-01"

  # Add regime assignments
 - name: "add_regimes"
    left: "<<add_cftc>>"
    right: "registry/regime_calendar.parquet"
    on: ["date"]
    how: "left"
    null_policy:
      allow: false  # Every date MUST have a regime
      fill: {"regime": "allhistory"}
    tests:
   - expect_rows_preserved
   - expect_regime_cardinality_gte: 7
   - expect_columns_present: ["market_regime", "training_weight"]

final_tests:
 - expect_total_rows_gte: 6000
 - expect_total_cols_gte: 150
 - expect_no_duplicate_dates
 - expect_date_range: ["2000-01-01", "2025-12-31"]
```

### 2.2 Build Join Executor (Automated Tests)

**Script:** `scripts/assemble/execute_joins.py`

```python
#!/usr/bin/env python3
"""
Execute joins from join_spec.yaml with automated tests.
Blocks on ANY test failure.
"""

import yaml
import pandas as pd
from pathlib import Path
from datetime import datetime

DRIVE = Path("/Volumes/Satechi Hub/Projects/CBI-V14")

class JoinExecutor:
    def __init__(self, spec_path):
        with open(spec_path) as f:
            self.spec = yaml.safe_load(f)
        self.results = {}
    
    def run_tests(self, df, tests, join_name):
        """Run all tests, raise on failure"""
        print(f"\n  üîç Testing {join_name}...")
        
        for test in tests:
            if 'expect_rows_preserved' in test:
                assert len(df) == self.last_row_count, \
                    f"Row count changed: {self.last_row_count} ‚Üí {len(df)}"
                print(f"    ‚úÖ Rows preserved: {len(df)}")
            
            elif 'expect_columns_present' in test:
                required = test['expect_columns_present']
                missing = [c for c in required if c not in df.columns]
                assert len(missing) == 0, f"Missing columns: {missing}"
                print(f"    ‚úÖ Columns present: {required}")
            
            elif 'expect_regime_cardinality_gte' in test:
                min_regimes = test['expect_regime_cardinality_gte']
                actual = df['market_regime'].nunique()
                assert actual >= min_regimes, \
                    f"Only {actual} regimes, need {min_regimes}+"
                print(f"    ‚úÖ Regime cardinality: {actual} (need {min_regimes}+)")
            
            elif 'expect_null_rate_below' in test:
                for col, max_null in test['expect_null_rate_below'].items():
                    null_pct = df[col].isnull().sum() / len(df)
                    assert null_pct < max_null, \
                        f"{col} has {null_pct:.1%} nulls (max {max_null:.1%})"
                    print(f"    ‚úÖ {col} nulls: {null_pct:.1%} (< {max_null:.1%})")
    
    def execute(self):
        """Execute all joins in order"""
        print("\n" + "="*80)
        print("EXECUTING DECLARATIVE JOINS")
        print("="*80)
        
        current_df = None
        
        for join in self.spec['joins']:
            name = join['name']
            print(f"\nüìã {name}")
            
            if 'source' in join:
                # Base load
                path = DRIVE / join['source']
                current_df = pd.read_parquet(path)
                print(f"  Loaded {len(current_df)} rows from {path.name}")
            else:
                # Join operation
                right_path = DRIVE / join['right']
                right_df = pd.read_parquet(right_path)
                
                self.last_row_count = len(current_df)
                current_df = current_df.merge(
                    right_df,
                    on=join['on'],
                    how=join['how']
                )
                print(f"  Joined {right_path.name}: {self.last_row_count} ‚Üí {len(current_df)} rows")
            
            # Run tests
            if 'tests' in join:
                self.run_tests(current_df, join['tests'], name)
        
        # Final tests
        print("\nüèÅ Running final tests...")
        self.run_tests(current_df, self.spec['final_tests'], "FINAL")
        
        return current_df

# Usage:
executor = JoinExecutor(DRIVE / "registry/join_spec.yaml")
df_final = executor.execute()
```

### 2.3 Collect All Free Data Sources

Same as before (FRED, NOAA, CFTC, USDA, EIA), but each now:

1. Downloads to `raw/`
2. Runs validation in `conform/` script
3. Passes data ‚Üí `staging/` OR fails ‚Üí `quarantine/`

**QA Gate:** Block Phase 2 if any CRITICAL source (FRED, yahoo) has >5% quarantine rate.

---

## Phase 2: Feature Engineering - Single Pass (4 hours)

### 2.1 Build Features ONCE (Not Per Horizon)

**Script:** `scripts/features/build_all_features.py`

```python
#!/usr/bin/env python3
"""
SINGLE-PASS FEATURE ENGINEERING
Calculate all 300+ features ONCE, then reuse for all 5 horizons.
Saves 4 hours vs recalculating per horizon.
"""

import pandas as pd
from pathlib import Path

DRIVE = Path("/Volumes/Satechi Hub/Projects/CBI-V14")

def build_features_single_pass():
    """Execute declarative joins, calculate all features"""
    
    # Step 1: Execute joins per spec
    from scripts.assemble.execute_joins import JoinExecutor
    executor = JoinExecutor(DRIVE / "registry/join_spec.yaml")
    df_base = executor.execute()  # All sources joined
    
    # Step 2: Calculate features (all categories)
    df_features = calculate_technical_indicators(df_base)
    df_features = calculate_cross_asset_features(df_features)
    df_features = calculate_volatility_features(df_features)
    df_features = calculate_seasonal_features(df_features)
    df_features = calculate_macro_regime_features(df_features)
    df_features = calculate_weather_aggregations(df_features)
    
    # Step 3: Add regime columns (master_regime_classification, crisis_intensity_score)
    df_features = add_regime_columns(df_features)
    
    # Step 4: Add override flags (vix_override_flag, harvest_override_flag, etc.)
    df_features = add_override_flags(df_features)
    
    # Save to features/ (single source of truth)
    df_features.to_parquet(DRIVE / "TrainingData/features/master_features_2000_2025.parquet")
    
    print(f"‚úÖ Features built: {len(df_features)} rows √ó {len(df_features.columns)} cols")
    return df_features

def create_horizon_exports(df_features):
    """Create 5 horizon exports from single feature set"""
    
    horizons = {'1w': 7, '1m': 30, '3m': 90, '6m': 180, '12m': 365}
    
    for horizon_name, days in horizons.items():
        df_export = df_features.copy()
        
        # Add forward target (only difference between horizons)
        df_export['target'] = df_export['zl_price_current'].shift(-days)
        
        # Save
        output = DRIVE / f"TrainingData/exports/zl_training_prod_allhistory_{horizon_name}.parquet"
        df_export.to_parquet(output, compression='zstd')
        
        print(f"‚úÖ {horizon_name}: {len(df_export)} rows √ó {len(df_export.columns)} cols ‚Üí {output.name}")

# Run
df_features = build_features_single_pass()
create_horizon_exports(df_features)
```

**QA Gate:** Feature count must be 150-500, regime count ‚â•7, no nulls in critical columns.

---

## Phase 3: File Organization (1 hour)

Clean structure as defined in Phase 0. Archive old files.

**QA Gate:** Final exports/ directory has EXACTLY 10 files, no extras.

---

## Phase 4: BigQuery Minimal Upload (2 hours)

Upload latest 100 rows per table. Identical to prior plan.

**QA Gate:** Verify BQ row counts = 100 per table, columns match parquet schema.

---

## Phase 5: Pre-Flight Performance Harness (3 hours)

### 5.1 Dry-Run Training + Metric Parity Check

**Script:** `scripts/qa/pre_flight_harness.py`

```python
#!/usr/bin/env python3
"""
PRE-FLIGHT PERFORMANCE HARNESS
Train tiny model on last 12 months, compute MAPE/Sharpe using EXACT BQ logic.
Must match dashboard metrics or BLOCK training.
"""

from google.cloud import bigquery
from lightgbm import LGBMRegressor
import pandas as pd
import numpy as np

def compute_local_mape(df_test, predictions):
    """Compute MAPE using EXACT logic from performance.vw_forecast_performance_tracking"""
    actuals = df_test['target'].values
    mape = np.mean(np.abs((predictions - actuals) / actuals)) * 100
    return mape

def get_bq_mape():
    """Query current MAPE from BigQuery dashboard view"""
    client = bigquery.Client(project='cbi-v14', location='us-central1')
    query = """
    SELECT overall_mape_1week
    FROM `cbi-v14.performance.vw_forecast_performance_tracking`
    """
    df = client.query(query).to_dataframe()
    return df['overall_mape_1week'].iloc[0]

def pre_flight_check():
    """THE GATE: Block training if local metrics don't match BQ"""
    
    # Load last 12 months
    df = pd.read_parquet("TrainingData/exports/zl_training_prod_allhistory_1m.parquet")
    df_recent = df[df['date'] >= (pd.Timestamp.now() - pd.Timedelta(days=365))]
    
    # Train tiny model
    feature_cols = [c for c in df_recent.columns if c not in ['date', 'target', 'market_regime']]
    X = df_recent[feature_cols].fillna(0)
    y = df_recent['target'].fillna(method='ffill')
    
    model = LGBMRegressor(n_estimators=50, max_depth=5, random_state=42)
    model.fit(X, y)
    
    # Predict
    preds = model.predict(X)
    
    # Compute local MAPE
    local_mape = compute_local_mape(df_recent, preds)
    
    # Get BQ MAPE
    bq_mape = get_bq_mape()
    
    # Check parity
    diff = abs(local_mape - bq_mape)
    
    print(f"\n{'='*60}")
    print(f"PRE-FLIGHT METRIC PARITY CHECK")
    print(f"{'='*60}")
    print(f"Local MAPE:  {local_mape:.2f}%")
    print(f"BQ MAPE:     {bq_mape:.2f}%")
    print(f"Difference:  {diff:.2f}%")
    
    if diff > 0.5:
        print(f"\n‚ùå PARITY FAILURE - BLOCKING TRAINING")
        print(f"   Local and BQ metrics diverged by {diff:.2f}%")
        print(f"   Fix metric calculation before proceeding.")
        raise ValueError("MAPE parity check failed")
    
    print(f"\n‚úÖ PARITY CHECK PASSED - TRAINING APPROVED")
    return True

# Run before ANY training
if __name__ == '__main__':
    pre_flight_check()
```

**CRITICAL:** This script MUST pass before any training begins.

### 5.2 Wire MAPE/Sharpe to API

Same as before - update `api.vw_ultimate_adaptive_signal`.

---

## Phase 6: Training Enhancements (10 hours)

### 6.1 Neural Network Training (Local Mac M4)

**Architecture:** TensorFlow with Metal GPU acceleration (Apple-optimized)

**Models to build:**

1. **Ultimate Adaptive Predictor** (Deep Neural Network)
   - Architecture: 512‚Üí256‚Üí128‚Üí64‚Üí32 (same as V14 build sheet intent)
   - Dropout: 0.3, Batch normalization
   - Multi-output: 2 targets (1w, 1m predictions)
   - Location: `Models/local/ultimate_adaptive_predictor/`

2. **LSTM for Time-Series**
   - Sequence model for temporal patterns
   - Location: `Models/local/lstm_temporal/`

3. **Attention/Transformer** (optional advanced)
   - For capturing long-range dependencies
   - Location: `Models/local/transformer_attention/`

**Training script:** `src/training/neural/train_local_dnn.py`

```python
import tensorflow as tf
from tensorflow import keras

# Enable Metal GPU
print(f"GPU available: {len(tf.config.list_physical_devices('GPU'))}")

# Build DNN (matches V14 spec)
model = keras.Sequential([
    keras.layers.Dense(512, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(256, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(2)  # 1w, 1m predictions
])

# Train on Metal GPU (fast)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.fit(X_train, y_train, epochs=100, batch_size=32)

# Save locally
model.save('Models/local/ultimate_adaptive_predictor/')
```

**Naming:** Matches V14 build sheet (`ultimate_adaptive_predictor`) but LOCAL, not BQML.

---

### 6.2 Dynamic Regime Classifier

**File:** `src/training/dynamic_regime_classifier.py`

ML-based regime detection from market conditions (VIX, yield curve, spreads, Big 8).

**Benefits:** Adapts to novel regimes, provides confidence scores.

---

### 6.3 Smooth Model Switching

**File:** `src/prediction/smooth_ensemble.py`

Weighted blending of regime models based on probabilities (no abrupt switches).

**Benefits:** Smooth transitions, hedges uncertainty.

---

### 6.4 SHAP Aggregation

**File:** `src/prediction/grouped_shap_explainer.py`

Aggregate 300+ SHAP values by category (Big 8, Technical, Macro, Weather).

**Output:** "Forecast driven by: Big 8 (+$1.80), Macro (+$0.70)"

---

### 6.5 Scenario Stress-Testing

**File:** `scripts/scenario_stress_tester.py`

War-game 7 scenarios: VIX spike, China embargo, supply shock, currency crisis, credit freeze, shipping crisis, policy shock.

**Tests:** Override logic, regime detection, prediction sanity.

---

## QA Gates (Ruthless Blocking)

Between EVERY phase:

```python
class ProductionQAGate:
    """Block next phase unless ALL checks pass"""
    
    GATES = {
        'POST_COLLECTION': [
            ('Date coverage', lambda df: df['date'].min() <= datetime(2000, 11, 15)),
            ('Row count', lambda df: len(df) >= 6000),
            ('No critical nulls', lambda df: df['date'].notna().all()),
        ],
        'POST_STAGING': [
            ('Quarantine rate', lambda stats: stats['quarantine_pct'] < 0.10),
            ('Range violations', lambda stats: stats['range_violations'] == 0),
        ],
        'POST_FEATURES': [
            ('Feature count', lambda df: len(df.columns) >= 150),
            ('Regime cardinality', lambda df: df['market_regime'].nunique() >= 7),
            ('Weight range', lambda df: df['training_weight'].min() >= 50 and df['training_weight'].max() >= 500),
            ('Override flags', lambda df: all(f in df.columns for f in ['vix_override_flag', 'master_regime_classification'])),
        ],
        'POST_ASSEMBLY': [
            ('No duplicates', lambda df: df['date'].duplicated().sum() == 0),
            ('Target not null', lambda df: df['target'].notna().sum() / len(df) > 0.95),
            ('No leakage', lambda df: verify_no_leakage(df)),
        ],
        'PRE_TRAINING': [
            ('MAPE parity', lambda: pre_flight_check()),  # From Phase 5
            ('File integrity', lambda: verify_all_exports_exist()),
        ]
    }
    
    @classmethod
    def check(cls, gate_name, *args):
        """Run gate, raise if any check fails"""
        print(f"\n{'='*80}")
        print(f"QA GATE: {gate_name}")
        print(f"{'='*80}")
        
        checks = cls.GATES[gate_name]
        
        for check_name, check_func in checks:
            try:
                result = check_func(*args)
                if result or result is None:
                    print(f"  ‚úÖ {check_name}")
                else:
                    raise AssertionError(f"{check_name} returned False")
            except Exception as e:
                print(f"  ‚ùå {check_name}: {e}")
                raise ValueError(f"QA GATE {gate_name} FAILED: {check_name}")
        
        print(f"\n‚úÖ {gate_name} PASSED - Proceeding\n")

# Usage between phases:
# ProductionQAGate.check('POST_COLLECTION', df)
# ProductionQAGate.check('POST_STAGING', stats)
# ProductionQAGate.check('POST_FEATURES', df)
# ProductionQAGate.check('POST_ASSEMBLY', df)
# ProductionQAGate.check('PRE_TRAINING')
```

---

## Timeline

| Phase | Task | Duration | QA Gate |
|-------|------|----------|---------|
| 0 | Setup + Forensic audit | 2 hrs | N/A |
| 1 | Data collection + validation | 8 hrs | POST_COLLECTION, POST_STAGING |
| 2 | Feature engineering (single-pass) | 4 hrs | POST_FEATURES |
| 3 | Organization | 1 hr | N/A |
| 4 | BQ upload | 2 hrs | N/A |
| 5 | Pre-flight harness + MAPE/Sharpe | 3 hrs | PRE_TRAINING |
| 6 | Training enhancements | 10 hrs | N/A |
| **Total** | | **30 hours** | **5 gates** |

---

## Acceptance Criteria (Must Pass All Gates)

**POST_COLLECTION:**
- [ ] Date range: 2000-2025
- [ ] Row count: ‚â•6,000
- [ ] Sources: FRED (30+), NOAA (10+), CFTC, yahoo (55 symbols)

**POST_STAGING:**
- [ ] Quarantine rate: <10%
- [ ] Range violations: 0
- [ ] Duplicates removed

**POST_FEATURES:**
- [ ] Feature count: 150-500
- [ ] Regime cardinality: ‚â•7
- [ ] Weight range: 50-5000
- [ ] Override flags: present

**POST_ASSEMBLY:**
- [ ] No duplicate dates
- [ ] Target coverage: >95%
- [ ] No data leakage

**PRE_TRAINING:**
- [ ] MAPE parity: <0.5% difference (local vs BQ)
- [ ] Sharpe parity: <5% difference
- [ ] All 10 export files exist
- [ ] Training scripts can read files

---

## Key Improvements Over Original Plan

1. **Staging + Quarantine** - Bad data never reaches training
2. **Join spec YAML** - Declarative, testable, prevents silent failures
3. **Feature registry** - Metadata-driven weighting
4. **QA gates** - Automated blocking between phases
5. **Single-pass build** - 4 hours faster
6. **Pre-flight harness** - Guarantees metric parity
7. **Retry logic** - Resilient to API failures

**Result:** Production-grade, institutional quality, bulletproof.

---

## Phase 7: Production Automation Infrastructure (4 hours)

**Goal:** Scalable automation for daily/weekly/monthly updates. Configuration-driven to easily add future sources.

### 7.1 Data Source Registry (Central Configuration)

**File:** `registry/data_sources.yaml`

```yaml
version: "1.0"
description: "Centralized registry for all data sources - add new sources here"

# CURRENT SOURCES (Tier 1)
sources:
  # === DAILY (FREE, RELIABLE) ===
  fred_macro:
    script: "scripts/ingest/collect_fred_historical.py"
    schedule: "daily"
    time: "06:10"
    args: ["--mode", "incremental", "--lookback-days", "10"]
    priority: "P0"
    reliability: 0.98
    free: true
    
  yahoo_prices:
    script: "scripts/ingest/collect_yahoo_prices.py"
    schedule: "daily"
    time: "07:00"
    args: ["--symbols", "all", "--lookback-days", "7"]
    priority: "P0"
    reliability: 0.95
    free: true
    
  noaa_weather:
    script: "scripts/ingest/collect_noaa_historical.py"
    schedule: "daily"
    time: "06:40"
    args: ["--mode", "incremental", "--lookback-days", "14"]
    priority: "P0"
    reliability: 0.95
    free: true
  
  # === WEEKLY (FREE, GOVERNMENT) ===
  cftc_cot:
    script: "scripts/backfill/backfill_cftc_cot_historical.py"
    schedule: "weekly"
    time: "09:15"
    day_of_week: "Saturday"
    args: ["2006", "now", "--mode", "incremental"]
    priority: "P0"
    reliability: 0.99
    free: true
    
  eia_biofuels:
    script: "scripts/ingest/collect_eia_biofuels.py"
    schedule: "weekly"
    time: "07:10"
    day_of_week: "Thursday"
    args: ["--lookback-weeks", "4"]
    priority: "P1"
    reliability: 0.95
    free: true
    
  usda_exports:
    script: "scripts/ingest/collect_usda_exports.py"
    schedule: "weekly"
    time: "08:00"
    day_of_week: "Thursday"
    args: ["--lookback-weeks", "8"]
    priority: "P1"
    reliability: 0.95
    free: true
  
  # === MONTHLY (FREE, GOVERNMENT) ===
  usda_nass:
    script: "scripts/ingest/collect_usda_nass.py"
    schedule: "monthly"
    time: "08:05"
    day_of_month: 15
    args: ["--lookback-months", "3"]
    priority: "P1"
    reliability: 0.95
    free: true
    
  bls_indicators:
    script: "scripts/ingest/collect_bls_data.py"
    schedule: "monthly"
    time: "09:00"
    day_of_month: 15
    args: ["--series", "CPIAUCSL,APU0000701111"]
    priority: "P1"
    reliability: 0.98
    free: true

# TEMPLATE FOR FUTURE SOURCES (just uncomment and customize)
# future_sources:
#   china_customs:
#     script: "scripts/ingest/collect_china_customs.py"
#     schedule: "monthly"
#     time: "10:30"
#     day_of_month: 10
#     args: ["--commodities", "soybean,soy_oil"]
#     priority: "P1"
#     reliability: 0.85
#     free: true
#   
#   inmet_brazil:
#     script: "scripts/ingest/collect_inmet_brazil.py"
#     schedule: "daily"
#     time: "07:30"
#     args: ["--stations", "all", "--lookback-days", "10"]
#     priority: "P1"
#     reliability: 0.90
#     free: true
```

---

### 7.2 Generic Job Runner (Works for ALL Sources)

**File:** `scripts/automation/job_runner.py`

```python
#!/usr/bin/env python3
"""
Generic job runner - works for ANY data collection script.
Features: locking, jitter, state management, atomic writes, error handling.
"""

import os, sys, json, fcntl, subprocess, pathlib, random, time
from datetime import datetime

ROOT = pathlib.Path(__file__).resolve().parents[2]
STATE_DIR = ROOT / "state"
LOG_DIR = ROOT / "logs"
LOCK_DIR = pathlib.Path("/tmp")

def jitter(max_seconds=90):
    """Random delay to avoid API rate limit spikes"""
    time.sleep(random.randint(1, max_seconds))

def load_state(job_name):
    """Load checkpoint from last run"""
    STATE_DIR.mkdir(exist_ok=True)
    state_file = STATE_DIR / f"{job_name}.json"
    if not state_file.exists():
        return {"last_success": None, "runs": 0}
    return json.loads(state_file.read_text())

def save_state(job_name, state_obj):
    """Save checkpoint after successful run"""
    state_file = STATE_DIR / f"{job_name}.json"
    state_obj["last_updated"] = datetime.now().isoformat()
    state_file.write_text(json.dumps(state_obj, indent=2, default=str))

def acquire_lock(job_name):
    """Prevent duplicate runs"""
    lock_file = LOCK_DIR / f"cbi_v14_{job_name}.lock"
    lf = open(lock_file, "w")
    try:
        fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)
        return lf
    except BlockingIOError:
        print(f"[{job_name}] Already running, exiting")
        sys.exit(0)

def run_job(job_name, script_path, args):
    """Execute with full guardrails"""
    
    # Preflight
    preflight = ROOT / "scripts/automation/preflight.sh"
    subprocess.run([str(preflight)], check=True)
    
    # Jitter
    jitter(90)
    
    # Lock
    lock = acquire_lock(job_name)
    
    # Log setup
    LOG_DIR.mkdir(exist_ok=True)
    log_file = LOG_DIR / f"{job_name}.log"
    
    with open(log_file, "a") as log:
        log.write(f"\n{'='*60}\n{datetime.now().isoformat()} :: {job_name}\n{'='*60}\n")
        
        state = load_state(job_name)
        venv_python = ROOT / "venv/bin/python"
        cmd = [str(venv_python), str(ROOT / script_path)] + args
        
        result = subprocess.run(cmd, stdout=log, stderr=log)
        
        if result.returncode == 0:
            state["last_success"] = datetime.now().isoformat()
            state["runs"] = state.get("runs", 0) + 1
            save_state(job_name, state)
            log.write(f"‚úÖ SUCCESS\n")
        else:
            log.write(f"‚ùå FAILED (exit {result.returncode})\n")
            sys.exit(result.returncode)
    
    lock.close()

if __name__ == "__main__":
    run_job(sys.argv[1], sys.argv[2], sys.argv[3:])
```

---

### 7.3 LaunchAgent Generator (Auto-Create from Registry)

**File:** `scripts/automation/generate_launchagents.py`

```python
#!/usr/bin/env python3
"""
Generate macOS LaunchAgents from data_sources.yaml.
To add new source: update YAML, run this script, load plist. Done!
"""

import yaml
from pathlib import Path
from jinja2 import Template

# ... (full implementation in plan above)

# Generates one .plist per source from registry
# Makes adding new sources trivial: just update YAML
```

---

### 7.4 Installation & Setup

**One-time setup script:** `scripts/automation/install.sh`

```bash
#!/usr/bin/env bash
set -euo pipefail

echo "Installing CBI-V14 Automation..."

# Directories
mkdir -p ~/CBI-V14/{state,logs,registry}

# Venv
python3 -m venv ~/CBI-V14/venv
~/CBI-V14/venv/bin/pip install -r requirements.txt

# Secrets (Keychain)
python3 scripts/automation/setup_secrets.py

# Generate LaunchAgents
python3 scripts/automation/generate_launchagents.py

# Load all
cd ~/Library/LaunchAgents
for plist in com.cbi-v14.*.plist; do
  launchctl load "$plist"
done

# Log rotation
sudo tee /etc/newsyslog.d/cbi-v14.conf > /dev/null <<EOF
$HOME/CBI-V14/logs/*.log  $USER:staff  644  7  10240  *  Z
EOF

echo "‚úÖ Installation complete!"
```

---

### 7.5 Health Monitoring

**Daily health check:** `scripts/automation/check_health.py`

```python
#!/usr/bin/env python3
"""Show status of all automated jobs"""

import json, glob
from pathlib import Path
from datetime import datetime, timedelta

STATE_DIR = Path.home() / "CBI-V14/state"

for state_file in sorted(STATE_DIR.glob("*.json")):
    job = state_file.stem
    state = json.load(open(state_file))
    
    last = state.get('last_success')
    if last:
        age = datetime.now() - datetime.fromisoformat(last)
        status = "‚úÖ" if age < timedelta(days=1) else "‚ö†Ô∏è" if age < timedelta(days=7) else "‚ùå"
    else:
        age = "Never"
        status = "‚ùì"
    
    print(f"{status} {job:<25} last_success={last or 'Never':<20} age={age} runs={state.get('runs', 0)}")
```

---

## Adding Future Sources (2-Minute Process)

**Example: Adding China Customs Import Data**

**Step 1:** Edit `registry/data_sources.yaml`:

```yaml
  china_customs_imports:
    script: "scripts/ingest/collect_china_customs.py"
    schedule: "monthly"
    time: "10:30"
    day_of_month: 10
    args: ["--commodities", "soybean,soy_oil", "--lookback-months", "6"]
    priority: "P1"
    reliability: 0.85
    free: true
```

**Step 2:** Create script:

```bash
# Create scripts/ingest/collect_china_customs.py
# (Standard pattern: fetch, validate, atomic write, update state)
```

**Step 3:** Activate:

```bash
python3 scripts/automation/generate_launchagents.py
launchctl load ~/Library/LaunchAgents/com.cbi-v14.china_customs_imports.plist
```

**Done!** New source runs automatically on schedule.

---

## Timeline

**Data Phases (0-5):** 20 hours

**Training Enhancements (Phase 6):** +10 hours

**Automation Setup (Phase 7):** +4 hours (one-time)

**Total:** 34 hours (~5 days)

---

## Benefits of This Approach

**Scalability:**
- Add 100 sources without changing infrastructure code
- Just update YAML, regenerate plists

**Reliability:**
- File locking (no duplicate runs)
- State checkpoints (survive crashes)
- Atomic writes (no partial files)
- Retry with backoff (resilient to API hiccups)

**Maintainability:**
- One generic runner for all jobs
- Logs auto-rotate
- Health check shows all jobs at a glance
- Secrets in Keychain (secure)

**Flexibility:**
- Easy to pause/resume any source
- Easy to change schedules
- Easy to add catch-up logic

### To-dos

- [x] Query BigQuery __TABLES__ to get actual row counts for all datasets, identify 1M+ row tables
- [x] Verify yahoo_finance_comprehensive has ~801K rows and contains real price data (not placeholders)
- [x] Check all 10 training tables have proper row counts, date ranges, and regime assignments
- [x] Query production tables for 0.5 placeholder pattern and other fake values
- [x] Verify all table joins in SQL files reference existing tables and are properly structured
- [x] Verify Sharpe, MAPE, volatility calculations use real data, not placeholders
- [x] Verify models_v4 historical tables contain real data and are properly loaded
- [x] Generate comprehensive verification report with all findings
