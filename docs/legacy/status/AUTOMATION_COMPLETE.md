# CBI-V14 Automation Complete - Installation Required
**Date:** November 12, 2025  
**Status:** Ready for installation

---

## ğŸš¨ CRITICAL GAP IDENTIFIED AND FIXED

### The Problem (FOUND)

**Before:**
```
Cron ingests data â†’ BigQuery âœ… AUTOMATED
BigQuery â†’ External Drive  âŒ NOT AUTOMATED (manual export required)
```

**Impact:** Training data becomes stale immediately after first export.

---

## âœ… Solutions Created

### 1. Weekly Auto-Export (Added to Cron)

**What:** Automatically exports fresh training data every Sunday at 3 AM

**Added to:** `scripts/crontab_setup.sh` line 123

```bash
# WEEKLY TRAINING DATA REFRESH
# Export fresh training data to external drive (Sunday 3 AM - after weekend maintenance)
0 3 * * 0 cd $REPO_ROOT && . "$ENV_FILE" && python3 scripts/export_training_data.py >> $LOG_DIR/training_export.log 2>&1
```

**Result:** 
- Training data refreshed weekly
- Captures all week's ingestion
- Max staleness: 7 days (acceptable for model training)
- Cost: $0 (just exports existing data)

---

### 2. Smart Training Wrapper (Best Practice)

**Script:** `scripts/train_with_fresh_data.sh`

**What it does:**
1. Checks if training data exists
2. Checks age of existing exports
3. Auto-refreshes if >7 days old
4. Then runs your training script

**Usage:**
```bash
# Instead of:
python3 src/training/train_model.py --horizon=1m

# Use:
bash scripts/train_with_fresh_data.sh python3 src/training/train_model.py --horizon=1m

# Or for Vertex deployment:
bash scripts/train_with_fresh_data.sh python3 vertex-ai/deployment/train_local_deploy_vertex.py --horizon=1m
```

**Benefit:** Guarantees you're always training on fresh data.

---

## Installation Required

### Step 1: Install Updated Cron Schedule

**Run this command locally (not in sandbox):**
```bash
cd /Users/kirkmusick/Documents/GitHub/CBI-V14
bash scripts/crontab_setup.sh
```

This will install the updated schedule with 33 jobs (was 32):
- All previous jobs (32)
- New weekly export job (1)

### Step 2: Verify Installation

```bash
# Check for the new export job
crontab -l | grep "training_export"

# Should show:
# 0 3 * * 0 cd /Users/kirkmusick/Documents/GitHub/CBI-V14 && . "/Users/kirkmusick/Documents/GitHub/CBI-V14/.env.cron" && /usr/bin/python3 scripts/export_training_data.py >> /Users/kirkmusick/Documents/GitHub/CBI-V14/Logs/cron/training_export.log 2>&1
```

### Step 3: Test the Wrapper Script

```bash
# Test the fresh data wrapper (dry run)
bash scripts/train_with_fresh_data.sh echo "Training would start here"
```

---

## Complete Data Flow (After Installation)

### Automated Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DAILY/WEEKLY CRON INGESTION (32 jobs)                   â”‚
â”‚ Mac M4 â†’ APIs/Web â†’ BigQuery Cloud                      â”‚
â”‚ â€¢ Weather, prices, social intel, CFTC, USDA, etc.       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Data accumulates in BigQuery
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WEEKLY AUTO-EXPORT (NEW - Sunday 3 AM)                  â”‚
â”‚ BigQuery Cloud â†’ External Drive                         â”‚
â”‚ â€¢ Refreshes TrainingData/exports/*.parquet              â”‚
â”‚ â€¢ Captures week's ingestion                             â”‚
â”‚ â€¢ Max 7-day staleness                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Fresh Parquet files ready
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOCAL TRAINING (Manual, with fresh data wrapper)        â”‚
â”‚ Mac M4 + TensorFlow Metal                               â”‚
â”‚ â€¢ bash scripts/train_with_fresh_data.sh <training_cmd>  â”‚
â”‚ â€¢ Auto-checks data freshness before training            â”‚
â”‚ â€¢ Trains on external drive data                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Trained models
             â”‚
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEPLOYMENT (Manual)                                      â”‚
â”‚ Vertex AI Model Registry â†’ Endpoints                    â”‚
â”‚ â€¢ Upload SavedModel to cloud                            â”‚
â”‚ â€¢ Deploy endpoint for predictions                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Automation Summary

### What IS Automated âœ…

1. **Data Ingestion** (32 cron jobs)
   - Weather, prices, social intel, APIs
   - Daily, weekly, monthly schedules
   - Direct to BigQuery Cloud

2. **Training Data Export** (NEW - 1 cron job)
   - Weekly refresh (Sunday 3 AM)
   - BigQuery â†’ External Drive
   - Keeps Parquet files current

3. **Quality Monitoring** (3 cron jobs)
   - Data quality checks
   - Stale data detection
   - Missing data finder

4. **Maintenance** (3 cron jobs)
   - Weekend maintenance
   - Monthly Vertex predictions
   - Log rotation

### What is NOT Automated âŒ

1. **Training Table Rebuild** (BigQuery SQL)
   - Must run manually: `bq query < CREATE_PRODUCTION_TRAINING_*.sql`
   - Frequency: As needed (weekly/monthly)
   - Duration: ~5-10 minutes
   - Cost: ~$0.10 per rebuild

2. **Local Model Training**
   - Must run manually: `python3 src/training/*.py`
   - Frequency: As needed
   - Duration: 30 min - 4 hours depending on model
   - Cost: $0 (local)

3. **Vertex AI Deployment**
   - Must run manually: `python3 vertex-ai/deployment/*.py`
   - Frequency: Only when new model beats benchmark
   - Duration: ~10-15 minutes
   - Cost: Deployment charges

---

## Best Practice Workflows

### Weekly Training Workflow (Recommended)

**Every Sunday (after auto-export completes):**

```bash
# Data is auto-exported at 3 AM Sunday
# Check at 4 AM or later:

# 1. Verify fresh export
ls -lh TrainingData/exports/production_training_data_1m.parquet

# 2. Train with fresh data wrapper
bash scripts/train_with_fresh_data.sh python3 src/training/train_simple_lstm.py --horizon=1m

# 3. If model beats benchmark, deploy
python3 vertex-ai/deployment/upload_to_vertex.py --model=Models/local/lstm_1m.h5 --horizon=1m
```

### Monthly Training Table Rebuild (Recommended)

**First Sunday of each month:**

```bash
# 1. Rebuild training tables in BigQuery
bq query --use_legacy_sql=false < config/bigquery/bigquery-sql/PRODUCTION_HORIZON_SPECIFIC/CREATE_PRODUCTION_TRAINING_1M.sql

# 2. Export fresh data (auto-export will run anyway, but you can trigger manually)
python3 scripts/export_training_data.py

# 3. Train comprehensive models
bash scripts/train_with_fresh_data.sh python3 src/training/train_all_baselines.py
```

---

## Data Freshness Guarantees

| Component | Freshness | Update Method |
|-----------|-----------|---------------|
| BigQuery Raw Data | Current (daily/weekly) | Cron automation âœ… |
| BigQuery Training Tables | As needed (manual rebuild) | Manual SQL âŒ |
| External Drive Parquet | Weekly (max 7 days old) | Cron automation âœ… (NEW) |
| Local Model Checkpoints | On-demand | Manual training âŒ |

---

## Why Not Fully Automate Everything?

### Training is NOT automated because:

1. **Training is expensive** (30 min - 4 hours per model)
2. **Training requires monitoring** (check convergence, overfitting)
3. **Training requires decisions** (which model, which hyperparameters)
4. **You don't retrain daily** (weekly/monthly is sufficient)

### Training Table Rebuild is NOT automated because:

1. **BigQuery costs** (~$0.10 per rebuild)
2. **Not needed daily** (raw data changes, but aggregates stable)
3. **Manual trigger is fine** (monthly is sufficient)

---

## Installation Steps (DO THIS NOW)

### 1. Install Updated Cron
```bash
cd /Users/kirkmusick/Documents/GitHub/CBI-V14
bash scripts/crontab_setup.sh
```

### 2. Verify Installation
```bash
# Check total jobs
crontab -l | grep -c "CBI-V14"
# Should show: 33 (was 32, added 1 export job)

# Check export job specifically
crontab -l | grep "training_export"
# Should show the Sunday 3 AM export job
```

### 3. Test Wrapper Script
```bash
# Dry run test
bash scripts/train_with_fresh_data.sh echo "Test successful"
# Should check data freshness and show status
```

---

## Expected Behavior After Installation

### Every Sunday at 3 AM

```
2:00 AM - Weekend maintenance runs (daily_data_pull_and_migrate.py)
3:00 AM - Training data export runs (export_training_data.py)
         â†“
         Export 13 Parquet files to TrainingData/exports/
         â†“
         Log results to Logs/cron/training_export.log
         â†“
         Fresh data ready for training
```

### When You Train (Any Day)

```bash
bash scripts/train_with_fresh_data.sh python3 src/training/train_model.py

Checks:
  - Data exists? If not â†’ export now
  - Data age? If >7 days â†’ re-export now
  - Data fresh? â†’ proceed with training
```

**Result:** You always train on data <7 days old (usually fresher if recently exported)

---

## Files Created/Modified

### Created
1. `docs/reference/INGESTION_PIPELINE_FLOW.md` - This document
2. `scripts/train_with_fresh_data.sh` - Fresh data wrapper
3. `AUTOMATION_COMPLETE.md` - Installation guide

### Modified
1. `scripts/crontab_setup.sh` - Added weekly export (line 123)
2. `scripts/export_training_data.py` - Fixed bug (lines 230, 246)

---

## Summary

### Before This Fix
- âŒ No auto-export
- âŒ Training data gets stale
- âŒ Must remember to export manually

### After Installation
- âœ… Weekly auto-export (Sunday 3 AM)
- âœ… Training data stays fresh (<7 days)
- âœ… Wrapper ensures freshness before training
- âœ… Zero manual intervention for data freshness

### Installation Required
**You must run:** `bash scripts/crontab_setup.sh` (outside sandbox)

This completes the automation loop: **Ingest â†’ Export â†’ Train** âœ…

