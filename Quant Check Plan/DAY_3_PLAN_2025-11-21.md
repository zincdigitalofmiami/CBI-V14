# DAY 3 EXECUTION PLAN - Feature Consolidation & Bulk Load
**Date:** November 21, 2025  
**Status:** üü° PLANNING  
**Prerequisites:** Days 1 & 2 complete, integration test passed

---

## SITUATION ASSESSMENT

### ‚úÖ What We Have
1. **BigQuery Infrastructure:**
   - `training.regime_calendar` populated (1,908 rows)
   - `training.regime_weights` populated (2 rows)
   - `features.daily_ml_matrix` created (partitioned, clustered, empty)

2. **Ingestion Pipeline:**
   - `scripts/ingestion/ingest_features_hybrid.py` (verified working)
   - `scripts/tests/test_100_row_batch.py` (100-row test passed)
   - Regime lookup, data quality validation, micro-batch loading all working

3. **Staging Data:**
   - `yahoo_historical_prefixed.parquet` (6,380 rows, 55 columns, 2000-2025)
   - `zl_daily_aggregated.parquet` (3,998 rows, 14 columns, 2010-2025)
   - `mes_1hr_features.parquet` (1 row only - insufficient)
   - Various macro/weather/policy files

### ‚ùå What We're Missing
1. **Consolidated Feature Files:**
   - No single file with all features per (symbol, date)
   - Yahoo columns named `yahoo_*`, ZL columns named `zl_*`
   - Missing pivot points (P, R1, R2, S1, S2, distances)
   - Missing Trump/policy features (16 columns)
   - Missing golden_zone features (7 columns)

2. **Feature Producers:**
   - Pivot calculator exists (`cloud_function_pivot_calculator.py`) but hasn't run on historical data
   - Trump predictors exist but need historical backfill
   - Golden zone calculator not yet run on MES data

---

## RECOMMENDED APPROACH

### Phase A: Feature Consolidation (NEW - Must Do First)

**Goal:** Create consolidated parquet files that match `daily_ml_matrix` schema

**Steps:**

1. **Build ZL Consolidated Features**
   - Merge: Yahoo OHLCV + ZL aggregated + macro (FRED, weather, CFTC, USDA, EIA)
   - Calculate: Pivot points for historical dates
   - Backfill: Trump/policy features (use existing predictors on historical data)
   - Output: `zl_consolidated_features_2010_2025.parquet` with all required columns

2. **Build MES Consolidated Features**
   - Gather: MES 1hr OHLCV + technical indicators
   - Calculate: Golden zone features for historical data
   - Calculate: Pivot points for MES
   - Output: `mes_consolidated_features_1hr.parquet` with all required columns

3. **Validate Consolidated Files**
   - Check: All required columns present
   - Check: Column names match `daily_ml_matrix` schema exactly
   - Check: No duplicate (symbol, date) pairs
   - Check: Date ranges within regime_calendar coverage

### Phase B: Bulk Load (After Phase A)

**Goal:** Load consolidated features into BigQuery

**Steps:**

1. **Test with 10 rows** (safety check)
2. **Load ZL daily** (~3,000-5,000 rows, 2010-2025)
3. **Load MES 1hr** (if consolidated file ready)
4. **Verify data in BigQuery**
5. **Run data quality audits**

---

## DECISION POINT

**Option 1: Build Feature Consolidation Script (Recommended)**
- **Pros:** Clean, repeatable, matches schema exactly
- **Cons:** 2-4 hours of work
- **Output:** Production-ready consolidated features

**Option 2: Manual Mapping in Ingestion Script**
- **Pros:** Faster to start
- **Cons:** Complex, error-prone, technical debt
- **Not Recommended**

**Option 3: Simplified Phase 1 (Fastest)**
- Load only what's ready: Yahoo OHLCV + basic features
- Defer pivots, Trump features, golden zone to Phase 2
- **Pros:** Can load data today
- **Cons:** Incomplete feature set, can't train models yet

---

## RECOMMENDED: OPTION 1 (Feature Consolidation Script)

### Create: `scripts/features/build_consolidated_features.py`

**Purpose:** Combine all feature sources into schema-compliant files

**Inputs:**
- `TrainingData/staging/yahoo_historical_prefixed.parquet`
- `TrainingData/staging/zl_daily_aggregated.parquet`
- FRED, weather, CFTC, USDA, EIA parquet files
- Pivot calculator function
- Trump predictor function

**Processing:**
1. Load and merge all sources on (symbol, date)
2. Rename columns to schema format (yahoo_open ‚Üí open, etc.)
3. Calculate missing features:
   - Pivot points (P, R1, R2, S1, S2, distances)
   - Trump/policy scores (backfill using historical news if available, or NULLs)
   - Golden zone (for MES only)
4. Validate schema compliance
5. Save consolidated parquet

**Outputs:**
- `TrainingData/consolidated/zl_consolidated_2010_2025.parquet`
- `TrainingData/consolidated/mes_consolidated_1hr.parquet`

**Schema Requirements (must match `daily_ml_matrix`):**
```
Required top-level:
- symbol, data_date, timestamp

Market data (7 fields):
- open, high, low, close, volume, vwap, realized_vol_1h

Pivots (9 fields):
- P, R1, R2, S1, S2, distance_to_P, distance_to_nearest_pivot, 
  weekly_pivot_distance, price_above_P

Policy (16 fields):
- trump_action_prob, trump_expected_zl_move, trump_score, 
  trump_score_signed, trump_confidence, trump_sentiment_7d,
  trump_tariff_intensity, trump_procurement_alert, trump_mentions,
  trumpxi_china_mentions, trumpxi_sentiment_volatility,
  trumpxi_policy_impact, trumpxi_volatility_30d_ma,
  trump_soybean_sentiment_7d, policy_trump_topic_multiplier,
  policy_trump_recency_decay

Golden zone (7 fields, MES only):
- golden_zone_state, swing_high, swing_low, fib_50, fib_618,
  vol_decay_slope, qualified_trigger

Optional (can be NULL):
- vol_percentile, k_vol (calculated later during regime analysis)
```

---

## ALTERNATIVE: SIMPLIFIED PHASE 1 (Option 3)

If you want to load data **immediately** without full feature consolidation:

### Quick Win: Load Yahoo OHLCV Only

**Create:** `scripts/features/build_minimal_features.py`

**Processing:**
1. Take Yahoo data (6,380 rows, 2000-2025)
2. Map: `yahoo_open` ‚Üí `open`, etc.
3. Filter: 2010-2025 only (regime coverage + training period)
4. Set NULLs for: pivots, policy, golden_zone
5. Save: `zl_minimal_2010_2025.parquet`

**Load:** ~5,000 rows into BigQuery

**Result:** 
- ‚úÖ Data in BigQuery (can test training table joins)
- ‚ùå Missing pivots (can't use pivot features yet)
- ‚ùå Missing Trump features (can't use policy features yet)
- ‚ùå Missing MES (can't train MES models yet)

---

## MY RECOMMENDATION

**Do Option 1 (Full Feature Consolidation)** because:

1. **Quality over Speed:** We've built everything correctly so far (Days 1 & 2), don't cut corners now
2. **Avoid Rework:** Loading incomplete data means reloading later
3. **Training Ready:** Full features = can start training immediately after load
4. **One-Time Effort:** Consolidation script is reusable for future data updates

**Timeline:**
- Feature consolidation script: 2-3 hours
- Historical pivot calculation: 30 min
- Trump backfill (or accept NULLs for pre-2023): 30 min
- Testing & validation: 30 min
- Bulk load: 30 min
- **Total: 4-5 hours**

---

## NEXT STEPS (AWAITING YOUR DECISION)

**Option 1:** Build full feature consolidation script
- I'll create `build_consolidated_features.py`
- Backfill pivots, handle Trump features (NULL or backfill)
- Test with 10 rows, then bulk load

**Option 2:** Simplified immediate load
- I'll create `build_minimal_features.py`
- Load Yahoo OHLCV + basic features only
- Defer pivots/policy/MES to Phase 2

**Option 3:** Review & Plan More
- You review the staging data
- Decide what features are must-have vs optional
- Custom approach

---

## CLARIFYING QUESTIONS

Before I proceed, please clarify:

1. **Trump Features:** Do you have historical news/social data to backfill Trump features for 2010-2023? Or should we:
   - Set them to NULL for pre-2023 data?
   - Only include trump_anticipation_2024 + trump_second_term period (2023-2025)?

2. **Pivot Points:** Should I:
   - Calculate pivots for all historical data (2010-2025)?
   - Use the existing `cloud_function_pivot_calculator.py` logic?

3. **MES Data:** The `mes_1hr_features.parquet` only has 1 row. Should we:
   - Skip MES for Day 3?
   - Wait for more MES data collection?

4. **Timeline Preference:**
   - **Fast:** Load minimal features today (Option 3)
   - **Complete:** Build full consolidation (Option 1, my recommendation)

---

**Your decision?**

