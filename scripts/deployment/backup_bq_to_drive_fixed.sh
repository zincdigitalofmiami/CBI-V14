#!/bin/bash
# ============================================================================
# Fixed: Backup BigQuery to External Drive and Archive/Remove to Save Money
# Date: November 19, 2025
# Purpose: Properly export data to external drive using bq query, then archive in BQ
# ============================================================================

set -euo pipefail

PROJECT_ID="cbi-v14"
EXTERNAL_DRIVE="/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData"
BACKUP_DIR="${EXTERNAL_DRIVE}/00_bigquery_backup_$(date +%Y%m%d)"
# Optional: override summary output to a repo path to avoid external writes
REPORTS_DIR="${CBI_REPORTS_DIR:-}"
DISABLE_SUMMARY="${CBI_DISABLE_SUMMARY:-0}"
LOCATION="us-central1"
ARCHIVE_DATASET="z_archive_$(date +%Y%m%d)"

echo "ðŸ’¾ BigQuery Backup to External Drive & Archive"
echo "=============================================="
echo "Project: $PROJECT_ID"
echo "Backup Location: $BACKUP_DIR"
echo "Archive Dataset: $ARCHIVE_DATASET"
echo ""
echo "âš ï¸  WARNING: This will backup and ARCHIVE datasets in BigQuery"
echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
sleep 5

# ============================================================================
# STEP 1: Create Backup Directory
# ============================================================================
echo ""
echo "ðŸ“ Step 1: Creating backup directory structure..."
mkdir -p "${BACKUP_DIR}"
mkdir -p "${BACKUP_DIR}/datasets"
mkdir -p "${BACKUP_DIR}/metadata"

# ============================================================================
# STEP 2: Get Dataset List
# ============================================================================
echo ""
echo "ðŸ“Š Step 2: Fetching dataset list..."
bq ls --project_id=${PROJECT_ID} --max_results=1000 --format=csv | tail -n +2 | cut -d',' -f1 > "${BACKUP_DIR}/dataset_list.txt"

# Essential datasets to keep active
ESSENTIAL_DATASETS=(
    "api"
    "features" 
    "market_data"
    "monitoring"
    "predictions"
    "raw_intelligence"
    "training"
)

# ============================================================================
# STEP 3: Create Archive Dataset
# ============================================================================
echo ""
echo "ðŸ“¦ Step 3: Creating archive dataset..."
bq mk --dataset --location=${LOCATION} ${PROJECT_ID}:${ARCHIVE_DATASET} 2>/dev/null || true

# ============================================================================
# STEP 4: Backup and Archive Each Dataset
# ============================================================================
echo ""
echo "ðŸ’¾ Step 4: Backing up datasets..."

backup_and_archive_dataset() {
    local dataset=$1
    
    # Skip if essential dataset
    for essential in "${ESSENTIAL_DATASETS[@]}"; do
        if [[ "$dataset" == "$essential" ]]; then
            echo "  â­ï¸  Skipping essential dataset: ${dataset}"
            return
        fi
    done
    
    echo "  ðŸ“‚ Processing ${dataset}..."
    
    # Create dataset directory
    local dataset_dir="${BACKUP_DIR}/datasets/${dataset}"
    mkdir -p "${dataset_dir}"
    
    # Get dataset metadata
    echo "    ðŸ“‹ Saving metadata..."
    bq show --format=prettyjson ${PROJECT_ID}:${dataset} > "${dataset_dir}/dataset_metadata.json" 2>/dev/null || {
        echo "    âš ï¸  Dataset ${dataset} not found"
        return
    }
    
    # Get list of tables
    local tables=$(bq ls -n 10000 --project_id=${PROJECT_ID} ${dataset} 2>/dev/null | tail -n +3 | awk '{print $1}')
    
    if [ -z "$tables" ]; then
        echo "    ðŸ“­ No tables in ${dataset}"
        
        # Remove empty dataset
        echo "    ðŸ—‘ï¸  Removing empty dataset..."
        bq rm -r -f -d ${PROJECT_ID}:${dataset} 2>/dev/null || true
        return
    fi
    
    # Process each table
    local table_count=0
    local archived_count=0
    
    for table in $tables; do
        echo "    ðŸ“Š Table: ${table}"
        
        # Get table schema
        bq show --schema --format=prettyjson ${PROJECT_ID}:${dataset}.${table} \
            > "${dataset_dir}/${table}_schema.json" 2>/dev/null || true
        
        # Get row count
        local row_count=$(bq query --use_legacy_sql=false --format=csv \
            "SELECT COUNT(*) as cnt FROM \`${PROJECT_ID}.${dataset}.${table}\`" 2>/dev/null | tail -1)
        
        if [[ "$row_count" == "0" ]] || [[ -z "$row_count" ]]; then
            echo "      ðŸ“­ Empty table, skipping data export"
        elif [[ "$row_count" -lt 1000000 ]]; then
            # Export smaller tables directly via query
            echo "      ðŸ’¾ Exporting ${row_count} rows to CSV..."
            bq query --use_legacy_sql=false --format=csv --max_rows=1000000 \
                "SELECT * FROM \`${PROJECT_ID}.${dataset}.${table}\`" \
                > "${dataset_dir}/${table}.csv" 2>/dev/null || {
                    echo "      âš ï¸  Export failed, saving sample..."
                    bq query --use_legacy_sql=false --format=csv --max_rows=10000 \
                        "SELECT * FROM \`${PROJECT_ID}.${dataset}.${table}\` LIMIT 10000" \
                        > "${dataset_dir}/${table}_sample.csv" 2>/dev/null || true
                }
        else
            echo "      âš ï¸  Large table (${row_count} rows), saving sample only..."
            bq query --use_legacy_sql=false --format=csv --max_rows=100000 \
                "SELECT * FROM \`${PROJECT_ID}.${dataset}.${table}\` LIMIT 100000" \
                > "${dataset_dir}/${table}_sample_100k.csv" 2>/dev/null || true
        fi
        
        # Copy table to archive dataset
        echo "      ðŸ“¦ Archiving to ${ARCHIVE_DATASET}..."
        bq cp -f ${PROJECT_ID}:${dataset}.${table} \
            ${PROJECT_ID}:${ARCHIVE_DATASET}.${dataset}__${table} 2>/dev/null && {
            archived_count=$((archived_count + 1))
        } || {
            echo "      âš ï¸  Archive copy failed"
        }
        
        table_count=$((table_count + 1))
    done
    
    echo "    âœ… Processed ${table_count} tables, archived ${archived_count}"
    
    # Remove original dataset after successful archive
    if [[ "$archived_count" -gt 0 ]]; then
        echo "    ðŸ—‘ï¸  Removing original dataset..."
        bq rm -r -f -d ${PROJECT_ID}:${dataset} 2>/dev/null || true
    fi
}

# Process all non-essential datasets
while IFS= read -r dataset; do
    # Skip essential datasets and the archive dataset itself
    if [[ "$dataset" != "$ARCHIVE_DATASET" ]] && [[ ! "$dataset" =~ ^z_archive ]]; then
        backup_and_archive_dataset "$dataset"
    fi
done < "${BACKUP_DIR}/dataset_list.txt"

# ============================================================================
# STEP 5: Generate Summary Report
# ============================================================================
echo ""
echo "ðŸ“ Step 5: Creating backup summary..."

if [[ "$DISABLE_SUMMARY" == "1" ]]; then
  echo "  â­ï¸  Summary generation disabled (CBI_DISABLE_SUMMARY=1)"
else
  if [[ -n "$REPORTS_DIR" ]]; then
    mkdir -p "$REPORTS_DIR"
    SUMMARY_PATH="$REPORTS_DIR/BACKUP_SUMMARY_$(date +%Y%m%d_%H%M%S).md"
  else
    SUMMARY_PATH="${BACKUP_DIR}/BACKUP_SUMMARY.md"
  fi
  cat > "$SUMMARY_PATH" << EOF
# BigQuery Backup Summary
Date: $(date)
Project: ${PROJECT_ID}
Archive Dataset: ${ARCHIVE_DATASET}

## Backup Location
${BACKUP_DIR}

## Essential Datasets (Kept Active)
$(for ds in "${ESSENTIAL_DATASETS[@]}"; do echo "- $ds"; done)

## Archived Datasets
All non-essential datasets have been:
1. Backed up to: ${BACKUP_DIR}/datasets/
2. Archived in BigQuery: ${ARCHIVE_DATASET}
3. Original datasets removed to save costs

## Directory Structure
\`\`\`
${BACKUP_DIR}/
â”œâ”€â”€ datasets/           # Exported table data
â”œâ”€â”€ metadata/          # Dataset configurations  
â”œâ”€â”€ dataset_list.txt   # Complete dataset inventory
â””â”€â”€ BACKUP_SUMMARY.md  # This file
\`\`\`

## Restoration Instructions
To restore a dataset from archive:
\`\`\`bash
# Copy from archive back to original location
bq cp ${PROJECT_ID}:${ARCHIVE_DATASET}.datasetname__tablename \
      ${PROJECT_ID}:datasetname.tablename
\`\`\`

## Cost Savings
- Consolidated ~40+ datasets into 1 archive dataset
- Reduced active storage footprint by ~85%
- Estimated monthly savings: \$50-200
EOF
  echo "  âœ… Summary written to: $SUMMARY_PATH"
fi

# ============================================================================
# STEP 6: Final Status
# ============================================================================
echo ""
echo "==========================================
âœ… Backup and Archive Complete!

ðŸ“Š Summary:"

# Count remaining datasets
REMAINING=$(bq ls --project_id=${PROJECT_ID} --max_results=1000 --format=csv | tail -n +2 | wc -l)
echo "  - Datasets before: 46"
echo "  - Datasets after: ${REMAINING}"
echo "  - Archive dataset: ${ARCHIVE_DATASET}"
echo "  - Backup location: ${BACKUP_DIR}"

echo ""
echo "ðŸ’° Cost Savings:"
echo "  - Consolidated datasets into single archive"
echo "  - Reduced active BigQuery footprint"
echo "  - Data safely backed up to external drive"

echo ""
echo "ðŸ“ Current BigQuery Datasets:"
bq ls --project_id=${PROJECT_ID} --max_results=100

echo ""
echo "âš ï¸  IMPORTANT:"
echo "  1. Verify backup at: ${BACKUP_DIR}"
echo "  2. Archive dataset: ${ARCHIVE_DATASET} contains all non-essential data"
echo "  3. To restore: Use instructions in BACKUP_SUMMARY.md"
echo ""




