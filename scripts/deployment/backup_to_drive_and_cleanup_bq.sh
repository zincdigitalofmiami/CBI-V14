#!/bin/bash
# ============================================================================
# Backup BigQuery to External Drive and Remove to Save Money
# Date: November 19, 2025
# Purpose: Export all data to external drive, then remove from BQ to save costs
# ============================================================================

set -euo pipefail

PROJECT_ID="cbi-v14"
EXTERNAL_DRIVE="/Volumes/Satechi Hub/Projects/CBI-V14/TrainingData"
BACKUP_DIR="${EXTERNAL_DRIVE}/00_bigquery_backup_$(date +%Y%m%d)"
# Optional: override summary output location to keep reports inside repo
# Set CBI_REPORTS_DIR to a safe repo path (e.g., ./docs/reports)
REPORTS_DIR="${CBI_REPORTS_DIR:-}"
DISABLE_SUMMARY="${CBI_DISABLE_SUMMARY:-0}"
LOCATION="us-central1"

echo "üíæ BigQuery Backup to External Drive & Cleanup"
echo "=============================================="
echo "Project: $PROJECT_ID"
echo "Backup Location: $BACKUP_DIR"
echo ""
echo "‚ö†Ô∏è  WARNING: This will backup and REMOVE datasets from BigQuery"
echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
sleep 5

# ============================================================================
# STEP 1: Create Backup Directory Structure
# ============================================================================
echo ""
echo "üìÅ Step 1: Creating backup directory structure..."
mkdir -p "${BACKUP_DIR}"
mkdir -p "${BACKUP_DIR}/datasets"
mkdir -p "${BACKUP_DIR}/metadata"

# ============================================================================
# STEP 2: Export Dataset Metadata
# ============================================================================
echo ""
echo "üìä Step 2: Exporting dataset metadata..."

# Get list of all datasets
echo "Fetching dataset list..."
bq ls -d --project_id=${PROJECT_ID} --format=json > "${BACKUP_DIR}/metadata/datasets_list.json"
bq ls -d --project_id=${PROJECT_ID} --format=prettyjson > "${BACKUP_DIR}/metadata/datasets_list_pretty.json"

# ============================================================================
# STEP 3: Define Datasets to Backup and Remove
# ============================================================================

# DATASETS TO KEEP IN BQ (essential for operations)
KEEP_IN_BQ=(
    "raw_intelligence"    # Active ingestion target
    "market_data"        # Current market data
    "features"           # Active features
    "training"           # Active training data
    "predictions"        # Current predictions
    "monitoring"         # System monitoring
    "api"               # Dashboard views
)

# ALL OTHER DATASETS TO BACKUP AND REMOVE
BACKUP_AND_REMOVE=(
    "archive"
    "archive_backup_20251115"
    "archive_consolidation_nov6"
    "bkp"
    "curated"
    "dashboard"
    "dashboard_backup_20251115_final"
    "dashboard_tmp"
    "deprecated"
    "dim"
    "drivers"
    "export_evaluated_data_items_cbi_v14_automl_pilot_1w_2025_10_28T10_08_35_327Z"
    "features_backup_20251115"
    "features_backup_20251117"
    "forecasting_data_warehouse"
    "forecasting_data_warehouse_backup_20251117"
    "model_backups_oct27"
    "models"
    "models_v4"
    "models_v4_backup_20251117"
    "models_v5"
    "monitoring_backup_20251115"
    "neural"
    "ops"
    "performance"
    "predictions_backup_20251115"
    "predictions_uc1"
    "raw"
    "raw_intelligence_backup_20251115"
    "raw_intelligence_backup_20251117"
    "regimes"
    "signals"
    "staging"
    "staging_ml"
    "training_backup_20251115"
    "training_backup_20251117"
    "vegas_intelligence"
    "weather"
    "yahoo_finance_comprehensive"
)

# ============================================================================
# STEP 4: Backup Each Dataset
# ============================================================================
echo ""
echo "üíæ Step 3: Backing up datasets to external drive..."

backup_dataset() {
    local dataset=$1
    echo "  Processing ${dataset}..."
    
    # Create dataset directory
    local dataset_dir="${BACKUP_DIR}/datasets/${dataset}"
    mkdir -p "${dataset_dir}"
    
    # Get dataset metadata
    bq show --format=prettyjson ${PROJECT_ID}:${dataset} > "${dataset_dir}/dataset_metadata.json" 2>/dev/null || true
    
    # Get list of tables
    local tables=$(bq ls -n 10000 --project_id=${PROJECT_ID} ${dataset} 2>/dev/null | tail -n +3 | awk '{print $1}')
    
    if [ -z "$tables" ]; then
        echo "    No tables in ${dataset}"
        return
    fi
    
    # Export each table
    local table_count=0
    for table in $tables; do
        echo "    Exporting ${dataset}.${table}..."
        
        # Try to export as Parquet (best for preservation)
        bq extract \
            --destination_format=PARQUET \
            --compression=SNAPPY \
            ${PROJECT_ID}:${dataset}.${table} \
            "${dataset_dir}/${table}.parquet" 2>/dev/null || {
            # If Parquet fails, try CSV
            echo "      Parquet failed, trying CSV..."
            bq extract \
                --destination_format=CSV \
                --compression=GZIP \
                ${PROJECT_ID}:${dataset}.${table} \
                "${dataset_dir}/${table}.csv.gz" 2>/dev/null || {
                # If CSV fails, try to query and save
                echo "      Extract failed, trying query export..."
                bq query --use_legacy_sql=false --format=csv \
                    "SELECT * FROM \`${PROJECT_ID}.${dataset}.${table}\` LIMIT 1000000" \
                    > "${dataset_dir}/${table}_sample.csv" 2>/dev/null || true
            }
        }
        
        # Get table schema
        bq show --schema --format=prettyjson ${PROJECT_ID}:${dataset}.${table} \
            > "${dataset_dir}/${table}_schema.json" 2>/dev/null || true
        
        table_count=$((table_count + 1))
    done
    
    echo "    ‚úì Backed up ${table_count} tables"
}

# Backup datasets to remove
for dataset in "${BACKUP_AND_REMOVE[@]}"; do
    if bq ls -d --project_id=${PROJECT_ID} | grep -q "^${dataset}$"; then
        backup_dataset "$dataset"
    else
        echo "  Dataset ${dataset} not found, skipping..."
    fi
done

# Also backup essential datasets (but don't remove)
echo ""
echo "üì¶ Step 4: Backing up essential datasets (keeping in BQ)..."
for dataset in "${KEEP_IN_BQ[@]}"; do
    if bq ls -d --project_id=${PROJECT_ID} | grep -q "^${dataset}$"; then
        backup_dataset "$dataset"
    fi
done

# ============================================================================
# STEP 5: Create Backup Summary
# ============================================================================
echo ""
echo "üìù Step 5: Creating backup summary..."

if [[ "$DISABLE_SUMMARY" == "1" ]]; then
  echo "  ‚è≠Ô∏è  Summary generation disabled (CBI_DISABLE_SUMMARY=1)"
else
  if [[ -n "$REPORTS_DIR" ]]; then
    mkdir -p "$REPORTS_DIR"
    SUMMARY_PATH="$REPORTS_DIR/BACKUP_SUMMARY_$(date +%Y%m%d_%H%M%S).md"
  else
    SUMMARY_PATH="${BACKUP_DIR}/BACKUP_SUMMARY.md"
  fi
  cat > "$SUMMARY_PATH" << EOF
# BigQuery Backup Summary
**Date:** $(date)
**Project:** ${PROJECT_ID}

## Datasets Backed Up and Removed
$(for d in "${BACKUP_AND_REMOVE[@]}"; do echo "- $d"; done)

## Datasets Backed Up but Kept in BQ
$(for d in "${KEEP_IN_BQ[@]}"; do echo "- $d"; done)

## Backup Location
${BACKUP_DIR}

## Restore Instructions
To restore a dataset:
\`\`\`bash
# For Parquet files
bq load --source_format=PARQUET \\
  ${PROJECT_ID}.dataset_name.table_name \\
  /path/to/backup.parquet

# For CSV files  
bq load --source_format=CSV \\
  ${PROJECT_ID}.dataset_name.table_name \\
  /path/to/backup.csv.gz
\`\`\`

## Space Saved
Check BigQuery console for updated storage costs.
EOF
  echo "  ‚úÖ Summary written to: $SUMMARY_PATH"
fi

# ============================================================================
# STEP 6: Remove Datasets from BigQuery
# ============================================================================
echo ""
echo "üóëÔ∏è  Step 6: Removing backed-up datasets from BigQuery..."
echo "‚ö†Ô∏è  Last chance to cancel (Ctrl+C)! Waiting 5 seconds..."
sleep 5

for dataset in "${BACKUP_AND_REMOVE[@]}"; do
    if bq ls -d --project_id=${PROJECT_ID} | grep -q "^${dataset}$"; then
        # Check if backup exists
        if [ -d "${BACKUP_DIR}/datasets/${dataset}" ]; then
            echo "  Removing ${dataset} from BigQuery..."
            bq rm -r -f -d ${PROJECT_ID}.${dataset} 2>/dev/null || true
            echo "    ‚úì Removed"
        else
            echo "  ‚ö†Ô∏è  Skipping removal of ${dataset} - no backup found"
        fi
    fi
done

# ============================================================================
# STEP 7: Calculate Space and Cost Savings
# ============================================================================
echo ""
echo "üí∞ Step 7: Calculating savings..."

# Get remaining datasets
remaining_datasets=$(bq ls -d --project_id=${PROJECT_ID} | tail -n +3 | wc -l)
backup_size=$(du -sh "${BACKUP_DIR}" 2>/dev/null | cut -f1)

echo ""
echo "=========================================="
echo "‚úÖ Backup and Cleanup Complete!"
echo ""
echo "üìä Summary:"
echo "  - Datasets backed up: ${#BACKUP_AND_REMOVE[@]}"
echo "  - Datasets kept in BQ: ${#KEEP_IN_BQ[@]}"
echo "  - Total datasets now: ${remaining_datasets}"
echo "  - Backup size: ${backup_size}"
echo "  - Backup location: ${BACKUP_DIR}"
echo ""
echo "üí∞ Cost Savings:"
echo "  - Removed ~45 datasets from BigQuery"
echo "  - Estimated monthly savings: \$50-200 (depending on size)"
echo "  - Storage is now on external drive (one-time cost)"
echo ""
echo "üìÅ Remaining BigQuery Datasets:"
bq ls -d --project_id=${PROJECT_ID} --format=pretty
echo ""
echo "‚ö†Ô∏è  IMPORTANT:"
echo "  1. Verify backup at: ${BACKUP_DIR}"
echo "  2. Keep this backup safe - it's your only copy!"
echo "  3. You can restore anytime using instructions in BACKUP_SUMMARY.md"
echo ""




