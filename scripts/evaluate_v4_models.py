#!/usr/bin/env python3
"""
V4 MODEL EVALUATION - Comprehensive Performance Analysis
Compares all V4 models against V3 baseline
"""

from google.cloud import bigquery
from datetime import datetime
import pandas as pd

client = bigquery.Client(project='cbi-v14')

print("=" * 80)
print("ðŸ”¬ V4 MODEL EVALUATION - COMPREHENSIVE PERFORMANCE ANALYSIS")
print("=" * 80)
print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

# Models to evaluate
models = [
    # V3 Baseline (Production)
    ("1w", "V3 Boosted Tree", "cbi-v14.models.zl_boosted_tree_1w_v3"),
    ("1m", "V3 Boosted Tree", "cbi-v14.models.zl_boosted_tree_1m_v3"),
    ("3m", "V3 Boosted Tree", "cbi-v14.models.zl_boosted_tree_3m_v3"),
    ("6m", "V3 Boosted Tree", "cbi-v14.models.zl_boosted_tree_6m_v3"),
    
    # V4 Fixed DNNs
    ("1w", "V4 DNN (Fixed)", "cbi-v14.models_v4.zl_dnn_1w_v4"),
    ("1m", "V4 DNN (Fixed)", "cbi-v14.models_v4.zl_dnn_1m_v4"),
    
    # V3 DNNs (working for 3m/6m)
    ("3m", "V3 DNN", "cbi-v14.models.zl_dnn_3m_production"),
    ("6m", "V3 DNN", "cbi-v14.models.zl_dnn_6m_production"),
]

results = []

print("ðŸ“Š Evaluating models...\n")

for horizon, name, model_path in models:
    print(f"Evaluating: {name} ({horizon})")
    
    try:
        # Get evaluation metrics
        eval_query = f"""
        SELECT 
            mean_absolute_error,
            mean_squared_error,
            r2_score
        FROM ML.EVALUATE(MODEL `{model_path}`)
        LIMIT 1
        """
        
        eval_df = client.query(eval_query).to_dataframe()
        
        if not eval_df.empty:
            mae = float(eval_df['mean_absolute_error'].iloc[0])
            mse = float(eval_df['mean_squared_error'].iloc[0])
            r2 = float(eval_df['r2_score'].iloc[0])
            rmse = mse ** 0.5
            
            # Calculate MAPE (assuming average price ~$50)
            mape = (mae / 50.0) * 100
            
            # Performance grade
            if mape < 2.0:
                grade = "â­â­â­ EXCELLENT"
            elif mape < 3.5:
                grade = "â­â­ GOOD"
            elif mape < 5.0:
                grade = "â­ FAIR"
            else:
                grade = "âŒ POOR"
            
            results.append({
                "Horizon": horizon,
                "Model": name,
                "MAE": round(mae, 2),
                "RMSE": round(rmse, 2),
                "RÂ²": round(r2, 3),
                "MAPE %": round(mape, 2),
                "Grade": grade,
                "Path": model_path
            })
            
            print(f"   âœ… MAE: {mae:.2f}, RÂ²: {r2:.3f}, MAPE: {mape:.2f}% {grade}")
        else:
            print(f"   âš ï¸  No evaluation data available")
            
    except Exception as e:
        print(f"   âŒ Error: {str(e)}")
        results.append({
            "Horizon": horizon,
            "Model": name,
            "MAE": None,
            "RMSE": None,
            "RÂ²": None,
            "MAPE %": None,
            "Grade": "ERROR",
            "Path": model_path
        })
    
    print()

# Create DataFrame for analysis
results_df = pd.DataFrame(results)

print("\n" + "=" * 80)
print("ðŸ“Š EVALUATION SUMMARY")
print("=" * 80)
print()

# Display full results
print(results_df.to_string(index=False))
print()

# Best model per horizon
print("=" * 80)
print("ðŸ† BEST MODEL PER HORIZON (Lowest MAPE)")
print("=" * 80)
print()

for horizon in ["1w", "1m", "3m", "6m"]:
    horizon_results = results_df[results_df['Horizon'] == horizon].copy()
    horizon_results = horizon_results.dropna(subset=['MAPE %'])
    
    if not horizon_results.empty:
        best = horizon_results.loc[horizon_results['MAPE %'].idxmin()]
        print(f"{horizon.upper()}: {best['Model']} - MAPE {best['MAPE %']:.2f}% (MAE {best['MAE']:.2f}, RÂ² {best['RÂ²']:.3f}) {best['Grade']}")
    else:
        print(f"{horizon.upper()}: No valid evaluations")

print()

# V3 vs V4 comparison
print("=" * 80)
print("ðŸ”¬ V3 VS V4 COMPARISON")
print("=" * 80)
print()

v3_results = results_df[results_df['Model'].str.contains('V3')].copy()
v4_results = results_df[results_df['Model'].str.contains('V4')].copy()

if not v3_results.empty and not v4_results.empty:
    v3_avg_mape = v3_results['MAPE %'].mean()
    v4_avg_mape = v4_results['MAPE %'].dropna().mean()
    
    print(f"V3 Average MAPE: {v3_avg_mape:.2f}%")
    print(f"V4 Average MAPE: {v4_avg_mape:.2f}%")
    print()
    
    if v4_avg_mape < v3_avg_mape:
        improvement = ((v3_avg_mape - v4_avg_mape) / v3_avg_mape) * 100
        print(f"âœ… V4 is {improvement:.1f}% better than V3 (lower MAPE)")
    else:
        degradation = ((v4_avg_mape - v3_avg_mape) / v3_avg_mape) * 100
        print(f"âš ï¸  V4 is {degradation:.1f}% worse than V3 (higher MAPE)")

print()

# Performance against targets
print("=" * 80)
print("ðŸŽ¯ PERFORMANCE AGAINST TARGETS")
print("=" * 80)
print()
print("Target: MAPE < 2.0% for all horizons")
print()

meeting_target = results_df[results_df['MAPE %'] < 2.0]
print(f"Models meeting target: {len(meeting_target)} / {len(results_df[results_df['MAPE %'].notna()])}")

if not meeting_target.empty:
    print("\nâœ… MODELS MEETING 2% TARGET:")
    for _, row in meeting_target.iterrows():
        print(f"   - {row['Model']} ({row['Horizon']}): MAPE {row['MAPE %']:.2f}%")
else:
    print("\nâŒ NO MODELS CURRENTLY MEET THE 2% MAPE TARGET")
    print("\nClosest models:")
    closest = results_df.dropna(subset=['MAPE %']).nsmallest(3, 'MAPE %')
    for _, row in closest.iterrows():
        print(f"   - {row['Model']} ({row['Horizon']}): MAPE {row['MAPE %']:.2f}%")

print()

# AutoML status check
print("=" * 80)
print("â³ AUTOML STATUS")
print("=" * 80)
print()

automl_check_query = """
SELECT table_id, creation_time
FROM `cbi-v14.models_v4.__TABLES_SUMMARY__`
WHERE table_id LIKE 'zl_automl%v4'
AND type = 2
ORDER BY table_id
"""

try:
    automl_df = client.query(automl_check_query).to_dataframe()
    
    if not automl_df.empty:
        print(f"âœ… {len(automl_df)} AutoML models found:")
        for _, row in automl_df.iterrows():
            print(f"   - {row['table_id']} (created: {row['creation_time']})")
        print("\nðŸ’¡ AutoML training complete! Re-run this evaluation to compare.")
    else:
        print("â³ AutoML models still training (~3-4 hours remaining)")
        print("   Expected models: zl_automl_1w_v4, zl_automl_1m_v4, zl_automl_3m_v4, zl_automl_6m_v4")
except Exception as e:
    print(f"âš ï¸  Could not check AutoML status: {str(e)}")

print()

# Recommendations
print("=" * 80)
print("ðŸ’¡ RECOMMENDATIONS")
print("=" * 80)
print()

print("1. CURRENT STATE:")
print("   - V3 Boosted Tree models are production-ready and performing well")
print("   - V4 Fixed DNNs show improvement over broken V3 DNNs but don't meet 2% target")
print("   - ARIMA models trained successfully (time series baseline)")
print()

print("2. NEXT STEPS:")
print("   - Wait for AutoML training to complete (~3-4 hours remaining)")
print("   - Create ensemble models combining best V3 and V4 models")
print("   - Re-evaluate to see if ensemble achieves 2% MAPE target")
print()

print("3. PRODUCTION RECOMMENDATION:")
if v3_results['MAPE %'].mean() < 3.5:
    print("   âœ… KEEP V3 AS DEFAULT - Proven, reliable, meets institutional standards")
    print("   âš¡ ADD V4 AS OPTIONAL - For advanced users wanting model selection")
else:
    print("   âš ï¸  EVALUATE ENSEMBLE - V3 alone may not meet all requirements")

print()
print("=" * 80)
print(f"Evaluation completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 80)

